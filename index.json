[{"content":"2025-03-02 符文般的JS 0 = [+[]]+[] 1 = [+!+[]]+[] 10 = [+!+[]]+[+[]] 100 = [+!+[]]+[+[]]+[+[]] 1. 0 = [+[]]+[]​ ​[]​ 是一个空数组。 ​+[]​ 将空数组转换为数字。空数组转换为数字时是 0​，所以 +[]​ 的结果是 0​。 ​[+[]]​ 将 0​ 放入一个数组中，结果是 [0]​。 ​[+[]]+[]​ 将数组 [0]​ 转换为字符串，结果是 \u0026quot;0\u0026quot;​。 最终结果：\u0026quot;0\u0026quot;​（字符串形式的 0​）。\n2. 1 = [+!+[]]+[]​ ​[]​ 是一个空数组。 ​+[]​ 将空数组转换为数字，结果是 0​。 ​!+[]​ 对 0​ 进行逻辑非运算。0​ 是假值，所以 !0​ 是 true​。 ​+!+[]​ 将 true​ 转换为数字，true​ 转换为数字是 1​。 ​[+!+[]]​ 将 1​ 放入一个数组中，结果是 [1]​。 ​[+!+[]]+[]​ 将数组 [1]​ 转换为字符串，结果是 \u0026quot;1\u0026quot;​。 最终结果：\u0026quot;1\u0026quot;​（字符串形式的 1​）。\n3. 10 = [+!+[]]+[+[]]​ ​+!+[]​ 如前所述，结果是 1​。 ​[+!+[]]​ 将 1​ 放入一个数组中，结果是 [1]​。 ​+[]​ 如前所述，结果是 0​。 ​[+[]]​ 将 0​ 放入一个数组中，结果是 [0]​。 ​[+!+[]]+[+[]]​ 将 [1]​ 和 [0]​ 转换为字符串并连接，结果是 \u0026quot;10\u0026quot;​。 最终结果：\u0026quot;10\u0026quot;​（字符串形式的 10​）。\n4. 100 = [+!+[]]+[+[]]+[+[]]​ ​+!+[]​ 如前所述，结果是 1​。 ​[+!+[]]​ 将 1​ 放入一个数组中，结果是 [1]​。 ​+[]​ 如前所述，结果是 0​。 ​[+[]]​ 将 0​ 放入一个数组中，结果是 [0]​。 ​[+!+[]]+[+[]]+[+[]]​ 将 [1]​、[0]​ 和 [0]​ 转换为字符串并连接，结果是 \u0026quot;100\u0026quot;​。 最终结果：\u0026quot;100\u0026quot;​（字符串形式的 100​）。\n总结 这些代码利用了 JavaScript 中的隐式类型转换和运算符的特性，通过将数组、布尔值和数字相互转换，最终生成了字符串形式的数字。虽然这些代码看起来很复杂，但它们展示了 JavaScript 中类型转换的灵活性。\n‍\n","permalink":"https://mobilephone724.github.io/post/20250302-8ctcr.html","summary":"2025-03-02 符文般的JS 0 = [+[]]+[] 1 = [+!+[]]+[] 10 = [+!+[]]+[+[]] 100 = [+!+[]]+[+[]]+[+[]] 1. 0 = [+[]]+[]​ ​[]​ 是一个空数组。 ​+[]​ 将空数组转换为数字。空数组转换为数字时是 0​，","title":"【日拱一卒】2025-03-02"},{"content":"Column-Oriented Storage Basic idea: Although fact tables are often over 100 columns wide, a typical data warehouse query only accesses 4 or 5 of them at one time (SELECT *​ queries are rarely needed for analytics)\nThe idea behind column-oriented storage is simple: don’t store all the values from one row together, but store all the values from each column together instead. If each column is stored in a separate file, a query only needs to read and parse those columns that are used in that query, which can save a lot of work\n‍\nColumn Compression ​​\nOften, the number of distinct values in a column is small compared to the number of rows (for example, a retailer may have billions of sales transactions, but only 100,000 distinct products). We can now take a column with n distinct values and turn it into n separate bitmaps: one bitmap for each distinct value, with one bit for each row. The bit is 1 if the row has that value, and 0 if not.\n​WHERE product_sk IN (30, 68, 69):​ Load the three bitmaps for product_sk = 30, product_sk = 68, and product_sk = 69, and calculate the bitwise OR of the three bitmaps, which can be done very efficiently. ​WHERE product_sk = 31 AND store_sk = 3:​ Load the bitmaps for product_sk = 31 and store_sk = 3, and calculate the bitwise AND. This works because the columns contain the rows in the same order, so the kth bit in one column’s bitmap corresponds to the same row as the kth bit in another column’s bitmap. Writing to Column-Oriented Storage Column-oriented storage, compression, and sorting all help to make those read queries faster. However, they have the downside of making writes more difficult.\nFortunately, we have already seen a good solution earlier in this chapter: LSM-trees. All writes first go to an in-memory store, where they are added to a sorted structure and prepared for writing to disk\n‍\nAggregation: Data Cubes and Materialized Views If the same aggregates are used by many different queries, it can be wasteful to crunch through the raw data every time. Why not cache some of the counts or sums that queries use most often?\n​​\n‍\nLSM TREE Short introduction this time. The data structure may worth a deep study!. The following contents are from deepseek:\nLSM-Tree（日志结构合并树）简介 LSM-Tree（Log-Structured Merge-Tree）是一种高效处理高写入吞吐量的存储数据结构，广泛应用于现代数据库（如LevelDB、RocksDB、Cassandra）和存储引擎中。其核心设计目标是通过顺序写入优化写性能，同时通过分层合并（Compaction）平衡读写效率。\n核心设计思想 写入优化：\n内存缓冲（MemTable） ：所有写入操作首先进入内存中的有序结构（MemTable），避免直接随机写磁盘。 顺序写入磁盘（SSTable） ：当MemTable填满后，冻结为不可变的SSTable（Sorted String Table）并顺序写入磁盘，减少磁盘寻址开销。 分层存储与合并（Compaction） ：\n分层结构：磁盘上的SSTable按层级组织，通常分为多个层级（如Level 0到Level N），低层级数据较新，高层级数据较旧且合并后更大。 合并过程：后台定期将低层级的SSTable与高层级合并，消除重复数据（如删除标记），减少读取时的I/O放大问题。 关键组件 MemTable：\n基于内存的有序结构（如跳表或平衡树），支持快速插入和查询。 写入达到阈值后冻结，转为SSTable写入磁盘。 SSTable（Sorted String Table） ：\n不可变的有序键值存储文件，按主键排序。 包含索引（如稀疏索引）和Bloom Filter，加速点查询。 Compaction策略：\nSize-Tiered Compaction（大小分级）：合并同层大小相近的SSTable。 Leveled Compaction（分层合并）：每层仅保留一个SSTable，合并时跨层优化，减少空间放大。 工作流程 写入路径：\n写入MemTable → MemTable写满后转为SSTable → 写入磁盘Level 0。 读取路径：\n先查询MemTable → 未命中则逐层查询SSTable（Level 0到N）。 使用Bloom Filter快速过滤不存在的键，减少磁盘访问。 合并流程：\n后台线程选择层级中的SSTable进行合并，生成新的高层级SSTable，并删除旧文件。 优缺点分析 优点 缺点 ✅ 高写入吞吐量（顺序写优化） ❌ 读取延迟可能较高（需查多层级） ✅ 天然支持数据压缩和去重 ❌ Compaction可能占用CPU/IO资源 ✅ 适合SSD（减少随机写磨损） ❌ 范围查询效率依赖层级合并策略 应用场景 写密集型系统：日志存储、时序数据库（如InfluxDB）、物联网设备数据采集。 分布式数据库：Cassandra、HBase底层存储引擎。 消息队列：Kafka的日志存储（类似LSM思想）。 对比B-Tree 特性 LSM-Tree B-Tree 写入性能 高（顺序写） 低（随机写导致页分裂） 读取性能 点查询依赖Bloom Filter，范围查询较慢 点查询和范围查询均高效 存储开销 可能有写放大（Compaction） 空间利用率高，但需维护页结构 适用场景 写多读少，SSD环境 读多写少，传统硬盘环境 优化技术 Bloom Filter：加速点查询，减少不必要的磁盘访问。 分层索引：为SSTable建立稀疏索引，快速定位数据块。 并行Compaction：多线程合并减少对前台业务的影响。 Tiered vs Leveled Compaction：根据负载选择合并策略（写优化 vs 读优化）。 总结 LSM-Tree通过顺序写入和分层合并机制，在高吞吐写入场景中表现卓越，尤其适合现代SSD存储和分布式系统。尽管读取效率可能低于B-Tree，但通过Bloom Filter和合理的数据分层策略，仍能满足多数实时需求。理解其核心原理和权衡，是设计高性能存储系统的关键。\n‍\n","permalink":"https://mobilephone724.github.io/post/20250301-1r5xzc.html","summary":"Column-Oriented Storage Basic idea: Although fact tables are often over 100 columns wide, a typical data warehouse query only accesses 4 or 5 of them at one time (SELECT *​ queries are rarely needed for analytics) The idea behind column-oriented storage is simple: don’t store all the values from one row together, but store all the values from each column together instead. If each column","title":"【日拱一卒】2025-03-01"},{"content":"github ssh 失败 ssh 失败 github 可能会屏蔽 22 的 ssh 端口，导致使用 ssh 协议拉取推送代码失败\n解决方法为使用 443 端口，同时将 hostname 改为 ssh.github.com​\nhost github.com Hostname ssh.github.com User git IdentityFile ~/.ssh/github_mbp_linux Port 443 原因在 https://docs.github.com/en/authentication/troubleshooting-ssh/using-ssh-over-the-https-port 中有涉及。\n‍\n为什么使用 ProxyCommand 不可以： Host github.com AddKeysToAgent yes UseKeychain yes IdentityFile ~/.ssh/id_ed25519 ProxyCommand nc -x 127.0.0.1:7890 -X 5 %h %p 因为机场可能屏蔽 22 端口。\n","permalink":"https://mobilephone724.github.io/post/github-ssh-failed-2k9xdc.html","summary":"github ssh 失败 ssh 失败 github 可能会屏蔽 22 的 ssh 端口，导致使用 ssh 协议拉取推送代码失败 解决方法为使用 443 端口，同时将 hostname 改为 ssh.github.com​ host github.com Hostname ssh.github.com","title":"github ssh 失败"},{"content":"0x01-8级锁的不对称性 2024-08-17 00:14:26 +0800\n用于创建索引的 SHARE LOCK，虽然和修改时需要的 ROW EXCL 锁冲突，却不和自己冲突。虽然创建索引时不能插入数据，但是可以创建其他索引。 SHARE 这个名字非常贴切。\n注意：create index 可以事务块中执行\ntest=# begin transaction; BEGIN test=*# insert into test values (1); INSERT 0 1 test=*# create index on test (a); CREATE INDEX test=*# commit; COMMIT 而为了不阻塞读写，持有 SHARE UPDATE EXCL 锁的行为， 例如 vacuum , create index concurrent ，大都不能在事务块中进行。原因可能是这些操作需要感知其他正在执行的事务的状态(status of running processes)，其行为超出了一般意义上 MVCC 的。\n","permalink":"https://mobilephone724.github.io/posts/archive_posts/every_data_pg/","summary":"0x01-8级锁的不对称性 2024-08-17 00:14:26 +0800\n用于创建索引的 SHARE LOCK，虽然和修改时需要的 ROW EXCL 锁冲突，却不和自己冲突。虽然创建索引时不能插入数据，但是可以创建其他索引。 SHARE 这个名字非常贴切。\n注意：create index 可以事务块中执行\ntest=# begin transaction; BEGIN test=*# insert into test values (1); INSERT 0 1 test=*# create index on test (a); CREATE INDEX test=*# commit; COMMIT 而为了不阻塞读写，持有 SHARE UPDATE EXCL 锁的行为， 例如 vacuum , create index concurrent ，大都不能在事务块中进行。原因可能是这些操作需要感知其他正在执行的事务的状态(status of running processes)，其行为超出了一般意义上 MVCC 的。","title":"Everyday PostgreSQL"},{"content":"c - What does -fwrapv do? - Stack Overflow\n-fwrapv tells the compiler that overflow of signed integer arithmetic must be treated as well-defined behavior, even though it is undefined in the C standard.\nIt has two meaning full results:\nINT_MAX + 1 is overflowed to INT_MIN correctly. This is almost the default behavior in gcc. Don’t let the compiler assume x + 1 \u0026gt; x. See the program below\n╭─ycz at 9f38a58b120d in /home/dev 24-08-05 - 13:28:02 ╰─○ cat test.c #include \u0026lt;stdio.h\u0026gt; #define INT_MAX 0x7FFFFFFF static int compare(int x) {return x + 1 \u0026gt; x;} int main() { int x = 0; printf(\u0026#34;%d is bigger than %d?\\n%d\\n\u0026#34;, x + 1, x, compare(x)); x = INT_MAX; printf(\u0026#34;%d is bigger than %d?\\n%d\\n\u0026#34;, x + 1, x, compare(x)); return 0; } ╭─ycz at 9f38a58b120d in /home/dev 24-08-05 - 13:28:06 ╰─○ gcc test.c \u0026amp;\u0026amp; ./a.out 1 is bigger than 0? 1 -2147483648 is bigger than 2147483647? 1 ╭─ycz at 9f38a58b120d in /home/dev 24-08-05 - 13:28:13 ╰─○ gcc test.c -fwrapv \u0026amp;\u0026amp; ./a.out 1 is bigger than 0? 1 -2147483648 is bigger than 2147483647? 0 ","permalink":"https://mobilephone724.github.io/posts/archive_posts/fwrapv/","summary":"c - What does -fwrapv do? - Stack Overflow\n-fwrapv tells the compiler that overflow of signed integer arithmetic must be treated as well-defined behavior, even though it is undefined in the C standard.\nIt has two meaning full results:\nINT_MAX + 1 is overflowed to INT_MIN correctly. This is almost the default behavior in gcc. Don’t let the compiler assume x + 1 \u0026gt; x. See the program below","title":"\"-fwrapv\" option in gcc"},{"content":"0x0 what is inode From https://en.wikipedia.org/wiki/Inode\nThe inode (index node) is a data structure in a Unix-style file system that describes a file-system object such as a file or a directory. Each inode stores the attributes and disk block locations of the object\u0026rsquo;s data\nFrom https://www.redhat.com/sysadmin/inodes-linux-filesystem\nBy definition, an inode is an index node. It serves as a unique identifier for a specific piece of metadata on a given filesystem. Each piece of metadata describes what we think of as a file. That\u0026rsquo;s right, inodes operate on each filesystem, independent of the others.\nOnce you create a file, directory and so on, an inode is consumed. So it’s important to remain enough inodes.\n0x1 How many inodes are there? The inode upper limit is determined once the filesystem is initialized.\nFor ext4, there are two method to appoint the number\nbytes-per-inode : This is the default method to determine the number of inodes. This option approves an method to help estimate the inodes you may need through the average file size. mke2fs creates an inode for every bytes-per-inode bytes of space on the disk. The larger the bytes-per-inode ratio, the fewer inodes will be created. The default value is 16k. If you create too many 8k files, the inode will run out before the disk space. So the disk space is wasted If you create too many 32k files, the disk space will run out before the inode. So the inode is wasted number-of-inodes : Overrides the default calculation of the number of inodes that should be reserved for the filesystem. See https://wiki.archlinux.org/title/Ext4 for detail\n","permalink":"https://mobilephone724.github.io/posts/archive_posts/reversed_inode/","summary":"0x0 what is inode From https://en.wikipedia.org/wiki/Inode\nThe inode (index node) is a data structure in a Unix-style file system that describes a file-system object such as a file or a directory. Each inode stores the attributes and disk block locations of the object\u0026rsquo;s data\nFrom https://www.redhat.com/sysadmin/inodes-linux-filesystem\nBy definition, an inode is an index node. It serves as a unique identifier for a specific piece of metadata on a given filesystem. Each piece of metadata describes what we think of as a file.","title":"Number of Reversed Inode"},{"content":"0x0 backgroud Even though ARIES simplifies the recovery process and allows it to be generic for all transactional operations, recovering the database to a consistent state requires undoing all operations performed by uncommitted transactions which makes the cost of recovery proportional to the work performed by these transactions. This significantly impacts database availability since recovering a long running transaction can take several hours.\nThis paper describes the overall design of “Constant Time Recovery” (CTR)\nRecovering the database to a consistent state requires undoing all operations performed by uncommitted transactions, and recovering a long running transaction can take several hours\n0x1 BACKGROUND ON SQL SERVER 0x11 DatabaseRecovery Following ARIES, the SQL Server recovery process has three distinct phases. Figure 2 demonstrates these phases and the portion of the log they process.\n(The oldest transaction can run across many checkpoints)\nanalysis: identifys: any transactions that must be rolled back LSN of the oldest dirty page in the system Checkpoint process captured all active transactions and the oldest dirty page LSN at the time of the checkpoint, so analysis can start from that redo: bringing the database back to the state it was at the time of the failure: Since Analysis has recomputed the Oldest Dirty Page LSN, Redo should only process the log from this point. Only applies the operation if the Page LSN is lower Processes the log starting from the beginning of the oldest active transaction. This allows recovery to reacquire all the locks held by active transactions and make the database available at the end of Redo for improved availability undo: rolling back any transactions that were active at the time of the failure. As Redo has reacquired the locks required by these transactions, the Undo process can be performed while the database is available and user queries will be blocked only if they attempt to access the data modified by the transactions pending undo. Undoing these operations is also logged using Compensation Log Records (CLR) to guarantee that the database is recoverable even after a failure in the middle of the Undo process 0x12 Multi-versionConcurrencyControl Versioning is performed at the row level: for every user data update, SQL Server updates the row in-place in the data page and pushes the old version of the row to an append-only version store, linking the current row version to the previous version\nThe versions are linked to each other using their physical locator\nGiven that these versions are only used for the purposes of SI, the version store doesn’t need to be preserved across restarts and is stored in SQL Server’s “TempDB”, a system database that is recycled every time the SQL Server process restarts. This allows for efficient version generation, as these operations are not logged.\n0x2 CONSTANTTIMERECOVERY 0x21 Overview Database recovery in constant time, regardless of the user workload and transaction sizes. Transaction rollback in constant time regardless of the transaction size. Continuous transaction log truncation, even in the presence of long running transactions. CTR achieves these by separating transactional operations into three distinct categories and handling their recovery using the most appropriate mechanism.\n0x22 three transactional operations categories 0x221 Data Modifications All data modifications are versioned, storing the earlier versions of each row in the version store that is now redesigned to be persistent and recoverable\n（笑死我了，刚刚还在感叹 version store 无需记录，重启即删多么方便）\nWhen a transaction rolls back, it is simply marked as “aborted”, indicating that any new transactions should ignore the versions generated by this transaction and access the earlier committed versions\nDuring database recovery\nAnalysis identifies the state of every transaction Redo recovers the row and the version store content as of the time of the failure. Undo marks the uncommitted transactions as aborted making all updates by these transactions invisible. This allows Undo to complete in constant time, regardless of the transaction sizes. （仍然和 aborted xact count 相关，但相比于 modified row count，几乎可以忽略不计） 0x222 System Operations System operations refer to internal operations the DBMS uses to maintain its internal data structures, such as space allocation and deallocation, B-Tree page splits, etc.\ndifficulty: These operations cannot be easily versioned Additionally, these operations are usually tied to user data modifications and can be a significant percentage of the operations performed by a long-running transaction. For example, a large data load allocates a large number of pages Solution: These operations are always performed by short-lived, system transactions that update the internal data structures and immediately commit When a failure occurs, these operations will not be undone, but the allocated space and other updated data structures will be lazily reclaimed and fixed up in the background. 0x223 Logical and Other Non-versioned Operations This last category refers to operations that cannot be versioned because they are either:\nlogical: such as lock acquisition operations that indicate that a certain lock must be acquired during recovery cache invalidation operations that are responsible for invalidating in-memory caches when a transaction rolls back they are modifying data structures that need to be accessed during start up must maintain a very specific format that does not allow versioning CTR leverages an additional log stream, SLog, that allows tracking only the relevant operations and not having to process the full transaction log for the corresponding transactions. 0x23 Persistent Version Store Persistent Version Store (PVS) allows row versions to be recoverable by storing them in the user database and logging them in the transaction log as regular user data.\nHence, at the end of Redo all versions are fully recovered and can be accessed by user transactions 0x231 In-row Version Store Since in most cases the difference between the two versions is small (for example when only a few columns are updated), we can simply store the diff between the two versions\nEven though computing and reapplying the diff requires additional CPU cycles, the cost of generating an off-row version, by accessing another page and logging the version as a separate operation, is significantly higher （再读另一个页代价更大） ![[attachments/Pasted image 20240714101730.png]]\n（ diff 怎么存储呢？定长的 id 字段可能不够！）\nDespite its benefits in most common cases, in-row versioning can negatively impact the performance of the system if it significantly increases the size of rows in the data pages. This is particularly problematic for B-Trees as it can lead to page splits. （diff 会占用大量空间，引起 page 数量膨胀，B树分裂代价高 ）\n0x232 Off-row Version Store It is implemented as an internal table that has no indexes since all version accesses are based on the version’s physical locator (Page Id, Slot Id) （纯 heap，无索引）\nEach version of user data is stored as a separate row in this table, having some columns for persisting version metadata and a generic binary column that contains the full version content, regardless of the schema of the user table this version belongs to. （存全量数据，而不仅是 diff)\nBy leveraging regular logging, off-row PVS is recovered using the traditional recovery mechanisms\n0x23 Logical Revert 0x231 overview CTR leverages the PVS to instantly roll back data modifications.\nWhen a query accesses a row, it first checks the state (active, committed or aborted) of the transaction that generated the latest version.\nIf the transaction is active or has been committed: visibility depends on the query isolation level. （ 事务提交，则看第一个 version ） but if the transaction is aborted, this version is definitely not visible and the query traverses the version chain to identify the version that belongs to a committed transaction and is visible. （ 事务回滚，则遍历 version chain ） Additionally, if a new transaction updates a row with an aborted version, it must first revert the effects of the aborted transaction before proceeding with the update. （ 做完 redo 后，the latest version 可能是 abort 状态，真正有效的 version 在 version chain 中，所以需要修复 the latest version ）\nCTR implements two different mechanisms for reverting the updates performed by aborted transactions:\nLogical Revert is the process of bringing the committed version of a row back to the main row in the data page,\nso that all queries can access it directly and versions in the version store are no longer required This process compares the state of the aborted and committed versions and performs the required compensating operation the revert operations are not versioned Since these transactions only revert a row at a time, they are guaranteed to be short-lived and don’t affect recovery time. Logical Revert is used by a background cleanup process to eliminate all updates performed by aborted transactions and eventually remove the aborted transactions from the system. ![[attachments/Pasted image 20240714110353.png]] overwrite the aborted version with the new version it is generating\nThis process minimizes the overhead for these operations and allows them to be almost as fast as if there was no aborted version. ![[attachments/Pasted image 20240714110439.png]] In CTR, the database is fully available, releasing all locks, while row versions are lazily cleaned up in the background.\n0x232 Transaction State Management For SI, visibility depends on the commit timestamp of the transaction that generated the version. Since SQL Server does not allow snapshot transactions to span server restarts, the commit timestamps can be stored in memory and need not be recovered. CTR, however, requires tracking the state of aborted transactions until all their versions have been logically reverted and are no longer accessible.\nCTR stores the aborted transaction information in the “Aborted Transaction Map” (ATM)\nRestore ATM after crash:\nWhen a transaction aborts, before releasing any locks, it will add its Transaction Id to the ATM and generate an “ABORT” log record indicating that it was aborted. （产生 ATM 信息） When a checkpoint occurs, the full content of the ATM is serialized into the transaction log as part of the checkpoint information. （ checkpoint 整理 ATM 信息 ） Since Analysis starts processing the log from the Checkpoint Begin LSN of the last successful checkpoint, or earlier, it will process this information regarding the aborted transactions and reconstruct the ATM. （ analysis 根据 checkpoint 重建 ATM ） Any transactions that aborted after the last checkpoint will not be included in the checkpoint, but Analysis will process their ABORT log records and add them to the map （ analysis 分析 checkpoint 后面事务的 ABORT log，并写入 ATM ） Following this process, Analysis can reconstruct the ATM as of the time of the failure, so that it is available when the database becomes available at the end of Redo.\nOnce all versions generated by an aborted transaction have been reverted, the transaction is no longer interesting for recovery and can be removed from the ATM.\nRemoving a transaction is also a logged operation, using a “FORGET” log record, to guarantee that the content of the ATM is recovered correctly. （Removing from ATM 也要新加日志？这也太复杂了。。。。）\n0x233 Short Transaction Optimization Maintaining the Aborted Transaction Map and forcing queries to visit additional versions incur a performance penalty, short OLTP transactions as they would significantly increase the size of the ATM\nWhen a transaction attempts to roll back, we evaluate the number of operations it performed and the amount of log it generated and qualify it as “short” if these don’t exceed certain thresholds.\nShort transactions will not go through the CTR rollback process, but use traditional undo, so that they are immediately removed from the system. （ 太多短事务导致 ATM 膨胀，解决方案为混用 CTR 和 undo？这么复杂么？ ）\n0x24 Non-versioned Operations A variety of operations that cannot be versioned because they are:\nLogical: such as acquiring coarse-grained locks invalidating various caches when a transaction rolls back accumulating row and page statistics Updating system metadata in data structures Updating critical system metadata required for starting up the database, before recovery can reconstruct versioning information, such as updates to the “boot page”, a special page that contains the core information required for initialization.（ ？这些东西还能做成page？，还有版本管理？这就是商业数据么？ ） To handle these operations while guaranteeing recovery in constant time, we are leveraging two different mechanisms:\n0x241 SLog: A Secondary Log Stream SLog is a secondary log stream designed to only track non-versioned operations that must be redone or undone using information from the corresponding log records.\nFor example, when altering the data type of a column in a large table, the transaction will have to update millions of rows, but SLog will only contain a handful log records, for acquiring the exclusive lock and invalidating metadata caches.\n","permalink":"https://mobilephone724.github.io/article/constant_recovery/","summary":"0x0 backgroud Even though ARIES simplifies the recovery process and allows it to be generic for all transactional operations, recovering the database to a consistent state requires undoing all operations performed by uncommitted transactions which makes the cost of recovery proportional to the work performed by these transactions. This significantly impacts database availability since recovering a long running transaction can take several hours.\nThis paper describes the overall design of “Constant Time Recovery” (CTR)","title":"constant recovery with undo"},{"content":" paper Introduction: Memory Barriers: a Hardware View for Software Hackers 0x0 why we need memory barrier In short, because reordering memory references allows much better performance, and so memory barriers are needed to force ordering in things like synchronization primitives whose correct operation depends on ordered memory references.\n0x1 Cache Structure 0x11 some cases of cache miss(not important) The cache miss means that the CPU will have to wait (or be “stalled”) for hundreds of cycles while the item is fetched from memory.\ncapacity miss: After some time, the CPU’s cache will fill, and sub- sequent misses will likely need to eject an item from the cache in order to make room for the newly fetched item\nassociativity miss: occur in set-associative caches.\nAn \u0026ldquo;associativity cache miss\u0026rdquo; refers to a specific type of cache miss that can occur in set-associative caches.\nIn a set-associative cache, the cache memory is divided into sets, and each set contains multiple cache lines (or cache blocks). When the CPU needs to access data in memory, it first checks the cache to see if the data is present. The cache lookup is done by first identifying the set that the data would be stored in, and then searching through the multiple cache lines within that set to see if the data is present.\nAn associativity cache miss occurs when the data the CPU needs is not found in any of the cache lines within the identified set. This means the CPU has to go to main memory to fetch the data, which is slower than finding it in the cache.\nThe number of cache lines per set is called the \u0026ldquo;associativity\u0026rdquo; of the cache. Caches with higher associativity (more cache lines per set) generally have lower associativity cache miss rates, but they are also more complex and expensive to implement. The goal is to find the right balance of associativity to minimize cache misses without making the cache design overly complex.\nwrite miss: Before a given CPU writes to that data item, it must first cause it to be removed, or “invalidated”, from other CPUs’ caches. Once this invalidation has completed, the CPU may safely modify the data item. If the data item was present in this CPU’s cache, but was read- only, this process is termed a “write miss”.\ncache structure: one cache address can store two(or more?) sets of data.\n0x2 Cache-Coherence Protocols 0x21 Four state: MESI States The four types of states represent the state of a cache line in one cpu.\nHere, we use “I” to represent the cpu.\nmodified: I have changed the value in private cache and not written it back to memory. Others can’t access the memory until change their states(signal me). exclusive: I haven’t changed the value in private cache. (But may change it later, transfer to modified state) Others can’t access the memory until change their states(signal me). shared Others can read the memory without consulting me. invalid: the cache line holds no data. 0x22 Messages between the cpus(and memory): MESI Protocol Messages Read: a request for reading a line Read Response: The line data for a previous read. Either of memory or other cpu. Invalidate: invalidate the line in all other cpus Invalidate Acknowledge: successful response to a previous Invalidate message Read Invalidate: a atomic combination of “read” and “invalidate”. Requires both a “read response” and a set of “in- validate acknowledge” messages in reply. Writeback: write a line to memory 0x23 State Machine: MESI State Diagram Transitions are explained below:\n(a): write back to memory (b): modify the data in cache (c): I haven’t written back to memory but another cpu requests it (and will change it). So I return the value in cache and invalidate the private one without writing back to memory (d): I want to change a cache, so I emit a “invalidate” to other cpus. Now all others have acknowledge me, so I read and change my private cache. (e): Similar to “(d)”, but don’t need to read in memory. (f): Similar to (c), I haven’t written back to memory but another cpu requests it (but will not change it), so I return my private value. (g): Similar to (f) (h): similar to “(d)”, but don’t modify it now (i): similar to “(c)”, but don’t need to return my private value since that in memory is still the newest. (j): Similar to “(d)” (k): read in memory( or other cpu) (l): receive a “invalidate” message Examples are in the paper.\n0x3 Optimize 1: Stores Result in Unnecessary Stalls Consider to modify a cache that isn’t in modified or exclusive state.\nProblem: there is no real reason to force CPU 0 to stall for so long — after all, regardless of what data happens to be in the cache line that CPU 1 sends it, CPU 0 is going to unconditionally overwrite it. Solution: Add “store buffers” between each CPU and its cache CPU0 write to its store buffer immediately When CPU0 is acknowledged, the data will be moved from the store buffer to the cache line 0x31: Store Forwarding problem Thinking of the program below:\nCPU0 has value b CPU1 has value a = 0 in exclusive mode a = 1; b = a + 1; assert(b == 2); step1: cpu0 invalidate cpu1 with “a” , change private cache line to 1, and write it to store buffer.\nstep2: cpu0 receive the value “a” from cpu1. The value is 0, and it’s stored in private cache.(Note that the value in the store buffer is 1)\nstep3: cpu0 executes “b = a + 1”, load “a” from cache, and its value is 0\nstep4: cpu0 store the value of “b” to cache, whose value is 1\nstep5: cpu0 move the value of “a” from store buffer(1) to cache(0)\nstep6: CPU0 executes assert(b==2), which fails.\nThe problem is that we have two copies of “a”, one in the cache and the other in the store buffer.\nThe hardware guys took pity and implemented “store forwarding”, where each CPU refers to (or “snoops”) its store buffer as well as its cache when performing loads\nIn other words, a given CPU’s stores are directly forwarded to its subsequent loads, without hav- ing to pass through the cache.\n0x32 Store Buffers and Memory Barriers Think of the program below\nvoid foo(void) { a=1; b=1; } void bar(void) { while (b == 0) continue; assert(a == 1); } cpu0 executes foo own “b” cpu1 executes bar own “a” The problem is that, cpu1 reads “a” before being acknowledged that other cpus have changed it. Although CPU0 can continue to execute before writing it to stored buffer, but CPU1 doesn’t know that.\nThe hardware designers cannot help directly here, since the CPUs have no idea which variables are related, let alone how they might be related\nTherefore, the hardware designers provide memory-barrier instructions to allow the software to tell the CPU about such relations. The program fragment must be updated to contain the memory barrier:\nvoid foo(void) { a=1; smp_mb(); b=1; } void bar(void) { while (b == 0) continue; assert(a == 1); } The memory barrier smp_mb() will cause the CPU to flush its store buffer before applying subsequent stores to their cache lines. The CPU could either\nsimply stall until the store buffer was empty before proceeding, or it could only use the store buffer to hold subsequent stores until all of the prior entries in the store buffer had been applied. This is to prevent other cpus from getting the subsequent value before getting the prior entries So that, CPU0 will\nwait for the “invalidate acknowledge” message of “a” before executing “b=1;” or\n(1)while executing smp_mb ,marks all current store-buffer entries (namely, the a=1) (2) while executing “b=1”, only stores it store buffer. (3) wait “invalidate acknowledge” of “a” (4) store the value of b in stored buffer and send a “invalidate” message 0x4 Optimize 2: Store Sequences Result in Unnecessary Stalls Once the stored buffer is full or a memory barrier is encountered, the CPU must once again wait for invalidations to complete in order to drain its store buffer before it can continue executing invalidate acknowledge messages can take so long: they must ensure that the corre- sponding cache line is actually invalidated, and this invalidation can be delayed if the cache is busy, for example, if the CPU is intensively loading and storing data, all of which resides in the cache. However, the CPU need not actually invalidate the cache line before sending the acknowledgement.\n0x42 Invalidate Queues and Invalidate Acknowledge A CPU with an invalidate queue may acknowledge an invalidate message as soon as it is placed in the queue, instead of having to wait until the corresponding line is actually invalidated.\n0x43 Invalidate Queues and Memory Barriers Thinking of the following code:\nvoid foo(void) { a=1; smp_mb(); b=1; } void bar(void) { while (b == 0) continue; assert(a == 1); } CPU0: execute foo a is shared state b is exclusive CPU1: execute bar Once again, the CPU designers cannot do much about this situation However, the memory-barrier instructions can interact with the invalidate queue. When a given CPU executes a memory barrier, it marks all the entries currently in its invalidate queue, and forces any subsequent load to wait until all marked entries have been applied to the CPU’s cache.\nvoid foo(void) { a=1; smp_mb(); b=1; } void bar(void) { while (b == 0) continue; smp_mb(); assert(a == 1); } So that, CPU0 will\n(1) executes the smp_mb(), marking the entry in its invalidate queue. (2) start executing the assert(a==1), but a is in the invalidate queue, CPU 1 must stall this load until that entry in the invalidate queue has been applied. 0x5 Summary: Read and Write Memory Barriers In the previous section, memory barriers were used to mark entries in both the store buffer and the inval- idate queue. But in our code fragment, foo() had no reason to do anything with the invalidate queue, and bar() similarly had no reason to do anything with the store queue.\nMany CPU architectures therefore provide weaker memory-barrier instructions that do only one or the other of these two.\nread memory barrier: marks only the invalidate queue forces any subsequent load to wait until all marked entries have been applied from the invalidate queue write memory barrier: marks only the store buffer. only use the store buffer to hold subsequent stores until all of the prior entries in the store buffer had been applied. void foo(void) { a=1; smp_wmb(); b=1; } void bar(void) { while (b == 0) continue; smp_rmb(); assert(a == 1); } ","permalink":"https://mobilephone724.github.io/article/mesi/","summary":"paper Introduction: Memory Barriers: a Hardware View for Software Hackers 0x0 why we need memory barrier In short, because reordering memory references allows much better performance, and so memory barriers are needed to force ordering in things like synchronization primitives whose correct operation depends on ordered memory references.\n0x1 Cache Structure 0x11 some cases of cache miss(not important) The cache miss means that the CPU will have to wait (or be “stalled”) for hundreds of cycles while the item is fetched from memory.","title":"MESI AND MEMORY_BARRIER: paper reading"},{"content":"0x0 Introduction A bitmap, also known as a bit array or bitset, is a data structure that represents a fixed-size sequence of bits. That is the value of the ith bit representing the existence of the the ith object. Bare bitmap can cost much memory according to the total substantial data size, even if we have stored little infomation. Roaring bitmap provide a new method to compress the bitmap structure.\n0x1 Related Infomation: blogs: (Very Important Introduction) [Blog of Vikram Oberoi]:A primer on Roaring bitmaps: what they are and how they work [blog of charlieroro] roaring bitmaps 【木东居士】：不深入而浅出 Roaring Bitmaps 的基本原理 paper Introduction: Better bitmap performance with Roaring bitmaps.pdf Opitmazition: Consistently faster and smaller compressed bitmaps with Roaring.pdf 0x2 Introduction TO Roaring Bitmap 0x21 two types of containers We partition the range of 32-bit indexes ([0, n)) into chunks of $2^{16}$ integers sharing the same 16 most significant digits. We use specialized containers to store their 16 least significant bits.\n(One chunk\u0026rsquo; size is up to 8KB, that is 4096 integers.)\nWhen a chunk contains no more than 4096 integers, we use a sorted array of packed 16-bit integers. When there are more than 4096 integers, we use a $2^{16}$​-bit bitmap. Thus, we have two types of containers: an array container for sparse chunks and a bitmap container for dense chunks.\nSince the size of a chunk is up to 8KB, we may save much memoy if the cardinality is small. Don\u0026rsquo;t worry about the memory allocator, it can deal with the small memory with local buffer. And I believe it\u0026rsquo;s the most important meaning of two types of containers.\n0x22 conversion between the two types of container timing\nWhen removing an integer, a bitmap container might become an array container if its cardinality reaches 4096. When adding an integer, an array container might become a bitmap container when its cardinality exceeds 4096. method\nWhen this happens, a new container is created with the updated data while the old container is discarded. Converting an array container to a bitmap container is done by creating a new bitmap container initialized with zeros, and setting the corresponding bits. To convert a bitmap container to an array container, we extract the location of the set bits using an optimized algorithm 0x23 index array To check for the presence of a 32-bit integer x, we first seek the container corresponding to $x/2^{16}$ using binary search. If a bitmap container is found, we access the (x mod $2^{16}$)th bit. If an array container is found, we use a binary search again\nThe containers are stored in a dynamic array with the shared 16 most-significant bits: this serves as a first-level index. The array keeps the containers sorted by the 16 most-significant bits.\n0x3 set operations There are\nTwo basic opertions: union (bitwise OR) and intersection (bitwise AND); And three container type combinations: bitmap vs bitmap, array vs array annd bitmap vs array 0x31 bitmap vs bitmap union operation(the result must be a bitmap container) :\nIt might seem like computing bitwise ORs and computing the cardinality of the result\nwould be significantly slower than merely computing the bitwise ORs. However, four factors mitigate this potential problem\n[built in cpu instructions]: popular processors (Intel, AMD, ARM) have fast instructions to compute the number of ones in a word. Intel and AMD’s popcnt instruction has a throughput as high as one operation per CPU cycle. [Java Opitimization]: Recent Java implementations translate a call to Long.bitCount into such fast instructions. [superscalar]: Popular processors are superscalar: they can execute several operations at once. Thus, while we retrieve the next data elements, compute their bitwise OR and store it in memory, the processor can apply the popcnt instruction on the last result and increment the cardinality counter accordingly. [enough L1 cache]: For inexpensive data processing operations, the processor may not run at full capacity due to cache misses. intersection operation:\nFor computing intersections, we use a less direct route. First, we compute the cardinality of the result, using 1024 bitwise AND instructions. If the cardinality is larger than 4096, then we proceed as with the union, writing the result of bitwise ANDs to a new bitmap container. Otherwise, we create a new array container. We extract the set bits from the bitwise ANDs on the fly. See chapter \u0026ldquo;0x22\u0026rdquo; for detail\n0x32 bitmap vs array intersection(the result must be an array container)：we iterate over the sorted dynamic array, and verify the existence of each 16-bit integer in the bitmap container. The result is written out to an array container Unions(the result must be a bit map container)：we create a copy of the bitmap and simply iterate over the array, setting the corresponding bits 0x33 Array vs Array For unions: if the sum of the cardinalities is no more than 4096(the result must be an array container): we use a merge algorithm between the two arrays otherwise: Otherwise, we set the bits corresponding to both arrays in a bitmap container. We then compute the cardinality using fast instructions. If the cardinality is no more than 4096, we convert the bitmap container to an array containe. intersection(the result must be an array container): if the two arrays have cardinalities that differ by less than a factor of 64: merge otherwise: galloping intersection Galloping is superior to a simple merge when one array ($r$) is much smaller than other one ($f$) because it can skip many comparisons. Starting from the beginning of both arrays, we pick the next available integer $r_i$ from the small array $r$ and seek an integer at least as large $f_j$ in the large array $f$ , looking first at the next value, then looking at a value twice as far, and so on. Then, we use binary search to advance in the second list to the first value larger or equal to $r_i$ .\nGalloping(exponential search) Introduction\nThe initial value of bound can alway advance in each search.\n// Returns the position of key in the array arr of length size. template \u0026lt;typename T\u0026gt; int exponential_search(T arr[], int size, T key) { if (size == 0) { return NOT_FOUND; } int bound = 1; while (bound \u0026lt; size \u0026amp;\u0026amp; arr[bound] \u0026lt; key) { bound *= 2; } return binary_search(arr, key, bound/2, min(bound + 1, size)); } 0x34 in place operations When computing the union between two bitmap containers, we can modify one of the two bitmap containers instead of generating a new bitmap container. Similarly, for the intersection between two bitmap containers, we can modify one of the two containers if the cardinality of the result exceeds 4096 When computing the union between an array and a bitmap container, we can write the result to the bitmap container, by iterating over the values of the array container and setting the corresponding bits in the bitmap container. We can update the cardinality each time by checking whether the word value has been modified. 0x4 The \u0026ldquo;run\u0026rdquo; type container 0x41 To introduction to \u0026ldquo;run\u0026rdquo; The original Roaring has a limitation in some scenarios because it does not compress long runs of values. Indeed, given a bitset made of a few long runs (e.g., all integers in [10, 1000]), Roaring—as presented so far—can only offer suboptimal compression. If we consider the case of a bitmap made of all integers in [10, 1000], Roaring without support for runs would use 8 kB, whereas a few bytes ought to suffice.\nSuch unnecessarily large bitmaps can stress memory bandwidth. computing the intersection of two bitmaps representing the ranges [10, 1000] and [500, 10000] can be done in a few cycles when using RLE-compressed bitmaps. But the original Roaring would require intersecting two bitmap containers and possibly thousands of cycles. See chapter \u0026ldquo;0xF1\u0026rdquo; for detail. To solve this problem, we decided to add a third type of container to Roaring, one that is ideally suited to coding data made of runs of consecutive values. The new container is conceptually simple: **given a run (e.g., [10, 1000]), we store the starting point (10) and its length minus one (990). By packing the starting points and the lengths in pairs, using 16 bits each, **we preserve the ability to support fast random access by binary search through the coded runs\nThe run container, is made of a packed array of pairs of 16-bit integers. The first value of each pair represents a starting value, whereas the second value is the length of a run. For example, we would store the values 11, 12, 13, 14, 15 as the pair 11, 4 where 4 means that beyond 11 itself, there are 4 contiguous values that follow.\nIn addition to this packed array, we need to maintain the number of runs stored in the packed array. Like the array container, the run container is stored in a dynamic array. During serialization, we write out the number of runs, followed by the corresponding packed array.\n0x42 Decide The Best Container To decide the best container type, we are motivated to minimize storage. In serialized form, a run container uses 2 + 4r bytes(16-bit integer is 2 bytes and we need a pair; plus the number of runs) given r runs, a bitmap container always uses 8192 bytes and an array container uses 2c + 2 bytes, where c is the cardinality. Therefore, we apply the following rules:\nAll array containers are such that they use no more space than they would as a bitmap container: they contain no more than 4096 values. Bitmap containers use less space than they would as array containers: they contain more than 4096 values. A run container is only allowed to exist if it is smaller than either the array container or the bitmap container that could equivalently store the same values. If the run container has cardinality greater than 4096 values, then the number of runs must be no more than $\\lceil(8192 − 2)/4\\rceil = 2047$ runs. (Or it must be converted to a bitmap container) If the run container has cardinality no more than 4096, then the number of runs must be less than half the cardinality. (Or it must be converted to an array container) **So, the critical step in deciding whether an array or bitmap container should be converted to a run container is to count the number of runs of consecutive numbers it contains. **\n0x43 Compute The Number Of Runs For array containers, we count this number by iterating through the 16-bit integers and comparing them two by two in a straightforward manner. Because array containers have at most 4096 integers, this computation is expected to be fast.\nFor bitmap containers, the below algorithm shows how to compute the number of runs.\nWe can illustrate the core operation of the algorithm using a single 32-bit word containing 6 runs of consecutive ones:\nWe can verify that $\\mathrm{bitCount}((C_i \\ll 1)\\ \\mathrm{ANDNOT}\\ C_i) = 6$, that is, we have effectively computed the number of runs. ($a\\ \\mathrm{ANDNOT}\\ b$is true iff a=1 and b=0) In the case where a run continues up to the left-most bit, and does not continue in the next word, it does not get counted, but we add another term (($C_i \\gg 63$) ANDNOT $C_i+1$ when using 64-bit words) to check for this case. Nevertheless, the computation may be expensive—exceeding the cost of computing the union or intersection between two bitmap containers. Thus, instead of always computing the number of runs exactly, we rely on the observation that no bitmap container with more than 2047 runs should be converted. As soon as we can produce a lower bound exceeding 2047 on the number of runs, we can stop. An exact computation of the number of runs is important only when our lower bound is less than 2048. In short: estimate the lower bound count of runs first, and only do the precise computation if the lower bound is less than 2048.\nThere are several method to implement the heuristic algorithm, and see the paper for details.\n0x44 Logical operations There are many necessary logical operations, but we present primarily the union and intersection.\n0x441 Bitmap vs Bitmap: 0xF appendix 0xF1 RLE-based compressed bitmaps There are many RLE-based compression formats.\n0xF11 Introduction To WAH For example, WAH organizes the data in literal and fill words.\nLiteral words contain a mix of W − 1 zeros and ones (e.g., $01011 · · · 01$) where W denotes the word size in bits: typically W = 32 or W = 64. Fill words are made of just W − 1 ones or just W − 1 zeros (i.e., $11 · · · 11$ or $00 · · · 00$). WAH compresses sequences of consecutive identical fill words The most significant bit of each word distinguishes between fill and literal words\nWhen it is set to one, the remaining W −1 bits store the W −1 bits of a literal word. When it is set to zero, the second most significant bit indicates the bit value whereas the remaining bits are used to store the number of consecutive identical fill words (the run length) 0xF12 Introduction To Concise Concise is a variation that reduces the memory usage when the bitmap is _moderately sparse. _Instead of storing the run length using $W − 2$ bits, Concise uses only $W − 2 − \\lceil log2(W )\\rceil$ bits to indicate a run length $r$, reserving $\\lceil log2(W )\\rceil$ bits to store a value $p$. When $p$ is non-zero, we decode $r$ fill words, plus a single $W − 1$ bit word with its $p^{th}$ bit flipped.\nBelow is an example: ","permalink":"https://mobilephone724.github.io/article/roaring_bitmap/","summary":"0x0 Introduction A bitmap, also known as a bit array or bitset, is a data structure that represents a fixed-size sequence of bits. That is the value of the ith bit representing the existence of the the ith object. Bare bitmap can cost much memory according to the total substantial data size, even if we have stored little infomation. Roaring bitmap provide a new method to compress the bitmap structure.","title":"roaring bitmap"},{"content":"Download through git See official docs for detail. Below is a simple example:\nexport user=dev export src_dir=postgresql export build_dir=/home/${user}/build export data_dir=/home/${user}/data export superuser=postgres export defaultdb=test ${build_dir}/bin/pg_ctl -D ${data_dir} stop rm -rf ${build_dir} rm -rf ${data} cd ~ #start from home/${user} git clone https://git.postgresql.org/git/postgresql.git cd ${src_dir} git clean -xdf # may be too dangerous # delete for add some configures accordingly ./configure \\ --prefix=${build_dir} \\ --enable-cassert \\ --with-tcl \\ --with-perl \\ --with-python \\ --enable-debug \\ --without-icu \\ --with-openssl \\ CC=/usr/bin/gcc \\ CFLAGS=\u0026#39;-O0 -pipe -Wall -g3\u0026#39; make -j8 \u0026amp;\u0026amp; make install make -C contrib install ${build_dir}/bin/initdb --username=${superuser} --pgdata=${data_dir} ${build_dir}/bin/pg_ctl -D ${data_dir} -l ${data_dir}/logfile start ${build_dir}/bin/psql -U${superuser} postgres -c \u0026#34;create database ${defaultdb};\u0026#34; echo \u0026#34;----------------- all finished -----------------------\u0026#34; echo \u0026#34;use ************** \u0026#34; echo \u0026#34;[ ${build_dir}/bin/psql -U${superuser} ${defaultdb} ] \u0026#34; echo \u0026#34;to connect postgresql\u0026#34; cd .. ‍\n","permalink":"https://mobilephone724.github.io/posts/archive_posts/build_from_source/","summary":"Download through git See official docs for detail. Below is a simple example:\nexport user=dev export src_dir=postgresql export build_dir=/home/${user}/build export data_dir=/home/${user}/data export superuser=postgres export defaultdb=test ${build_dir}/bin/pg_ctl -D ${data_dir} stop rm -rf ${build_dir} rm -rf ${data} cd ~ #start from home/${user} git clone https://git.postgresql.org/git/postgresql.git cd ${src_dir} git clean -xdf # may be too dangerous # delete for add some configures accordingly ./configure \\ --prefix=${build_dir} \\ --enable-cassert \\ --with-tcl \\ --with-perl \\ --with-python \\ --enable-debug \\ --without-icu \\ --with-openssl \\ CC=/usr/bin/gcc \\ CFLAGS=\u0026#39;-O0 -pipe -Wall -g3\u0026#39; make -j8 \u0026amp;\u0026amp; make install make -C contrib install ${build_dir}/bin/initdb --username=${superuser} --pgdata=${data_dir} ${build_dir}/bin/pg_ctl -D ${data_dir} -l ${data_dir}/logfile start ${build_dir}/bin/psql -U${superuser} postgres -c \u0026#34;create database ${defaultdb};\u0026#34; echo \u0026#34;----------------- all finished -----------------------\u0026#34; echo \u0026#34;use ************** \u0026#34; echo \u0026#34;[ ${build_dir}/bin/psql -U${superuser} ${defaultdb} ] \u0026#34; echo \u0026#34;to connect postgresql\u0026#34; cd .","title":"Build PostgreSQL From Source"},{"content":"In PostgreSQL, the adding and dropping a column is an instant ddl(This name seems only to be used in mysql, but I like it). In this article, I try to explain the implement of that.\nThe reference:\nhttps://www.postgresql.org/docs/current/sql-altertable.html Basic Concepts instant ddl For a table with $n$ tuples, if a ddl post can be performed in time $O(1)$ ,we call this ddl instant. So to implement an instant ddl, the data organization must remain unchanged. Instead, only the schema information can be changed, along withthe method to used to interpret the table\u0026rsquo;s binary data according to the schema.\nIn this scenario, pg only changes the pg_attribute catalog, which records the attributes[#todo is this OK] of each relations.\nheap page representation Before illustrate the situation where interpreting the binary data with two different schemas, we figure out the way to organize the heap pages.\n","permalink":"https://mobilephone724.github.io/posts/archive_posts/column-schema-change/","summary":"In PostgreSQL, the adding and dropping a column is an instant ddl(This name seems only to be used in mysql, but I like it). In this article, I try to explain the implement of that.\nThe reference:\nhttps://www.postgresql.org/docs/current/sql-altertable.html Basic Concepts instant ddl For a table with $n$ tuples, if a ddl post can be performed in time $O(1)$ ,we call this ddl instant. So to implement an instant ddl, the data organization must remain unchanged.","title":"Column Schema Change"},{"content":"Overview In application level, ”PostgreSQL“ has native supports for using SSL connections. This requires that OpenSSL is installed on both client and server systems and that support in PostgreSQL is enabled at build time.\nWith SSL, we can:\nEncrypted data on Internet transmission Allow client to authorize the server(PostgreSQL), which can protect the client from connecting to the attacker’s server Allow server to authorize the client, which can stop the attacker from connecting to the database even if password leak. Just encrypt internet transmission build binary from source just configure with -with-openssl option. You may need to install ssl-dev tools first\nsudo apt-get install libssl-dev Below is a building example:\nexport build_dir=/home/dev/build export data_dir=/home/dev/data export superuser=postgres export defaultdb=test ${build_dir}/bin/pg_ctl -D ${data_dir} stop rm -rf build rm -rf data cd postgresql git clean -xdf ./configure \\ --prefix=${build_dir} \\ --enable-debug \\ --enable-cassert \\ --with-tcl \\ --with-perl \\ --with-python \\ --enable-debug \\ --without-icu \\ --with-openssl \\ CC=/usr/bin/gcc \\ CFLAGS=\u0026#39;-O0 -pipe -Wall -g3\u0026#39; # export CLFAGS=\u0026#34;-O0 -Wall -g3\u0026#34; # export CPPLFAGS=\u0026#34;-O0 -Wall -g3\u0026#34; make -j8 \u0026amp;\u0026amp; make install make -C contrib install ${build_dir}/bin/initdb --username=${superuser} --pgdata=${data_dir} ${build_dir}/bin/pg_ctl -D ${data_dir} -l ${data_dir}/logfile start ${build_dir}/bin/psql -U${superuser} postgres -c \u0026#34;create database ${defaultdb};\u0026#34; echo \u0026#34;----------------- all finished -----------------------\u0026#34; echo \u0026#34;use ************** \u0026#34; echo \u0026#34;[ ${build_dir}/bin/psql -U${superuser} ${defaultdb} ] \u0026#34; echo \u0026#34;to connect postgresql\u0026#34; cd .. Configure ssl on server prepare a certification use openssl command to generate one. The 127.0.0.1 means that the certification only protects localhost connections\n# generate root certification openssl req -new -x509 -days 3650 -nodes \\ -out ca.crt -keyout ca.key -subj \u0026#34;/CN=root-server-ca\u0026#34; # generate csr and key openssl req -new -nodes -text -out server.csr \\ -keyout server.key -subj \u0026#34;/CN=127.0.0.1\u0026#34; # generate certification openssl x509 -req -in server.csr -text -days 365 \\ -CA ca.crt -CAkey ca.key -CAcreateserial -out server.crt configure in $PGDATA copy the key and crt to $PGDATA\nexport $PGDATA=/home/dev/data cp server.key $PGDATA/. cp server.crt $PGDATA/. configure in postgresql.conf\nssl = on ssl_cert_file = \u0026#39;server.crt\u0026#39; ssl_key_file = \u0026#39;server.key\u0026#39; And (re)start the server\nconnect and test psql \u0026#34;host=127.0.0.1 port=5432 dbname=postgres user=postgres sslmode=require\u0026#34; Server sides Authorization Note that the client hasn’t check the certification of the server now. Check in this way:\nPGSSLROOTCERT=ca.crt \\ psql \u0026#34;host=127.0.0.1 port=5432 dbname=postgres user=postgres sslmode=require\u0026#34; Client sides Authorization Generate certification similarly\nopenssl req -new -x509 -days 3650 -nodes \\ -out ca-client.crt -keyout ca-client.key -subj \u0026#34;/CN=root-client-ca\u0026#34; openssl req -new -nodes -text -out client.csr \\ -keyout client.key -subj \u0026#34;/CN=postgres\u0026#34; openssl x509 -req -in client.csr -text -days 365 \\ -CA ca-client.crt -CAkey ca-client.key -CAcreateserial -out client.crt prepare in $PGDATA\ncp ca-client.crt $PGDATA/. echo -e \u0026#34;\\nssl_ca_file = \u0026#39;ca-client.crt\u0026#39;\u0026#34; \u0026gt;\u0026gt; $PGDATA/postgresql.conf and restart\ntest connection Before connection, remember to set pg_hba.conf to only authorized with cetification.\nhostssl all all 127.0.0.1/32 cert psql \u0026#34;sslrootcert=ca.crt sslcert=client.crt sslkey=client.key \\ host=127.0.0.1 dbname=postgres user=postgres sslmode=verify-full\u0026#34; ","permalink":"https://mobilephone724.github.io/posts/archive_posts/ssl-in-pg/","summary":"Overview In application level, ”PostgreSQL“ has native supports for using SSL connections. This requires that OpenSSL is installed on both client and server systems and that support in PostgreSQL is enabled at build time.\nWith SSL, we can:\nEncrypted data on Internet transmission Allow client to authorize the server(PostgreSQL), which can protect the client from connecting to the attacker’s server Allow server to authorize the client, which can stop the attacker from connecting to the database even if password leak.","title":"SSL in PG"},{"content":"sequence type background From official documents:\n9.17. Sequence Manipulation Functions\nCREATE SEQUENCE\nSequence objects are special single-row tables created with CREATE SEQUENCE. Sequence objects are commonly used to generate unique identifiers for rows of a table. The sequence functions, provide simple, multiuser-safe methods for obtaining successive sequence values from sequence objects. Main function There is no much concerns about these functions\nnextval\nAdvances the sequence object to its next value and returns that value setval\nSome examples SELECT setval(\u0026lsquo;myseq\u0026rsquo;, 42); Next nextval will return 43 SELECT setval(\u0026lsquo;myseq\u0026rsquo;, 42, true); Same as above SELECT setval(\u0026lsquo;myseq\u0026rsquo;, 42, false); Next nextval will return 42 currval\nReturns the value most recently obtained by nextval for this sequence in the current session lastval\nReturns the value most recently returned by nextval in the current session. This function is identical to currval, except that instead of taking the sequence name as an argument it refers to whichever sequence nextval was most recently applied to in the current session. ** There is a caution: **\nThere is no rollback of the sequence type. Official document is post below:\nTo avoid blocking concurrent transactions that obtain numbers from the same sequence, the value obtained by nextval is not reclaimed for re-use if the calling transaction later aborts. This means that transaction aborts or database crashes can result in gaps in the sequence of assigned values. That can happen without a transaction abort, too. For example an INSERT with an ON CONFLICT clause will compute the to-be-inserted tuple, including doing any required nextval calls, before detecting any conflict that would cause it to follow the ON CONFLICT rule instead. Thus, PostgreSQL sequence objects cannot be used to obtain “gapless”（无缝的） sequences.\nMost important All things above doesn’t worth a post, but the replication hack of this type does. Considering a master-standby example, the currval in standby is always bigger than the master\u0026rsquo;s. And once the value in master advances and the new value doesn\u0026rsquo;t precede the standby\u0026rsquo;s one, the currval in standby would\u0026rsquo;t advanced immediately. This is a greate skill to reduce the wal records.\n","permalink":"https://mobilephone724.github.io/posts/archive_posts/sequence_type/","summary":"sequence type background From official documents:\n9.17. Sequence Manipulation Functions\nCREATE SEQUENCE\nSequence objects are special single-row tables created with CREATE SEQUENCE. Sequence objects are commonly used to generate unique identifiers for rows of a table. The sequence functions, provide simple, multiuser-safe methods for obtaining successive sequence values from sequence objects. Main function There is no much concerns about these functions\nnextval\nAdvances the sequence object to its next value and returns that value setval","title":"pg_squeence_type"},{"content":"Abstract a protocol for schema evolution in a globally distributed database management system with shared data, stateless servers, and no global membership. asynchronous all servers can access and update all data during a schema change INTRODUCTION Schema evolution: the ability to change a database’s definition without the loss of data F1 is built on top of Spanner, a globally distributed KV data store Main feature The main features of F1 that impact schema changes are:\nMassively distributed: An instance of F1 consists of hundreds of individual F1 servers Relational schema: Each F1 server has a copy of a relational schema that describes tables, columns, indexes, and constraints. Any modification to the schema requires a distributed schema change to update all servers Shared data storage: All F1 servers in all datacenters have access to all data stored in Spanner. Stateless servers: F1 servers must tolerate machine failures, preemption(取代), and loss of access to network resources clients may connect to any F1 server, even for different statements in the same transaction. No global membership: no reliable mechanism for determining currently running F1 servers, and explicit global synchronization is not possible several constraints on the schema change process:\nFull data availability: the availability of the data managed by F1 is paramount(至为重要的) it is unacceptable to take even a portion of the database offline during a schema change (e.g., locking a column to build an index). Minimal performance impact: the F1 schema changes rapidly to support new features Asynchronous schema change In other words, different F1 servers may transition to using a new schema at different times These requirements influenced the design in several ways\nSince all data must be as available as possible, we do not restrict access to data undergoing reorganization. Because the schema change must have minimal impact on user transactions, we allow transactions to span an arbitrary number of schema changes, although we do not automatically rewrite queries to conform to the schema in use Applying schema changes asynchronously on individual F1 servers means that multiple versions of the schema may be in use simultaneously An example Consider a schema change from schema S1 to schema S2 that adds index I on table R Assume two different servers, M1 and M2, execute the following sequence of operations: Server M2, using schema S2, inserts a new row r to table R. Because S2 contains index I, server M2 also adds a new index entry corresponding to r to the key– value store. Server M1, using schema S1, deletes r. Because S1 does not contain I, M1 removes r from the key–value store but fails to remove the corresponding index entry in I. The second delete leaves the database corrupt. We consider not only changes to the logical schema, such as the addition or removal of columns, but also changes to the physical schema like adding or removing secondary indexes.\nBy ensuring that:\nno more than two schema versions are in use at any given time those schema versions have specific properties? enables distributed schema changes in a way that\ndoes not require global membership, implicit or explicit synchronization between nodes, or the need to retain old schema versions once a schema change is complete\nBackGround In this section, we:\nseparate the interface provided by the key–value store from its implementation show how we map traditional relational database features into this unique setting Key-value store F1 assumes the key–value store supports three operations put: insert a value with a given key del: delete a value with a given key get: returns any stored values whose key matches a given prefix Note that put and del reference exactly one key–value pair, while get may return multiple key–value pairs\nTwo more requirements Commit timestamps: Every key–value pair has a last-modified timestamp which is updated atomically by the key–value store Atomic test-and-set support: Multiple get and put operations can be executed atomically Relational schema An F1 schema is a set of table definitions that enable F1 to interpret the database located in the key–value store Each table definition has: a list of columns a list of secondary indexes a list of integrity constraints(foreign key or index uniqueness constraints) a list of optimistic locks. required columns that cannot be read directly by client transactions A subset of columns in a table forms the primary key of the table We call a column required if its value must be present in every row. All primary-key columns are implicitly required, while non-key columns may be either required or optional Row representation one pair for each non-primary-key column\nEach key logically includes\nthe name of the table, the primary key values of the containing row, the name of the column whose value is stored in the pair Although this appears to needlessly repeat all primary key values in the key for each column value, in practice, F1’s physical storage format eliminates this redundancy\nA secondary index covers a non-empty subset of columns on a table is itself represented by a set of key–value pairs in the key– value store Each row in the indexed table has an associated index key–value pair The key for this pair is formed by concatenating the table name the index name the row’s indexed column values and the row’s primary key values We denote the index key for row $r$ in index $I$ as $k_r(I)$ the special exists column doesn\u0026rsquo;t have the associated value Relational operations F1 supports a set of standard relational operations:\n$insert(R,vk_r,vc_r)$ inserts row r to table R with primary key values $vk_r$ and non-key column values $vc_r$. Insert fails if a row with the same primary key values already exists in table R. $delete (R, vk_r )$ $update(R,vk_r,vc_r)$ $query(\\vec{R},\\vec{C},P)$ :returns a projection $\\vec{C}$ of rows from tables in $\\vec{R}$ that satisfy predicate $P$ . ","permalink":"https://mobilephone724.github.io/posts/archive_posts/google-f1/","summary":"Abstract a protocol for schema evolution in a globally distributed database management system with shared data, stateless servers, and no global membership. asynchronous all servers can access and update all data during a schema change INTRODUCTION Schema evolution: the ability to change a database’s definition without the loss of data F1 is built on top of Spanner, a globally distributed KV data store Main feature The main features of F1 that impact schema changes are:","title":"Read Google F1"},{"content":"Primitive Operation if Transactions There are three address spaces that transaction interact in important ways:\nThe space of disk blocks holding the database elements. The memory address space managed by buffer manager. The local address space of the transaction. To describe a transaction, we need some operation notions:(X below is a database element while t is a local varible, and we suppose a database element is no larger than a single block)\nINPUT(X): copy disk block containing X to memroy buffer READ(X, t): Copy X to transaction\u0026rsquo;s local varible t no matter where X is, which means INPUT(X) may be executed first before READ(X,t). WRITE(X, t): Copy the value of t to X no matter where X is. OUTPUT(X): Copy the block containing X from its buffer to disk undo logging Undo log makes repairs to the database state by undoing the effects of transactions that may not completed before the crash.\nAn Undo log has the form [T,X,v] which means transaction T has changed the database elememnt X, and its before value was v. The log record is a response to a WRITE action into memory, not an OUTPUT action.\nAn undo log is suffcient to allow recovery from system failure, provided transactions and buffer manager obey two rules:\nIf transaction T modifies database element X, then the log record of form \u0026lt;T,X,v\u0026gt; must be written to disk before the new value of X is written to disk If a transaction commits, the its commit log record must be written to disk only after all database elements changed by the transaction have been written to disk, but as soon there after is possible If a transaction aborts, recovery manager is need to repair the values. When recovering, the recovery manager scan the log from the end. As it travels, it remembers all thos transactions T for which it has seen a [COMMIT T] record or [ABORT T] record, the:\nIf T\u0026rsquo;s COMMIT record is found, do nothing. Otherwise, T is an incomplete transaction, or aborted transaction. The recovery manager must change the value of X in the database to v,in case X had been altered just before the crash. After this, the recovery manager must write a log record [ABORT T] for each incomplete transaction T that was not previously aborted, and then flush the log ","permalink":"https://mobilephone724.github.io/posts/archive_posts/database-log/","summary":"Primitive Operation if Transactions There are three address spaces that transaction interact in important ways:\nThe space of disk blocks holding the database elements. The memory address space managed by buffer manager. The local address space of the transaction. To describe a transaction, we need some operation notions:(X below is a database element while t is a local varible, and we suppose a database element is no larger than a single block)","title":"Basic Knowledge of Database Log"},{"content":"Definiteness： Suppose Alice has number $i$ and Bob has number $j$ and $1\\leq i,j \\leq 9$. We need a protocol for them to decide whether $i \u0026lt; j$ in the end(aside from their own values)\nSolution: Let $M$ be the set of all $N$-bit nonnegative integers\nLet $Q_N$ be the set of all one-one and onto function from $M$ to $M$\nAlice generates a public key from $Q_N$, called $E_a$, and the inverse function of $E_a$ is $D_a$ Bob picks a random value $x \\in M$, compute $k = E_a(x)$, then send $k - j$ to Alice Alice computes $y_u=D_a(k - j + u)$ for $u = 1,2,\\dots,9$ Alice generates a random prime $p$ of $N/2$-bit, and computes $z_u=y_u(\\mod p)$ for all $u$. Alice repeats step 4 until all $z_u$ differ by at least 2 in the $\\mod p$ sense Alice sends the $p$ and $z_1,z_2,\\dots,z_i,z_{i+1}+1,\\dots,z_{9} +1$ (all in $\\mod p$ sense)to Bob Bob looks at the $j$-th value(not counting p) sent from Alice, and decides that $i\\geq j$ if it is equal to $x \\mod p$, or $i\u0026lt;j$ otherwise ","permalink":"https://mobilephone724.github.io/posts/archive_posts/howtoknowwhoseisbigger/","summary":"Definiteness： Suppose Alice has number $i$ and Bob has number $j$ and $1\\leq i,j \\leq 9$. We need a protocol for them to decide whether $i \u0026lt; j$ in the end(aside from their own values)\nSolution: Let $M$ be the set of all $N$-bit nonnegative integers\nLet $Q_N$ be the set of all one-one and onto function from $M$ to $M$\nAlice generates a public key from $Q_N$, called $E_a$, and the inverse function of $E_a$ is $D_a$ Bob picks a random value $x \\in M$, compute $k = E_a(x)$, then send $k - j$ to Alice Alice computes $y_u=D_a(k - j + u)$ for $u = 1,2,\\dots,9$ Alice generates a random prime $p$ of $N/2$-bit, and computes $z_u=y_u(\\mod p)$ for all $u$.","title":"Alice and Bob how to know whose number is bigger without giving away their own's"},{"content":"The functions described in this chapter are often referred to as unbuffered I/O(which each read or write invokes a system call in the kernel), in contrast to the standard I/O routines\nFile Descriptors To the kernel, all open files are referred to by file descriptors. A file descriptor is a non-negative integer. When we open an existing file or create a new file, the kernel returns a file descriptor to the process. When we want to read or write a file, we identify the file with the file descriptor that was returned by open or creat as an argument to either read or write. By convention, UNIX System shells associate file descriptor 0 with the standard input of a process, file descriptor 1 with the standard output, and file descriptor 2 with the standard error File descriptors range from 0 through OPEN_MAX−1 open and openat code #include \u0026lt;fcntl.h\u0026gt; int open(const char *path, int oflag, ... /* mode_t mode */ ); int openat(int fd, const char *path, int oflag, ... /* mode_t mode */ ); //Both return: file descriptor if OK, −1 on error This function has a multitude of options, which are specified by the oflag argument. This argument is formed by ORing together one or more of the following constants from the\u0026lt;fcntl.h\u0026gt; header\nO_SYNC Have each write wait for physical I/O to complete, including I/O necessary to update file attributes modified as a result of the write. O_DSYNC Have each write wait for physical I/O to complete, but don’t wait for file attributes to be updated if they don’t affect the ability to read the data just written. The O_DSYNC flag affects a file’s attributes only when they need to be updated to reflect a change in the file’s data (for example, update the file’s size to reflect more data)\nO_RSYNC Have each read operation on the file descriptor wait until any pending writes for the same portion of the file are complete\nThe fd parameter distinguishes the openat function from the open function. There are three possibilities\nThe path parameter specifies an absolute pathname. In this case, the fd parameter is ignored and the openat function behaves like the open function. The path parameter specifies a relative pathname and the fd parameter is a file descriptor that specifies the starting location in the file system where the relative pathname is to be evaluated. The fd parameter is obtained by opening the directory where the relative pathname is to be evaluated. The path parameter specifies a relative pathname and the fd parameter has the special value AT_FDCWD. In this case, the pathname is evaluated starting in the current working directory and the openat function behaves like the open function. openat solved two problems\nIt gives threads a way to use relative pathnames to open files in directories other than the current working directory while all threads in the same process share the same current working directory, so this makes it difficult for multiple threads in the same process to work in different directories at the same time it provides a way to avoid time-of-check-to-time-of-use (TOCTTOU) errors whose baisc idea is that a program is vulnerable if it makes two file-based function calls where the second call depends on the results of the first call. Because the two calls are not atomic, the file can change between the two calls, thereby invalidating the results of the first call, leading to a program error. creat function #include \u0026lt;fcntl.h\u0026gt; int creat(const char *path, mode_t mode); This is equivalent to\nopen(path, O_WRONLY | O_CREAT | O_TRUNC, mode); close function #include \u0026lt;unistd.h\u0026gt; int close(int fd); When a process terminates, all of its open files are closed automatically by the kernel\nlseek function Every open file has an associated \u0026lsquo;\u0026rsquo;current file offset,’’ normally a non-negative integer that measures the number of bytes from the beginning of the file.Read and write operations normally start at the current file offset and cause the offset to be incremented by the number of bytes read or written An open file’s offset can be set explicitly by calling lseek #include \u0026lt;unistd.h\u0026gt; off_t lseek(int fd, off_t offset, int whence); If whence is SEEK_SET, the file’s offset is set to offset bytes from the beginning of the file If whence is SEEK_CUR, the file’s offset is set to its current value plus the offset. The offset can be positive or negative If whence is SEEK_END, the file’s offset is set to the size of the file plus the offset. The offset can be positive or negative Because a successful call to lseek returns the new file offset, we can seek zero bytes from the current position to determine the current offset off_t currpos; currpos = lseek(fd, 0, SEEK_CUR); This technique can also be used to determine if a file is capable of seeking. If the file descriptor refers to a pipe, FIFO, or socket, lseek sets errno to ESPIPE and returns −1 Normally,a file’s current offset must be a non-negative integer . Because negative offsets are possible, we should be careful to compare the return value from lseek as being equal to or not equal to −1, rather than testing whether it is less than 0. The file’s offset can be greater than the file’s current size, in which case the next write to the file will extend the file. This is referred to as creating a hole in a file and is allowed. Any bytes in a file that have not been written are read back as 0. A hole in a file isn’t required to have storage backing it on disk read function #include \u0026lt;unistd.h\u0026gt; ssize_t read(int fd, void *buf, size_t nbytes); If the read is successful, the number of bytes read is returned. If the end of file is encountered, 0 is returned. There are several cases in which the number of bytes actually read is less than the amount requested:\nWhen reading from a regular file, if the end of file is reached before the requested number of bytes has been read. When reading from a terminal device. When reading from a network When reading from a pipe or FIFO. When reading from a record-oriented device When interrupted by a signal and a partial amount of data has already been read. classic definition int read(int fd, char *buf, unsigned nbytes); difference\nvoid * to char * 0 for end-of-file and -1 for an error write function #include \u0026lt;unistd.h\u0026gt; ssize_t write(int fd, const void *buf, size_t nbytes) The return value is usually equal to the nbytes argument; otherwise, an error has occurred. A common cause for a write error is either filling up a disk or exceeding the file size limit for a given process\nI/O efficiency an example\n#include \u0026#34;apue.h\u0026#34; #define BUFFSIZE 4096 int main(void) { int n; char buf[BUFFSIZE]; while ((n = read(STDIN_FILENO, buf, BUFFSIZE)) \u0026gt; 0) if (write(STDOUT_FILENO, buf, n) != n) err_sys(\u0026#34;write error\u0026#34;); if (n \u0026lt; 0) err_sys(\u0026#34;read error\u0026#34;); exit(0); } some caveats\nIt reads from standard input and writes to standard output, assuming that these have been set up by the shell before this program is executed The program doesn’t close the input file or output file. This example works for both text files and binary file how we chose the BUFFSIZE value?\nfile sharing The UNIX System supports the sharing of open files among different processes.\nThe kernel uses three data structures to represent an open file, and the relationships among them determine the effect one process has on another with regard to file sharing\nEvery process has an entry in the process table. Within each process table entry is a table of open file descriptors, which we can think of as a vector, with one entry per descriptor. Associated with each file descriptor are The file descriptor flags A pointer to a file table entry The kernel maintains a file table for all open files. Each file table entry contains The file status flags for the file, such as read, write, append, sync, and nonblocking; more on these in Section 3.14 The current file offset A pointer to the v-node table entry for the file Each open file (or device) has a v-node structure that contains information about the type of file and pointers to functions that operate on the file. For most files, the v-node also contains the i-node for the file. This information is read from disk when the file is opened, so that all the pertinent information about the file is readily available. For example, the i-node contains the owner of the file, the size of the file, pointers to where the actual data blocks for the file are located on disk, and so on If two independent processes have the same file open, we could have the arrangement\nEach process that opens the file gets its own file table entry, but only a single v-node table entry is required for a given file. One reason each process gets its own file table entry is so that each process has its own current offset for the file.\nAfter each write is complete, the current file offset in the file table entry is incremented by the number of bytes written. If this causes the current file offset to exceed the current file size, the current file size in the i-node table entry is set to the current file offset (for example, the file is extended). If a file is opened with the O_APPEND flag, a corresponding flag is set in the file status flags of the file table entry. Each time a write is performed for a file with this append flag set, the current file offset in the file table entry is first set to the current file size from the i-node table entry. This forces every write to be appended to the current end of file. If a file is positioned to its current end of file using lseek, all that happens is the current file offset in the file table entry is set to the current file size from the i-node table entry (Note that this is not the same as if the file was opened with the O_APPEND flag) The lseek function modifies only the current file offset in the file table entry. No I/O takes place It is possible for more than one file descriptor entry to point to the same file table entry. This also happens after a fork when the parent and the child share the same file table entry for each open descriptor\nNote the difference in scope between the file descriptor flags and the file status flags. The former apply only to a single descriptor in a single process, whereas the latter apply to all descriptors in any process that point to the given file table entry\nAtomic Operations Any operation that requires more than one function call cannot be atomic, as there is always the possibility that the kernel might temporarily suspend the process between the two function calls\n#include \u0026lt;unistd.h\u0026gt; ssize_t pread(int fd, void *buf, size_t nbytes, off_t offset); //Returns: number of bytes read, 0 if end of file, −1 on error ssize_t pwrite(int fd, const void *buf, size_t nbytes, off_t offset); //Returns: number of bytes written if OK, −1 on error If the operation is performed atomically, either all the steps are performed (on success) or none are performed (on failure).\ndup and dup2 Functions An existing file descriptor is duplicated by either of the following functions\n#include \u0026lt;unistd.h\u0026gt; int dup(int fd); int dup2(int fd, int fd2); //Both return: new file descriptor if OK, −1 on error With dup2, we specify the value of the new descriptor with the fd2 argument. If fd2 is already open, it is first closed. If fd equals fd2, then dup2 returns fd2 without closing it. Otherwise, the FD_CLOEXEC file descriptor flag is cleared for fd2, so that fd2 is left open if the process calls exec\nsync,fsync , and fdatasync function Traditional implementations of the UNIX System have a buffer cache or page cache in the kernel through which most disk I/O passes. When we write data to a file, the data is normally copied by the kernel into one of its buffers and queued for writing to disk at some later time. This is called delayed write\nTo ensure consistency of the file system on disk with the contents of the buffer cache, the sync, fsync, and fdatasync functions are provided.\n#include \u0026lt;unistd.h\u0026gt; int fsync(int fd); int fdatasync(int fd); //Returns: 0 if OK, −1 on error void sync(void); The sync function simply queues all the modified block buffers for writing and returns; it does not wait for the disk writes to take place.The function sync is normally called periodically (usually every 30 seconds) from a system daemon, often called update.\nThe function fsync refers only to a single file, specified by the file descriptor fd, and waits for the disk writes to complete before returning.(database)\nThe fdatasync function is similar to fsync, but it affects only the data portions of a file. With fsync, the file’s attributes are also updated synchronously\nfcntl function #include \u0026lt;fcntl.h\u0026gt; int fcntl(int fd, int cmd, ... /* int arg */ ); //Returns: depends on cmd if OK (see following), −1 on error The fcntl function is used for five different purposes\nDuplicate an existing descriptor (cmd = F_DUPFD or F_DUPFD_CLOEXEC) Get/set file descriptor flags (cmd = F_GETFD or F_SETFD) Get/set file status flags (cmd = F_GETFL or F_SETFL) Get/set asynchronous I/O ownership (cmd = F_GETOWN or F_SETOWN) Get/set record locks (cmd = F_GETLK, F_SETLK, or F_SETLKW) ioctl function The ioctl function has always been the catchall for I/O operations. Terminal I/O was the biggest user of this function\n#include \u0026lt;unistd.h\u0026gt; /* System V */ #include \u0026lt;sys/ioctl.h\u0026gt; /* BSD and Linux */ int ioctl(int fd, int request, ...); //Returns: −1 on error, something else if OK Normally, additional device-specific headers are required. For example, the ioctl commands for terminal I/O, beyond the basic operations specified by POSIX.1, all require the header.\nEach device driver can define its own set of ioctl commands. The system, however, provides generic ioctl commands for different classes of devices\n/dev/fd Newer systems provide a directory named /dev/fd whose entries are files named 0, 1, 2, and so on\nIn the function call\nfd = open(\u0026#34;/dev/fd/0\u0026#34;, mode); most systems ignore the specified mode, whereas others require that it be a subset of the mode used when the referenced file (standard input, in this case) was originally opened. Because the previous open is equivalent to\nfd = dup(0); the descriptors 0 and fd share the same file table entry\nFor example, if descriptor 0 was opened read-only, we can only read on fd. Even if the system ignores the open mode and the call succeeds, we still can’t write to fd.\nThe main use of the /dev/fd files is from the shell. It allows programs that use pathname arguments to handle standard input and standard output in the same manner as other pathnames, like cat - to cat /dev/fd/0\nThe special meaning of - as a command-line argument to refer to the standard input or the standard output is a kludge that has crept into many programs. There are also problems if we specify - as the first file, as it looks like the start of another command-line option. Using /dev/fd is a step toward uniformity and cleanliness.\n","permalink":"https://mobilephone724.github.io/posts/archive_posts/linux-file/","summary":"The functions described in this chapter are often referred to as unbuffered I/O(which each read or write invokes a system call in the kernel), in contrast to the standard I/O routines\nFile Descriptors To the kernel, all open files are referred to by file descriptors. A file descriptor is a non-negative integer. When we open an existing file or create a new file, the kernel returns a file descriptor to the process.","title":"APUE/Chapter3: file and I/O"},{"content":"Overview This chapter explains the content of clog\nclog(commit log), records the commit status of each transaction. The log exists both in memory mannaged by slru buffer and disk for durability. The commit status can be the four kinds below:\n#define TRANSACTION_STATUS_IN_PROGRESS\t0x00 #define TRANSACTION_STATUS_COMMITTED\t0x01 #define TRANSACTION_STATUS_ABORTED\t0x02 #define TRANSACTION_STATUS_SUB_COMMITTED\t0x03 In-Disk Representation Thinking that the commit status of each transaction composites an array clog[] and clog[xid] records the status, we can easily store the array to disk by the slru.\nThe status of one transaction needs two bits to represent:\n#define CLOG_BITS_PER_XACT\t2 #define CLOG_XACTS_PER_BYTE 4 #define CLOG_XACTS_PER_PAGE (BLCKSZ * CLOG_XACTS_PER_BYTE) #define CLOG_XACT_BITMASK\t((1 \u0026lt;\u0026lt; CLOG_BITS_PER_XACT) - 1) So we can get the xid\u0026rsquo;s index and offset in page and byte.\n#define TransactionIdToPage(xid)\t((xid) / (TransactionId) CLOG_XACTS_PER_PAGE) #define TransactionIdToPgIndex(xid) ((xid) % (TransactionId) CLOG_XACTS_PER_PAGE) #define TransactionIdToByte(xid)\t(TransactionIdToPgIndex(xid) / CLOG_XACTS_PER_BYTE) #define TransactionIdToBIndex(xid)\t((xid) % (TransactionId) CLOG_XACTS_PER_BYTE) Thinking of that one slru segment contains 32 pages, so we name the clog file as 0000(contains xid in [0, 32 * CLOG_XACTS_PER_PAGE - 1]), 0001(contains xid in [32 * CLOG_XACTS_PER_PAGE, 32 * CLOG_XACTS_PER_PAGE * 2 - 1]) and so on. Because four hex numbers can represent $16^4=2^{12}$ files with $2^{12} \\times 32 \\times 8192 \\times 4 = 2^{32}$ transactions\u0026rsquo; status(a int32 size)\nAttension, such simple mapping means that the pages in clog file don\u0026rsquo;t have page headers. So we can\u0026rsquo;t record LSN, checksum in each page. The lack of LSN means the changes of clog page wouldn\u0026rsquo;t be recorded in WAL but clog doesn\u0026rsquo;t need it indeed.\nExtend And Truncate During the process of generating a new xid, we make sure that the slru page exists.\nIf it\u0026rsquo;s the first xid of the page, we allocate a new page in clog buffer. Also generate a WAL to record the birth of the page. If not, the page must exist in memory or flushed into disk. So it\u0026rsquo;s for slru layer to manage such situation. Keep in mind that the general self-increment xid does\u0026rsquo;t begin at zero:\n#define FirstNormalTransactionId\t((TransactionId) 3) so:\nDuring bootstrap, initialize the first clog page During extend new pages, be careful about the FirstNormalTransactionId, since it is not the first xid in page representation but the first general one. The above behaviors indicate that although a clog segment at most occupies 256K space, it doesn\u0026rsquo;t have such size just after initialization. We extend 8K pages one by one during the xid increment.\nSince at most half of uint32 xids can be in use, it\u0026rsquo;s natural to clean up out of date clog files. Different from extending a page, we always delete a whole page. So once we promote the frozenxid, we try to find some clog files to delete:\nThe judgement whether there is a file can be deleted is completed in slru layer(a loop to scan the directory), but clog layer supports a hook to judge one file. Advance the oldest clog xid in shared memory Generate a clog truncate WAL record Real truncate. Complemented in slru layer. Details of the two kind WAL record will be shown later.\nSet And Get Concerned with subtransactions \u0026hellip;\nI can\u0026rsquo;t totally figure out the commit tree without knowing the mechanism of subtransaction. Just assuming subxids as a set of xids related to the main xid seems not convictive enough for me. So I remain it here now and will finish it after reading subtransactions)\nFor now, it\u0026rsquo;s enough to knowing that\nThe pair of operations wouldn\u0026rsquo;t generate any WAL record They are done during the commit or abort procedure. Record changes in WAL Recall what mentioned above:\nExtending a new page and delete a segment will generata a WAL record. Setting commit status wouldn\u0026rsquo;t For the latter one, it\u0026rsquo;s unbelievable but tricky. Since only the transactions that changes the content data(some hint flags are exception, such as tuple infomask) will have a xid(and then record on clog segment). During the replay of such transactions\u0026rsquo; commit(or abort) WAL record, we can redo the clog by the way.\nFor the former one, it\u0026rsquo;s a matter of course, since we must guarantee the clog to be recovery-safe. But some details deserve a glance;\nFor extending a new page, it makes no difference that we flush the WAL record now or later. Since once we want to set status in a non-existent page during recovery, we can padding a new empty page. This trick doesn\u0026rsquo;t affect the page usage. For deleting a clog segment, we have no chance to remedy the lost of clogs, and the disaster means a lot of tuple can be accessed at all. So regardless of the synchronous commit level, we must ensure the WAL record has flushed into disk before really delete the segments. ","permalink":"https://mobilephone724.github.io/article/clog/","summary":"Overview This chapter explains the content of clog\nclog(commit log), records the commit status of each transaction. The log exists both in memory mannaged by slru buffer and disk for durability. The commit status can be the four kinds below:\n#define TRANSACTION_STATUS_IN_PROGRESS\t0x00 #define TRANSACTION_STATUS_COMMITTED\t0x01 #define TRANSACTION_STATUS_ABORTED\t0x02 #define TRANSACTION_STATUS_SUB_COMMITTED\t0x03 In-Disk Representation Thinking that the commit status of each transaction composites an array clog[] and clog[xid] records the status, we can easily store the array to disk by the slru.","title":"CLOG"},{"content":"concept cublasDgemm is a convenient function in cublas to compute the product of two matrix, while letter \u0026lsquo;D\u0026rsquo; in cublasDgemm means double.\nBefore reading this post, basic cuda functions like cudaMalloc are what you are supposed to know.\nbasic use Definition of this function\ncublasStatus_t cublasDgemm(cublasHandle_t handle, cublasOperation_t transa, cublasOperation_t transb, int m, int n, int k, const double *alpha, const double *A, int lda, const double *B, int ldb, const double *beta, double *C, int ldc) Basic information of parameters is show in this page. Simply put, $C = \\alpha A \\times B + \\beta C $ .But it may remains confused for fresher. Below is an simple example.\n/* A is matrix in gpu memory looks like * 1 2 3 * 4 5 6 * 7 8 9 * and ptr_A is a pointer to A * * B is matrix in gpu memory looks like * 1 2 * 3 4 * 5 6 * and ptr_A is a pointer to A * * While memory is one-dimensional while matrix is two-dimensional, I * suggeset that all matrix in gpu memory are stored in column major for * convevient use of cublas. In this case, A in memory is like * [1, 4, 7, 2, 5, 8, 3, 6, 9]. * C is a matrix to store the product of A * B */ //get handle and stat of this function cublasHandle_t handle; cublasStatus_t stat = cublasCreate(\u0026amp;handle); if (stat != CUBLAS_STATUS_SUCCESS) { printf(\u0026#34;CUBLAS initialization failed\\n\u0026#34;); return EXIT_FAILURE; } //setting alpha and cuda double alpha = 1.0, beta = 0.0; stat = cublasDgemm(\thandle, CUBLAS_OP_N,\t// we use matrix A instead of A^T CUBLAS_OP_N,\t// we use matrix B instead of B^T 3,\t// the row of A 2,\t// the col of B 3,\t// the row of B(ro col of A) \u0026amp;alpha, devPtrA, 3,\t// the leading dimension of A devPtrB, 3,\t// the leading dimension of B \u0026amp;beta, devPtrC, 3);\t// the leading dimension of C /* * if we want to compute C = A^T * B */ stat = cublasDgemm(\thandle, CUBLAS_OP_T,\t// we use matrix A^T instead of A CUBLAS_OP_N,\t// we use matrix A instead of B^T 3,\t// the row of A^T 2,\t// the col of B 3,\t// the row of B(or col of A^T) \u0026amp;alpha, devPtrA, 3,\t// the leading dimension of A^T. // So whether or not A or A^T, the leading dimension // of A or A^T is the row of A, decided when A is // initialized in memory devPtrB, 3,\t// the leading dimension of B \u0026amp;beta, devPtrC, 3);\t// the leading dimension of C if (stat != CUBLAS_STATUS_SUCCESS) { printf(\u0026#34;cublasSgemm failed\\n\u0026#34;); return EXIT_FAILURE; } An obvious question is what is leading dimension for we have know the column and row of A and B, no more information is need to finish this computation.\nMy understanding of leading dimension is the offest to get the element in next column at the same row. An implement to compute the product of submatrix. below is an example. A and B are the same matrix in the previous example.\nAnd what we want to compute is $A[0:1][0:1] \\times B[1:2][0:1]$.\nstat = cublasDgemm(\thandle, CUBLAS_OP_N,\t// we use matrix A[0:1][0:1] instead of A[0:1][0:1]^T CUBLAS_OP_N,\t// we use matrix B[1:2][0:1] instead of B[1:2][0:1]^T 2,\t// the row of A[0:1][0:1] 2,\t// the col of B[1:2][0:1] 2,\t// the row of A[0:1][0:1](or col of B[1:2][0:1]) \u0026amp;alpha, devPtrA,\t// pointer to A[0][0] 3,\t// the offset of A[0][0] to A[0][1] is 3 of double size devPtrB + 1,\t// pointer to B[1][0] 3,\t// the offset of B[1][0] to B[1][1] is 3 of double size \u0026amp;beta, devPtrC, 2);\t// the leading dimension of C So it\u0026rsquo;s the use of leading dimension which makes matrix production more flexible\n","permalink":"https://mobilephone724.github.io/posts/archive_posts/cublasdgemmtutor/","summary":"concept cublasDgemm is a convenient function in cublas to compute the product of two matrix, while letter \u0026lsquo;D\u0026rsquo; in cublasDgemm means double.\nBefore reading this post, basic cuda functions like cudaMalloc are what you are supposed to know.\nbasic use Definition of this function\ncublasStatus_t cublasDgemm(cublasHandle_t handle, cublasOperation_t transa, cublasOperation_t transb, int m, int n, int k, const double *alpha, const double *A, int lda, const double *B, int ldb, const double *beta, double *C, int ldc) Basic information of parameters is show in this page.","title":"cublasDgemm"},{"content":"high level view See Queries in PostgreSQL: 6. Hashing\nOne-pass hash join Note that join in PostgreSql, we scan the right relation first, which means that the right relation is the \u0026ldquo;inner relation\u0026rdquo; and the left relation is the outer one. Two-pass hash join Since we can\u0026rsquo;t allocate as much memory as we want, instead of building a hash table of the entire table, PG split the tables to several batches where all tuples have the same hash value flag.\nBatches are splited by hash value. Use several bits in hash value as a flag so we can put the tuples into different batches.\nThere is a simple optimization that we can build the hash table in the first batch while scanning the inner table, and match the pair while scanning the outer table.\nparallel one-pass hash join With parallel workers, we can\nscan inner table and build shared hash table parallelly scan outer table parallelly Although in most cases, the neck of tp system is disk io, but parallel workers can still advance the speed efficiently. Because:\nIn single process situation, the disk IO is synchronous，which means CPU is in idle while waiting IO. So, in the parallel case, CPU can be utilized more sufficiently. OS may has the technique to load the disk\u0026rsquo;s content in advance, which is perdicularly useful in sequence scan. So multi-workers can read data file content more efficiently. In hash join, the compute of hash value may cost more CPU resource than normal TP operation. parallel two-pass hash join Same as the basic two-pass hash join, parallel workers build batches parallelly, both in reading from inner/outer tuple and writing data to tmp file. Since no worker can obtain a whole batch\u0026rsquo;s data in the first scan, the technique described above can be used here.\nLow level complement Single process inner join What is inner join\nThis is the simplest join method in hash join. So we introduce a simple hash join state machine here. (See ExecHashJoinImpl for detail )\nSTART WITH: state ==\u0026gt; HJ_BUILD_HASHTABLE case HJ_BUILD_HASHTABLE: state ==\u0026gt; HJ_NEED_NEW_OUTER case HJ_NEED_NEW_OUTER: ### generate a new outer tuple state ==\u0026gt; HJ_NEED_NEW_BATCH ### No more tuple in this batch. ==\u0026gt; HJ_SCAN_BUCKET; ### Find a outer tuple. Can this one matches a inner one? case HJ_SCAN_BUCKET: ### Scan the selected hash bucket for matches to current outer state ==\u0026gt; HJ_NEED_NEW_OUTER ### Whether we can find a match or not, we always generate a new outer tuple. case HJ_NEED_NEW_BATCH: ### Try to advance to next batch state ==\u0026gt; HJ_NEED_NEW_OUTER; ==\u0026gt; FINISH right join To complete right join, we can just emit each outer tuple even if there\u0026rsquo;s no matched innner tuple.\ncase HJ_SCAN_BUCKET: state ==\u0026gt; HJ_FILL_OUTER_TUPLE ### Can not find a match. Is it a left join? ==\u0026gt; HJ_NEED_NEW_OUTER case HJ_FILL_OUTER_TUPLE: state ==\u0026gt; HJ_NEED_NEW_OUTER; ### Whether emit the outer tuple with null-filled left tuple or not, we always generate a new outer tuple. left join To complete this, we must remember whether a inner tuple has been matched. So\ncase HJ_NEED_NEW_OUTER: state ==\u0026gt; HJ_FILL_INNER_TUPLES ### This batch has been finished, see if there are unmatched inner tuples. ==\u0026gt; HJ_NEED_NEW_BATCH ==\u0026gt; HJ_SCAN_BUCKET case HJ_FILL_INNER_TUPLES: state ==\u0026gt; HJ_NEED_NEW_BATCH ### No more unmatched inner tuples, so start the next batch ==\u0026gt; HJ_FILL_INNER_TUPLES ### return an unmatched inner tuple. summary Until now, we can generate a full state machine in non-parallel mode\nSTART WITH: state ==\u0026gt; HJ_BUILD_HASHTABLE case HJ_BUILD_HASHTABLE: state ==\u0026gt; HJ_NEED_NEW_OUTER case HJ_NEED_NEW_OUTER: ### generate a new outer tuple state ==\u0026gt; HJ_FILL_INNER_TUPLES ### This batch has been finished, see if there are unmatched inner tuples. ==\u0026gt; HJ_NEED_NEW_BATCH ### No more tuple in this batch. ==\u0026gt; HJ_SCAN_BUCKET; ### Find a outer tuple. Can this one matches a inner one? case HJ_SCAN_BUCKET: ### Scan the selected hash bucket for matches to current outer state ==\u0026gt; HJ_FILL_OUTER_TUPLE ### Can not find a match. Is it a left join? ==\u0026gt; HJ_NEED_NEW_OUTER ### Whether we can find a match or not, we always generate a new outer tuple. case HJ_NEED_NEW_BATCH: ### Try to advance to next batch state ==\u0026gt; HJ_NEED_NEW_OUTER; ==\u0026gt; FINISH parallel hash Note that BarrierArriveAndWait will increase current phase. So each phase\u0026rsquo;s status is not be assigned directly but self-increased.\nLet introduce the state machine first\nSTART WITH: case HJ_BUILD_HASHTABLE: ### If multi-batch, we need to hash the outer relation up front. ExecParallelHashJoinPartitionOuter(node); state ==\u0026gt; HJ_NEED_NEW_BATCH ### Select a batch to work on. case HJ_NEED_NEW_OUTER: ExecParallelHashJoinOuterGetTuple sts_parallel_scan_next case HJ_NEED_NEW_BATCH: ExecParallelHashJoinNewBatch() switch PHJ_BATCH_STATE case PHJ_BATCH_ELECT: ### One backend allocates the hash table ExecParallelHashTableAlloc ### Fall through case PHJ_BATCH_ALLOCATE: ### Wait for allocation to complete and Fall through case PHJ_BATCH_LOAD: ### Start (or join in) loading tuples and Fall through. case PHJ_BATCH_PROBE: ### This batch is ready to probe ExecParallelHashTableSetCurrentBatch return true; case PHJ_BATCH_SCAN: ### detach and go around again case PHJ_BATCH_FREE: state ==\u0026gt; HJ_NEED_NEW_OUTER PHJ_BUILD_ELECT ==\u0026gt; PHJ_BUILD_ALLOCATE ExecParallelHashJoinNewBatch Code level Detail utility ExecHashGetBucketAndBatch : hash value to bucket number and batch number ExecHashGetBucketAndBatch(HashJoinTable hashtable, uint32 hashvalue, int *bucketno, int *batchno) { uint32 nbuckets = (uint32) hashtable-\u0026gt;nbuckets; uint32 nbatch = (uint32) hashtable-\u0026gt;nbatch; if (nbatch \u0026gt; 1) { *bucketno = hashvalue \u0026amp; (nbuckets - 1); ### tricky way as MOD *batchno = pg_rotate_right32(hashvalue, hashtable-\u0026gt;log2_nbuckets) \u0026amp; (nbatch - 1); ### rotate hashvalue and MOD nbatch } else { *bucketno = hashvalue \u0026amp; (nbuckets - 1); *batchno = 0; } } ExecHashTableInsert : insert hash value ExecHashTableInsert ExecHashGetBucketAndBatch(hashtable, hashvalue, \u0026amp;bucketno, \u0026amp;batchno); if (batchno == hashtable-\u0026gt;curbatch) ### put into hash table hashTuple = (HashJoinTuple) dense_alloc hashtable-\u0026gt;spaceUsed += hashTupleSize; ### For single batch, we may increase the nbucket if (hashtable-\u0026gt;nbatch == 1) if (ntuples \u0026gt; (hashtable-\u0026gt;nbuckets_optimal * NTUP_PER_BUCKET) \u0026amp;\u0026amp; xxx) hashtable-\u0026gt;nbuckets_optimal *= 2; hashtable-\u0026gt;log2_nbuckets_optimal += 1; ### For multi-batches, we may increase the batches if (hashtable-\u0026gt;spaceUsed + hashtable-\u0026gt;nbuckets_optimal * sizeof(HashJoinTuple) + \u0026gt; hashtable-\u0026gt;spaceAllowed) ExecHashIncreaseNumBatches() else ### put the tuple into a temp file for later batches ExecHashJoinSaveTuple() ExecHashIncreaseNumBatches : increase batches ExecHashIncreaseNumBatches nbatch = oldnbatch * 2; ### double nbatches ### init/update batchfiles if (hashtable-\u0026gt;innerBatchFile == NULL) hashtable-\u0026gt;innerBatchFile = palloc0_array(BufFile *, nbatch); hashtable-\u0026gt;outerBatchFile = palloc0_array(BufFile *, nbatch); PrepareTempTablespaces(); else hashtable-\u0026gt;innerBatchFile = repalloc0_array() hashtable-\u0026gt;outerBatchFile ### resize nbuckets? if (hashtable-\u0026gt;nbuckets_optimal != hashtable-\u0026gt;nbuckets) hashtable-\u0026gt;nbuckets = hashtable-\u0026gt;nbuckets_optimal; hashtable-\u0026gt;log2_nbuckets = hashtable-\u0026gt;log2_nbuckets_optimal; hashtable-\u0026gt;buckets.unshared = repalloc_array() ### scan through allchunks while (oldchunks != NULL) nextchunk = oldchunks-\u0026gt;next.unshared ### scan through all tuples in the chunk idx = 0 while (idx \u0026lt; oldchunks-\u0026gt;used) HashJoinTuple hashTuple = (HashJoinTuple) (HASH_CHUNK_DATA(oldchunks) + idx); ... ### where should the tuple go? ExecHashGetBucketAndBatch(hashtable, hashTuple-\u0026gt;hashvalue, \u0026amp;bucketno, \u0026amp;batchno); if (batchno == curbatch) ### keep the tuple but copy it into the new chunk copyTuple = (HashJoinTuple) dense_alloc(hashtable, hashTupleSize); hashtable-\u0026gt;buckets.unshared[bucketno] = copyTuple; else ### dump it out ExecHashJoinSaveTuple() idx += MAXALIGN(hashTupleSize); pfree(oldchunks); oldchunks = nextchunk; ExecHashJoinSaveTuple : save a tuple to a batch file. BufFileWrite(file, \u0026amp;hashvalue, sizeof(uint32)); BufFileWrite(file, tuple, tuple-\u0026gt;t_len); ### len is record in MinimalTupleData structure single worker build state MultiExecProcNode MultiExecPrivateHash for (;;) slot = ExecProcNode(outerNode); if (ExecHashGetHashValue()) bucketNumber = ExecHashGetSkewBucket if (bucketNumber != INVALID_SKEW_BUCKET_NO) ### skew tuple ExecHashSkewTableInsert else ExecHashTableInsert\t### normal tuple hashtable-\u0026gt;totalTuples += 1; xxx\n","permalink":"https://mobilephone724.github.io/article/hashjoin/","summary":"high level view See Queries in PostgreSQL: 6. Hashing\nOne-pass hash join Note that join in PostgreSql, we scan the right relation first, which means that the right relation is the \u0026ldquo;inner relation\u0026rdquo; and the left relation is the outer one. Two-pass hash join Since we can\u0026rsquo;t allocate as much memory as we want, instead of building a hash table of the entire table, PG split the tables to several batches where all tuples have the same hash value flag.","title":"hash join"},{"content":"principle pg_repack 1.5.0 \u0026ndash; Reorganize tables in PostgreSQL databases with minimal locks\nhttps://github.com/reorg/pg_repack\ncreate a log table to record changes made to the original table add a trigger onto the original table, logging INSERTs, UPDATEs and DELETEs into our log table create a new table containing all the rows in the old table build indexes on this new table apply all changes which have accrued in the log table to the new table swap the tables, including indexes and toast tables, using the system catalogs drop the original table The basic idea is\ntransport the existent data with a old snapshot record the incremental data into a table and replay the record And this idea is so general that almost all online-ddl ability in PG(supported in extensions) takes the way.\ndetails Although the idea is so simple, there are many problems to challenge. Such as how to ensure there are no duplicated or lost data in both existent part and incremental part. So code-level details are shown below:\nAll the 7 step are manipulated through 2 connections: See function repack_one_table for detail:\ncreate a log table to record changes made to the original table\nadd a trigger onto the original table, logging INSERTs, UPDATEs and DELETEs into our log table\nconn1 starts a transaction and acquire an advisory lock to prevent potential conflict with other repack process get the AccessiveExclusive lock to the original table, tbl for example create the trigger on tbl and the corresponding log-table where the incremental changes will be stored. (Just comments: If we release the exclusive lock here, we may not able to acquire a shared lock later if another process has gotten a exclusive lock in the interval, which can cause that we have no way to continue or revert what we have done. So we must acquire a lock during the whole process. 👌) conn2 tries to acquire the AccessiveShared lock on tbl. Since the conn1 \u0026rsquo;s transaction hasn\u0026rsquo;t finished, this lock acquisition will be blocked. conn1 kill all connections that tries to perform a DDL operation, whose character is waiting for AccessiveLock . Then, conn1 commits. Now conn2 get the AccessiveShared lock on tbl , which can ensure that no other dll operation on tbl ✌️ create a new table containing all the rows in the old table\nconn1 begins a serializable transaction( repeatable read, at least)\nconn1 get the vxids of current active transactions\nconn1 delete all data in tbl with the current snapshot (This means we don\u0026rsquo;t perform a “truncate” operation ). This is a very skillful technique:\nThe table shows the secret:\ntbl log table visible existent data empty invisible incremental data incremental data All existent data is visible in tbl through the current snapshot\nAll incremental data is invisible in log table and tbl (The latter one isn\u0026rsquo;t important\nSo there is no lost or duplicated data\nconn1 copies all data in tbl to a temp table tbl-tmp for example\nconn1 commits\nbuild indexes on this new table. (I don\u0026rsquo;t care this.)\napply all changes which have accrued in the log table to the new table\nconn1 apply at most 1000 records in log-table , until the remaining records are few. AND All transactions in vxids finish. This operation is to keep the ISOLATION, but it still has some accidence. #TODO (Just comments: Now we believe that there is few records in log-table .) conn2 acquire the AccessiveExclusive lock. Note that no other process can do that conn2 apply all data in log-table swap the tables, including indexes and toast tables, using the system catalogs\nconn2 swaps relfilenode between tbl-tmp and tbl conn2 commits drop the original table\nconn1 drop the current tbl-tmp conn1 analyze the current tbl conn1 release the advisory lock ","permalink":"https://mobilephone724.github.io/article/pg_repack/","summary":"principle pg_repack 1.5.0 \u0026ndash; Reorganize tables in PostgreSQL databases with minimal locks\nhttps://github.com/reorg/pg_repack\ncreate a log table to record changes made to the original table add a trigger onto the original table, logging INSERTs, UPDATEs and DELETEs into our log table create a new table containing all the rows in the old table build indexes on this new table apply all changes which have accrued in the log table to the new table swap the tables, including indexes and toast tables, using the system catalogs drop the original table The basic idea is","title":"pg_repack"},{"content":"序言 pgvector是一个向量搜索（根据近似度）的插件，用来加速AKNN（approximate nearest neighbor）。 PASE中提到，向量ANN算法包括4类\ntree-based algorithms KD-Tree RTree quantization-based algorithms IVFFlat IVFADC IMI graph based algorithms HNSW NSG SSG hash-base algorithms LSH pgvector 包括两个算法，IVFFlat 和 HNSW，后续内容将以这两个算法的内容及其实现展开。 IVFFlat 概览 IVFFlat 算法主要包括以下几个步骤\n索引构建阶段 使用 KMeans 将数据集划分成多个簇(cluster) 查询阶段 通过每个簇的中心点（向量是高维的点）获取N个最近的簇 遍历这N个簇的所有点，从中找到最近的K个点 算法介绍 基础算法kmeans reference k-means clustering - Wikipedia 算法目标：选取K个中心点，使得数据集中的所有点到其最近的中心点“距离”之和最近，以平方和距离为例：\nGiven a set of observations $(x_1, x_2, \\dots, x_n)$, where each observation is a $d$-dimensional real vector, k-means clustering aims to partition the $n$ observations into $k$ ($\\leq n$) sets $S = {S_1, S_2, \\dot, S_k}$ so as to minimize the within-cluster sum of squares (WCSS). Formally, the objective is to find: 算法过程： 我们可以很容易的证明目标函数是关于$S$的凸函数 Given an initial set of $k$ means $m_1^{1}, \\dots , m_k^{(1)}$ (see below), the algorithm proceeds by alternating between two steps:\nAssignment step: Assign each observation to the cluster with the nearest mean: where each $x_p$is assigned to exactly one $S^{t}$, even if it could be assigned to two or more of them.\nUpdate step: Recalculate means (centroids) for observations assigned to each cluster.\nkmeans 优化篇 上述算法虽然简洁，但计算上复杂度高。在pgvector的IVFFlat实现中，使用了一些优化算法，主要是如下两篇论文：\nUsing Triangle Inequality: 使用三角不等式减少两点间距离的计算次数 KMeans++ :使用随机点的选取技巧来提高收敛速度和准确率 Using the Triangle Inequality to Accelerate k-Means (aaai.org) kMeansPP-soda.pdf (stanford.edu) Using Triangle Inequality 思路：\n在高维向量中，计算一次两点之间的距离的代价较高。 根据一些朴素的思想，假如使用的距离函数满足三角不等式$d(a,b) \\leq d(a,c) + d(b,c)$，那么在一次kmeams迭代中，如果点 x 距其中心点 c(x) 的距离很近，而 c(x) 距另一个中心点 c(y) 的距离很远，那么c(y)必然不是x 的中心点，这样就可以避免一次计算。 根据三角不等式可以推出\nLet x be a point and let b and c be centers. If $d(b, c) \u0026gt; 2d(x,b)$, then $d(x,c) \\geq d(x,b)$ Let x be a point and let b and c be centers, then $d(x,c) \\geq \\mathrm{max} {0,d(x,b)-d(b,c)}$ 根据上述定理，在Kmeans迭代期间，维护一些状态，即可减少计算量 过程如\n使用三角不等式优化Kmeans\nKMeans++ 论文中的数学分析很多，其主要目的为：通过在初始化的时候选取恰当的中心点，减少迭代次数。方法为： 假设向量的全集为$X={x_1,x_2,\\dots,x_n}\\subset \\mathbb{R}^d$ ,$D(x)$ 表示点 $x$ 到其当前中心点的距离\n从$X$ 中随机选择一个点$c_1$ 以$\\frac{D(x\u0026rsquo;)}{\\sum_{x\\in X}D(x)}$ 的概率选择$x\u0026rsquo;$为$c_i$ 重复上一步直到我们选择了 $k$ 个中心点， 使用标准的k-means算法进行后续处理 实现介绍 page representation Key functions index build 索引构建分为以下几个步骤\n计算中心点 构建元信息页（\u0026lsquo;meta page\u0026rsquo;） 构建中心点页（\u0026lsquo;centerid pages\u0026rsquo;） 构建数据页（\u0026lsquo;data pages\u0026rsquo;） ivfflatbuild BuildIndex InitBuildState ComputeCenters CreateMetaPage CreateListPages CreateEntryPages FreeBuildState 计算中心点 实现上，没有扫描所有的行以计算中心点，而是“采样”一些block。 会选择$ncenter \\times 50$ 作为采样block的数量 ComputeCenters SampleRows /* The number of target samples is the number of centers times 50 */ numSamples = buildstate-\u0026gt;lists * 50; buildstate-\u0026gt;samples = VectorArrayInit(numSamples, buildstate-\u0026gt;dimensions); BlockSampler_Init \u0026gt; provides algorithm for block level sampling of a relation as discussed on pgsql-hackers 2004-04-02 (subject \u0026#34;Large DB\u0026#34;) Since we know the total number of blocks in advance, we can use the straightforward Algorithm S from Knuth 3.4.2, rather than Vitter\u0026#39;s algorithm. reservoir_init_selection_state while (BlockSampler_HasMore(\u0026amp;buildstate-\u0026gt;bs)) table_index_build_range_scan: callback=SampleCallback IvfflatKmeans # Do as kmeans algrithm if (samples-\u0026gt;length \u0026lt;= centers-\u0026gt;maxlen) QuickCenters(index, samples, centers); else ElkanKmeans(index, samples, centers); SampleCallback AddSample if (samples-\u0026gt;length \u0026lt; targsamples) VectorArraySet else if (buildstate-\u0026gt;rowstoskip \u0026lt; 0) rowstoskip = reservoir_get_next_S #skip some future samples else k = sampler_random_fract VectorArraySet\t# replace a old with this one randomly 构建元信息页 CreateMetaPage # info about meta information IvfflatNewBuffer IvfflatInitRegisterPage IvfflatCommitBuffer 构建中心点页 当一个页的剩余空间不够时，使用字段nextblkno指向下一个页\ntypedef struct IvfflatPageOpaqueData { BlockNumber nextblkno; uint16\tunused; uint16\tpage_id;\t/* for identification of IVFFlat indexes */ }\tIvfflatPageOpaqueData; CreateListPages # info about center infomation foreach sampled vector if (PageGetFreeSpace \u0026lt; listSize) # we need more free space to store the vector IvfflatAppendPage newbuf = IvfflatNewBuffer newpage = GenericXLogRegisterBuffer IvfflatPageGetOpaque old_page-\u0026gt;next = this_page IvfflatInitPage PageAddItem # copy this point to the page 构建数据页 CreateEntryPages # omit parallel optimization here AssignTuples # Scan table for tuples to index tuplesort_performsort InsertTuples for (int i = 0; i \u0026lt; buildstate-\u0026gt;centers-\u0026gt;length; i++) buf = IvfflatNewBuffer(index, forkNum); # add new page for each data page list startPage = BufferGetBlockNumber(buf); # the first page number foreach tuple in this list: if (PageGetFreeSpace(page) \u0026lt; itemsz) # append page IvfflatAppendPage(index, \u0026amp;buf, \u0026amp;page, \u0026amp;state, forkNum); PageAddItem() IvfflatUpdateList(); # update the first page record of the center page index scan begin scan ivfflatbeginscan IvfflatGetMetaPageInfo(index, \u0026amp;lists, \u0026amp;dimensions); # Get lists and dimensions from metapage get tupele ivfflatgettuple if (first) # try to get the first tuple GetScanLists # find \u0026#39;probe\u0026#39; centers that are closest while (BlockNumberIsValid(nextblkno)) # search all list pages if (distance \u0026lt; maxDistance) # omit probe here for easier understanding scanlist = (IvfflatScanList *) pairingheap_remove_first(so-\u0026gt;listQueue); pairingheap_add(so-\u0026gt;listQueue, \u0026amp;scanlist-\u0026gt;ph_node); maxDistance = ((IvfflatScanList *) pairingheap_first(so-\u0026gt;listQueue))-\u0026gt;distance; GetScanItems # find closest items in the above centers while (!pairingheap_is_empty(so-\u0026gt;listQueue)) # for each center while (BlockNumberIsValid(searchPage)) # for each block in the data list foreach (tuple) tuplesort_puttupleslot HNSW 概览 HNSW 算法主要包括以下几个步骤\n索引构建 构建层级邻近图 每一层都是邻近图 —— 每个点都记录它最近的几个点 高一层的图是低一层图的缩略图 —— 只有低一层图的部分点 ——，最低一层的图有全部点的信息。 查询阶段，对于目标点$p$ 对于每一层图： 维护一个图中距点$p$最近的点集合$S$，依次从候选点集合$C$中选取一个元素$c$：如果$c$的邻居$neighbor(c)$比$S$中距$p$最远的点$s$距$p$更近，即$d(neighbor(c), p) \u0026lt; d(s,c)$ ，则用$neighbor(c)$替换集合$S$中的点$s$，并将$s$加入到候选集合$C$中。重复以上步骤直到$|c| = 0$ 从高层图向底层图搜索，使用高层图的结果$S$作为低层图$S$和$C$的初始值。 算法介绍 一下顺序只是为了便于理解，不代表论文发布顺序。更多细节可参考论文。\nNSW —— HNSW的起源? NSW可以视为邻近图，每个点维护至多$K$个距离其最近的点，此时HNSW退化为只有一层的特殊情况。\nNSW的构建 构建NSW的算法如下（此处忽略边角情况以方便理解\nINPUT: a set of points S OUTPUT: graph G BUILD_LAYER(S) G = [] # Insert each point into the graph FOREACH point IN S: ---------- INSERT_POINT(graph, point) neighbors[] = select_one_random(G) candidate[] = neighbors[] visited_points[] = neighbors[] # Code in this WHILE loop is to find the neighbors of the point # in current graph WHILE (!candidate.empty()) nearest_candidate = candidate.pop_nearest(point) furthest_neighbor = neighbors.get_furthest(point) # no candidate can b closer IF (distant(nearest_candidate, point) \u0026gt; distant(furthest_neighbor, point)) break; # This candidate is great, but what about its neighbors? FOREACH candidate_neighbor in nearest_candidate.neighbors() IF (visited_points.has(candidate_neighbor)) continue visited_points.append(candidate_neighbor) # the furthest one can be changed furthest_neighbor = neighbors.get_furthest(point) # The neighbor of this candidate is also great, its neighbors # can also be candidates IF (distant(candidate_neighbor, point) \u0026lt; distant(furthest_neighbor, point)) candidate.append(candidate_neighbor) neighbors.append(candidate_neighbor) IF (neighbors.size() \u0026gt; MAX_NEIGHBORS) neighbors.pop_furthest(point) # Now we have found the neighbors, add a bidirection connections # between the each neighbor and the point FOREACH this_neighbor in neighbors add_bidirection_direction(this_neighbor, point) # Since the neighbor has one more connection, we may need # to shrink. This is a point to optimize. Read paper for detail. IF this_neighbor.neighbors().size() \u0026gt; MAX_NEIGHBORS this_neighbor.drop_longest_connection() # All points have been added RETURN G NSW的搜索 OUTPUT: graph G, point P RETURN K nearest neighbors SEARCH_LAYER(G, p, K) candidates = select_one_random(G) visited_points = candidates # LOOP until we have K stable points WHILE TRUE candidates_old = candidates FOREACH candidate in candidates FOREACH neighbor in candidate.neighbors() if (visited_points.has(neighbor)) continue visited_points.add(neighbor) furthest_candidate = candidates.get_furthest(point) IF (distant(neighbor, P) \u0026lt; distant(furthest_candidate, P) || candidates.size() \u0026lt; K) candidates.add(neighbor) IF (candidates.size() \u0026gt; K) candidates.pop_furthest(P) IF candidates_old == candidates BREAK RETURN candidates HNSW —— NSW的进化 显然，上述过程最大的问题之一为：\n对于图的构建：每新加入一个点，都需要从一个随机点开始搜索它的邻居。 对于图的搜索：需要从一个随机点开始搜索。 以上两点导致，NSW算法搜索了很多无用的点。 H(hierarchy)NSW 为解决这个问题，从NSW图（layer=0）中选出部分点，再构建一个缩略的NSW图（layer=1）。在搜索的时候，只需要从layer=1的图中搜索出一个粗略结果，将该结果用于layer=0搜索过程中的初始化，即可大量减少无用的搜索。同理，层数也不一定只有2层，可以有更多。 （这个思想在科研中似乎经常使用:先得出一个粗略的结果，再进一步精细化） 为了构建一个这样的图，我们在插入一个点时。\nINPUT: point P, a series of NSW graph G[] cur_layer = -ln(unif(0, 1)) * MAX_LAYER # for layer upper than current layer, just get a candidate FOR l from MAX_LAYER to cur_layer + 1 closest_points = SEARCH_LAYER(G[l], P, 1, candidates = closest_point) # insert into each layer from top to bottom of the below layers FOR l from cur_layer to 0 closest_points = SEARCH_LAYER(G[l], P, 1, candidates = closest_points) INSERT_POINT(graph, P, neighbors = closest_points) 同理在搜索时\nINPUT: point P, a series of NSW graph G[] cur_layer = P.layer # for layer upper than current layer, just get a candidate FOR l from MAX_LAYER to cur_layer + 1 closest_points = SEARCH_LAYER(G[l], P, 1, candidates = closest_point) # for layer leq than current layer, just get a candidate FOR l from cur_layer to 0 closest_points = SEARCH_LAYER(G[l], P, 1, candidates = closest_point) return closest_points PGVECTOR中的算法实现 INSERT /* * Algorithm 1 from paper: update graph by inserting an element * Parms: * @element: element to insert * @entryPoint: the initial entry point * @index? * @procinfo * @collation * @m: same as \u0026#34;M\u0026#34; in algo(number of established connections) */ HnswInsertElement(HnswElement element, HnswElement entryPoint, Relation index, FmgrInfo *procinfo, Oid collation, int m, int efConstruction, bool existing) level = element-\u0026gt;level; q = PointerGetDatum(element-\u0026gt;vec) # fill entry point list with the initial one ep = list_make1(HnswEntryCandidate(entryPoint,)) # for layers upper than the element\u0026#39;s level for (int lc = entryLevel; lc \u0026gt;= level + 1; lc--) # only get the nearest element now w = HnswSearchLayer() ep = w; # for the below layers for (int lc = level; lc \u0026gt;= 0; lc) # search for top efConstruction nearest ones w = HnswSearchLayer(efConstruction) lw = w # get neighbors neighbors = SelectNeighbors(lw, lm, lc, procinfo, collation, NULL); # add connection # Is this different from paper? # bidirectional vs single directional # shrink directions or not shrink AddConnections() foreach(lc2, neighbors) a-\u0026gt;items[a-\u0026gt;length++] = *((HnswCandidate *) lfirst(lc2)); search layer /* * Algorithm 2 from paper: search this layer with specifiyed enter points to * return \u0026#34;ef\u0026#34; closest neighbors * Parms: * @q: same as algo * @ep: enter points * @ef: count of closest neighbors * @lc: layer number * @index: * @procinfo: * @collation: * @inserting: * @skipElement: */ List * HnswSearchLayer(Datum q, List *ep, int ef, int lc, Relation index, FmgrInfo *procinfo, Oid collation, int m, bool inserting, HnswElement skipElement) v = NULL. # visited points C = NULL # set of candidates, nearer first W = NULL # dynamic found nearest neighbors # for each candidate in enter points foreach(lc2, ep) hc = (HnswCandidate *) lfirst(lc2); # HNSW candidates v.add(hc) C.add(hc) W.add(hc) # loop until no more candidates while (!C.empty()) c = C.pop_nearest() # for each neighbor \u0026#34;e\u0026#34; in the nearest candicate \u0026#34;c\u0026#34; neighborhood = \u0026amp;c-\u0026gt;element-\u0026gt;neighbors[lc]; for (int i = 0; i \u0026lt; neighborhood-\u0026gt;length; i++) # neighbor e HnswCandidate *e = \u0026amp;neighborhood-\u0026gt;items[i]; v.add(e) DO # continue if visited # f is the furthest element in dynamic neighbors f = W.furthest() # find a good neighbor who is closer to q than the worst one in W if (DISTANT(e, q) \u0026lt; DISTANT(f, q) || wlen \u0026lt; ef) ec = e # neighbor of ec can also be the candidates C.add(ec) # add ec to W to promote the lower bound W.add(ec) # clean W if it\u0026#39;s too large if (skipElement == NULL || list_length(e-\u0026gt;element-\u0026gt;heaptids) != 0) wlen++; /* No need to decrement wlen */ if (wlen \u0026gt; ef) W.pop_furthest return W pairing heap 配对堆 - OI Wiki (oi-wiki.org)\ninsert($\\mathrm{log}n$) random_select($\\mathrm{log} n$) select_min($\\mathrm{log} n$) delete_min($\\mathrm{log} n$) select neighbors /* * Algorithm 4: select neighbors starting with specified candidates * PARAMS: * @c : candidates * @m : number of neighbors to return * @lc: layer number * @ * * NOTES: * extendCandidates = false * keepPrunedConnections = true * pruned */ static List * SelectNeighbors(List *c, int m, int lc, FmgrInfo *procinfo, Oid collation, HnswCandidate * *pruned) r = NULL # results---returning neighbors w = c # working candidates wd = NULL; # discarded candidates; # Since we don\u0026#39;t extend candidates, if the starting candidates isn\u0026#39;t enought # just return. if (list_length(w) \u0026lt;= m) return w # loop untils no more working candidate or enought neighbors while (length(w) \u0026gt; 0 \u0026amp;\u0026amp; length(r) \u0026lt; m) *e = llast(w); # get the nearest candidates closer = CheckElementCloser(e, r, lc, procinfo, collation); if(closer) r.append(e) else wd.append(e) # loop until discarded candidates are empty or enough neighbors while (!wd.empty() \u0026amp;\u0026amp; length(r) \u0026lt; m) r.append(wd.pop_nearest()) prune = wd.nearest() return r data structure typedef struct HnswElementData { List\t*heaptids; uint8\tlevel; uint8\tdeleted; HnswNeighborArray *neighbors; BlockNumber blkno; OffsetNumber offno; OffsetNumber neighborOffno; BlockNumber neighborPage; Vector\t*vec; }\tHnswElementData; typedef struct HnswCandidate { HnswElement element; float\tdistance; }\tHnswCandidate; typedef struct HnswNeighborArray { int\tlength; HnswCandidate *items; }\tHnswNeighborArray; 底层实现中的问题 论文中的图是双向连接，而pgvector实现的是单向连接 pgvector中插入新向量时，没有更新其邻居的连接。（这么低级的问题有待验证） page representation vector database 调研 qdrant Vector databases are optimized for storing and querying these high-dimensional vectors efficiently, and they often using specialized data structures and indexing techniques such as Hierarchical Navigable Small World (HNSW) – which is used to implement Approximate Nearest Neighbors – and Product Quantization, among others.\n算法与存储 qdrant使用 hnsw 算法\nA key feature of Qdrant is the effective combination of vector and traditional indexes. It is essential to have this because for vector search to work effectively with filters, having vector index only is not enough. In simpler terms, a vector index speeds up vector search, and payload indexes speed up filtering.\npayload 索引仅用于过滤，我们关注向量索引部分\nQdrant currently only uses HNSW as a vector index.\nAll data within one collection is divided into segments. Each segment has its independent vector and payload storage as well as indexes.\n附录 trianlge-inequality-Kmeans 维护的状态： lower bound $l(x,c)$ of $d(x,c)$ for each point $x$ and center $c$ each time $d(x,c)$ is computed, set $l(x,c)=d(x,c)$ $c(x)= \\mathrm{argmin}_cd(x,c)$ get its center for each point $x$ upper bound $u(x)$ of $d(x,c)$ for each point $x$, indicating the upper bound of $x$ to its center $r(x)$ is a boolean value indicate whether $u(x, c)$ is out of date 过程： initialization compute $d(x,c)$ for each point $x$ and each center $c$, which means $l(x,c)$ is computed too $u(x)=\\mathrm{min}_c(d,c)$ for each point $x$ repeate until convergence: For each pair of centers $c$ and $c\u0026rsquo;$ , compute $d(c,c\u0026rsquo;)$, this is to compute $s(c)=1/2\\min_{c\u0026rsquo;\\neq c}d(c,c\u0026rsquo;)$ . So we get the distance to the nearest center of each center identify all point $x$ such that $u(x) \\le s(c(x))$ .If the point is so near to its center, its center can\u0026rsquo;t be changed in this iteration. See lemma 1 For each remaining point $x$ and centers $c$ such that $c\\neq c(x)$ (not the current center) and $u(x)\u0026gt;l(x,c)$ (upper bound to current center greater than lower bound of this center) and $u(x)\u0026gt;\\frac{1}{2}d(c(x),c)$(upper bound to current center greater than half of the two centers distant, See lemma 1) iterm 2 and 3 means $u(x)$ may be too big DO: If $r(x)$ , compute$d(x, c(x))$ and set $r(x)=false$, else $d(x,c(x))=u(x)$ if $u(x)\u0026gt;l(x,c)$ and $u(x)\u0026gt;\\frac{1}{2}d(c(x),c)$ then (same as the above) compute $d(x,c)$, if $d(x,c) \u0026lt; d(x,c(x))$ then assign $c(x)=c$ (update center) For each center $c$ , let $m(c)$ be the new mean point For each point $x$ and center $c$, assign $l(x,c)=\\max{l(x,c)-d(c,m(c))}$ (update lower bound by lemma 2) For each point x, assign $u(x)=u(x) + d(m(c(x)),c(x))$ (update lower bound by lemma 2 ) and $r(x)=true$ Replace each center $c$ by $m(c)$ 采样算法 Knuth\u0026rsquo;s algorithm S 算法描述： Select $n$ items from a set of $M$ iems with equal probility for $M \\geq n$ 实现 samples = set[0:n-1] for i in (n, M) with prob = n/i: samples[random()%n] = set[i] 参考文档 Knuth\u0026rsquo;s algorithm S - Rosetta Code 。\n社区讨论 [PostgreSQL: Re: GENERAL] Large DB\n后记 pg官方的新闻：PostgreSQL: pgvector 0.5.0 Released! 。pgvector在社区的热度不小 ","permalink":"https://mobilephone724.github.io/article/pgvector/","summary":"序言 pgvector是一个向量搜索（根据近似度）的插件，用来加速AKNN（approximate nearest neighbor）。 PASE中提到，向量ANN算法包括4类\ntree-based algorithms KD-Tree RTree quantization-based algorithms IVFFlat IVFADC IMI graph based algorithms HNSW NSG SSG hash-base algorithms LSH pgvector 包括两个算法，IVFFlat 和 HNSW，后续内容将以这两个算法的内容及其实现展开。 IVFFlat 概览 IVFFlat 算法主要包括以下几个步骤\n索引构建阶段 使用 KMeans 将数据集划分成多个簇(cluster) 查询阶段 通过每个簇的中心点（向量是高维的点）获取N个最近的簇 遍历这N个簇的所有点，从中找到最近的K个点 算法介绍 基础算法kmeans reference k-means clustering - Wikipedia 算法目标：选取K个中心点，使得数据集中的所有点到其最近的中心点“距离”之和最近，以平方和距离为例：\nGiven a set of observations $(x_1, x_2, \\dots, x_n)$, where each observation is a $d$-dimensional real vector, k-means clustering aims to partition the $n$ observations into $k$ ($\\leq n$) sets $S = {S_1, S_2, \\dot, S_k}$ so as to minimize the within-cluster sum of squares (WCSS).","title":"PGVECTOR AND VECTOR DATABASE"},{"content":"本文主要为SLRU本身的结构解读。\n简述 slru用来干什么？ slru是一个简单的buffer管理模块，simple slru 有了buffer pool manager，为什么还要slru？ bpm管理通用的page，比如heap，vm等 slru最大的特点就是lru，非常适合处理xid这样，递增的信息。 下面的代码分析基于pg15 存储结构 与bpm不同，通过slru管理的page，其文件大小固定，一个文件有32个page，一个page有8KB，故一个文件最大为256K。\n与WAL不同，WAL文件的大小在创建时就已经确定为16M，与WAL文件重用保持一致，而slru的文件，先在内存中产生相应的page，再会去落盘。\n#define SLRU_PAGES_PER_SEGMENT\t32 内存slru 全局 buffer 数组 typedef struct SlruSharedData { LWLock\t*ControlLock; /* Number of buffers managed by this SLRU structure */ int\tnum_slots; /* * Arrays holding info for each buffer slot. Page number is undefined * when status is EMPTY, as is page_lru_count. */ char\t**page_buffer; SlruPageStatus *page_status; bool\t*page_dirty; int\t*page_number; int\t*page_lru_count; LWLockPadded *buffer_locks; XLogRecPtr *group_lsn; int\tlsn_groups_per_page; /*---------- * We mark a page \u0026#34;most recently used\u0026#34; by setting *\tpage_lru_count[slotno] = ++cur_lru_count; * The oldest page is therefore the one with the highest value of *\tcur_lru_count - page_lru_count[slotno] * The counts will eventually wrap around, but this calculation still * works as long as no page\u0026#39;s age exceeds INT_MAX counts. *---------- */ int\tcur_lru_count; } SlruSharedData; 从内存结构上看，是一个数组，每个元素代表一个page。同时，记录这些page的使用次数。\npage_lru_count[slotno] = ++cur_lru_count; 同时每个page，都有状态标识，以在刷脏时，确定脏页。实际上这里没有脏页这个选项，因为只有 valid 状态的页才有可能是脏页，有包含关系。故在SlruSharedData 中使用 page_dirty 进行单独标识。\ntypedef enum { SLRU_PAGE_EMPTY,\t/* buffer is not in use */ SLRU_PAGE_READ_IN_PROGRESS, /* page is being read in */ SLRU_PAGE_VALID,\t/* page is valid and not being written */ SLRU_PAGE_WRITE_IN_PROGRESS /* page is being written out */ } SlruPageStatus; 关于为什么需要记录LSN信息 group_lsn：这与 WAL 设计有关。对于 WAL 而言，无论是同步提交或是异步提交，都需要在对应的 buffer page 落盘前落盘,所以 slru 也需要满足这样的规则。同时，可能是为了节约内存（节约的内存实在有限），或是减少WAL flush的调用次数以增加 IO 效率，slru的实现中并不记录每个buffer page的 LSN，而是记录一组 page 的 LSN，在刷下一个 page 前，需要把一组 page 中最大的 LSN 前的 WAL 落盘。而这样的“一组”的长度，就为lsn_groups_per_page\n各个进程私有的pointer /* * SlruCtlData is an unshared structure that points to the active information * in shared memory. */ typedef struct SlruCtlData { SlruShared\tshared; /* * Decide whether a page is \u0026#34;older\u0026#34; for truncation and as a hint for * evicting pages in LRU order. */ bool\t(*PagePrecedes) (int, int); /* * Dir is set during SimpleLruInit and does not change thereafter. Since * it\u0026#39;s always the same, it doesn\u0026#39;t need to be in shared memory. */ char\tDir[64]; } SlruCtlData; 初始化时，即返回一个SlruCtlData。Dir 是初始化时的标记，不同模块会填充对应的名称。\n核心功能 SimpleLruZeroPage：新增一个page SimpleLruReadPage ：读一个page SimpleLruWritePage ：写一个page 基础函数 选择一个空slot /* Select the slot to re-use when we need a free slot. */ /* Control lock must be held at entry, and will be held at exit. */ static int SlruSelectLRUPage(SlruCtl ctl, int pageno) { for (;;) # return if we have such a slot # return if we have an empty slot \u0026#34;SLRU_PAGE_EMPTY\u0026#34; # select a lru slot # return it if it\u0026#39;s clean. Or # victim it if dirty # loop end -- It\u0026#39;s a very clever design to dealing with corner cases # such as the victim page being re-dirtied while we wrote it. } 记录一个\u0026quot;most recently used\u0026quot;的page，cur_lru_count++ 并用其赋值 #define SlruRecentlyUsed(shared, slotno)\t\\ do { \\ int\tnew_lru_count = (shared)-\u0026gt;cur_lru_count; \\ if (new_lru_count != (shared)-\u0026gt;page_lru_count[slotno]) { \\ (shared)-\u0026gt;cur_lru_count = ++new_lru_count; \\ (shared)-\u0026gt;page_lru_count[slotno] = new_lru_count; \\ } \\ } while (0) 从磁盘中读取一个 page SlruPhysicalReadPage { int\tsegno = pageno / SLRU_PAGES_PER_SEGMENT; SlruFileName(ctl, path, segno); /* * In a crash-and-restart situation, it\u0026#39;s possible for us to receive * commands to set the commit status of transactions whose bits are in * already-truncated segments of the commit log */ fd = OpenTransientFile(path, O_RDONLY | PG_BINARY); if (fd \u0026lt; 0 \u0026amp;\u0026amp; !InRecovery) ereport() pg_pread(fd, shared-\u0026gt;page_buffer[slotno], BLCKSZ, offset) } 向磁盘中写入一个 page SlruPhysicalWritePage { /* We must flush WAL before flush slru pages */ if (shared-\u0026gt;group_lsn != NULL) { max_lsn = shared-\u0026gt;group_lsn[lsnindex++]; XLogFlush(max_lsn); } SlruFileName(ctl, path, segno); fd = OpenTransientFile(path, O_RDWR | O_CREAT | PG_BINARY); pg_pwrite(fd, shared-\u0026gt;page_buffer[slotno], BLCKSZ, offset) /* Queue up a sync request for the checkpointer. */ ... } interface 新增一个 page 到buffer。 /* Initialize (or reinitialize) a page to zeroes. */ int SimpleLruZeroPage(SlruCtl ctl, int pageno) { slotno = SlruSelectLRUPage(ctl, pageno); SlruRecentlyUsed(shared, slotno); # SlruSelectLRUPage may return a in-use page, we must clear it MemSet(shared-\u0026gt;page_buffer[slotno], 0, BLCKSZ); SimpleLruZeroLSNs(ctl, slotno); } 从 disk 中读取一个 page /* Control lock must be held at entry, and will be held at exit. */ SimpleLruReadPage { #infinite loop slotno = SlruSelectLRUPage(ctl, pageno); # for in IO slots, just wait /* update in-memory status */ shared-\u0026gt;page_number[slotno] = pageno; shared-\u0026gt;page_status[slotno] = SLRU_PAGE_READ_IN_PROGRESS; shared-\u0026gt;page_dirty[slotno] = false; /* Acquire per-buffer lock and release control lock */ LWLockAcquire(\u0026amp;shared-\u0026gt;buffer_locks[slotno].lock, LW_EXCLUSIVE); LWLockRelease(shared-\u0026gt;ControlLock); ok = SlruPhysicalReadPage(ctl, pageno, slotno); /* re-acquire control lock */ LWLockAcquire(shared-\u0026gt;ControlLock, LW_EXCLUSIVE); # others } 这里的锁设计很特别：\n在 SlruSelectLRUPage 需要获取全局锁 在 SimpleLruReadPage 中，先初始化内存，再获取 per-buffer 锁，同时释放 ControlLock 在看函数 SimpleLruZeroPage\n/* Control lock must be held at entry, and will be held at exit. */ SimpleLruZeroPage { slotno = SlruSelectLRUPage(ctl, pageno); shared-\u0026gt;page_number[slotno] = pageno; shared-\u0026gt;page_status[slotno] = SLRU_PAGE_VALID; shared-\u0026gt;page_dirty[slotno] = true; } 难道，一旦获取 ControlLock，即可对任意 slot 进行修改？\n实际上，SimpleLruReadPage 读取的 page，必须已存在于磁盘（或者经由 WAL 来保证）。 而 SimpleLruZeroPage 所初始化的 page 必须不存在。从使用逻辑上保证二者不产生冲突。\nSimpleLruWritePage(SlruInternalWritePage) /* Control lock must be held at entry, and will be held at exit. */ SlruInternalWritePage { /* If a write is in progress, wait for it to finish */ /* Do nothing if page is not dirty */ /* update in-memory status */ shared-\u0026gt;page_status[slotno] = SLRU_PAGE_WRITE_IN_PROGRESS; shared-\u0026gt;page_dirty[slotno] = false; /* Acquire per-buffer lock and release control lock */ LWLockAcquire(\u0026amp;shared-\u0026gt;buffer_locks[slotno].lock, LW_EXCLUSIVE); LWLockRelease(shared-\u0026gt;ControlLock); SlruPhysicalWritePage(ctl, pageno, slotno, fdata); /* re-acquire control lock */ LWLockAcquire(shared-\u0026gt;ControlLock, LW_EXCLUSIVE); shared-\u0026gt;page_status[slotno] = SLRU_PAGE_VALID; } ","permalink":"https://mobilephone724.github.io/article/slru/","summary":"本文主要为SLRU本身的结构解读。\n简述 slru用来干什么？ slru是一个简单的buffer管理模块，simple slru 有了buffer pool manager，为什么还要slru？ bpm管理通用的page，比如heap，vm等 slru最大的特点就是lru，非常适合处理xid这样，递增的信息。 下面的代码分析基于pg15 存储结构 与bpm不同，通过slru管理的page，其文件大小固定，一个文件有32个page，一个page有8KB，故一个文件最大为256K。\n与WAL不同，WAL文件的大小在创建时就已经确定为16M，与WAL文件重用保持一致，而slru的文件，先在内存中产生相应的page，再会去落盘。\n#define SLRU_PAGES_PER_SEGMENT\t32 内存slru 全局 buffer 数组 typedef struct SlruSharedData { LWLock\t*ControlLock; /* Number of buffers managed by this SLRU structure */ int\tnum_slots; /* * Arrays holding info for each buffer slot. Page number is undefined * when status is EMPTY, as is page_lru_count. */ char\t**page_buffer; SlruPageStatus *page_status; bool\t*page_dirty; int\t*page_number; int\t*page_lru_count; LWLockPadded *buffer_locks; XLogRecPtr *group_lsn; int\tlsn_groups_per_page; /*---------- * We mark a page \u0026#34;most recently used\u0026#34; by setting *\tpage_lru_count[slotno] = ++cur_lru_count; * The oldest page is therefore the one with the highest value of *\tcur_lru_count - page_lru_count[slotno] * The counts will eventually wrap around, but this calculation still * works as long as no page\u0026#39;s age exceeds INT_MAX counts.","title":"SLRU"},{"content":" From access/transam/README\nWrite-Ahead Log Coding 基本思想，日志在数据页前落盘\nLSN：刷脏前检查LSN对应的日志已经落盘 优势：仅在必要的时候等待XLOG的IO。（异步IO） LSN的检查模块只用在 buffer manager 中实现 在WAL回放时，避免相同的日志被重复回放（可重入）。（TODO：full page write是否在另一个层面上保证了可重入） WAL 包含一个（或一小组）页的增量更新的重做信息。 依赖文件系统和硬件的原子写，不可靠！ checkpoint，checkpointer后的第一次写全页。通过 checkpoint 留下的 LSN 来判断是否为第一次写 写下WAL日志的逻辑为 pin and exclusive-lock the shared buffer START_CRIT_SECTION，发生错误时确保整个数据库能立即重启 在shared buffer上，进行对应的修改 标记为脏页， 必须在WAL日志写入前完成（TODO，为什么？SyncOneBuffer） 只有在要写WAL时，才能标记脏页（TODO，为什么？） 使用XLogBeginInsert 和 XLogRegister* 函数构建WAL，使用返回的LSN来更新page END_CRIT_SECTION，退出 解锁和unpin （注意顺序） 一些复杂的操作，需要原子地写下一串WAL记录，但中间状态必须自洽(self-consistent)。这样在回放wal日志时，如果中断，系统还能够正常运行。注意：此时相当于事务回滚，但是其部分更改已经落盘。举例：\n在btree索引中，页的分裂分为两步（1）分配一个新页（2）在上一层的页(parent page)中新插入一条数据。 但是因为锁，这会形成两个独立的WAL日志。在回放WAL日志时 回放第（1）个日志： 分配一个新页，将元组移动进去 设置标记位，表示上一层的页没有更新 回放第（2）个日志： 在上一层的页中新插入一条数据 清除第（1）个日志中的标记位 标志位通常情况下不可见，因为对 child page 的修改时持有的锁，在两个操作完成后才会释放。 仅在写下第（2）个日志前，数据库恰好崩溃，标志位才会被感知。（该标志位应该没有MVCC，否则会在事务层屏蔽） 搜索时，不管这个中间状态 插入时，如果发现这个中间状态，先在上一层的页插入对应key，以修复这个“崩溃”状态，再继续插入 ","permalink":"https://mobilephone724.github.io/article/wal-basic/","summary":" From access/transam/README\nWrite-Ahead Log Coding 基本思想，日志在数据页前落盘\nLSN：刷脏前检查LSN对应的日志已经落盘 优势：仅在必要的时候等待XLOG的IO。（异步IO） LSN的检查模块只用在 buffer manager 中实现 在WAL回放时，避免相同的日志被重复回放（可重入）。（TODO：full page write是否在另一个层面上保证了可重入） WAL 包含一个（或一小组）页的增量更新的重做信息。 依赖文件系统和硬件的原子写，不可靠！ checkpoint，checkpointer后的第一次写全页。通过 checkpoint 留下的 LSN 来判断是否为第一次写 写下WAL日志的逻辑为 pin and exclusive-lock the shared buffer START_CRIT_SECTION，发生错误时确保整个数据库能立即重启 在shared buffer上，进行对应的修改 标记为脏页， 必须在WAL日志写入前完成（TODO，为什么？SyncOneBuffer） 只有在要写WAL时，才能标记脏页（TODO，为什么？） 使用XLogBeginInsert 和 XLogRegister* 函数构建WAL，使用返回的LSN来更新page END_CRIT_SECTION，退出 解锁和unpin （注意顺序） 一些复杂的操作，需要原子地写下一串WAL记录，但中间状态必须自洽(self-consistent)。这样在回放wal日志时，如果中断，系统还能够正常运行。注意：此时相当于事务回滚，但是其部分更改已经落盘。举例：\n在btree索引中，页的分裂分为两步（1）分配一个新页（2）在上一层的页(parent page)中新插入一条数据。 但是因为锁，这会形成两个独立的WAL日志。在回放WAL日志时 回放第（1）个日志： 分配一个新页，将元组移动进去 设置标记位，表示上一层的页没有更新 回放第（2）个日志： 在上一层的页中新插入一条数据 清除第（1）个日志中的标记位 标志位通常情况下不可见，因为对 child page 的修改时持有的锁，在两个操作完成后才会释放。 仅在写下第（2）个日志前，数据库恰好崩溃，标志位才会被感知。（该标志位应该没有MVCC，否则会在事务层屏蔽） 搜索时，不管这个中间状态 插入时，如果发现这个中间状态，先在上一层的页插入对应key，以修复这个“崩溃”状态，再继续插入 ","title":"WAL基础"},{"content":"接口函数 一个WAL记录包含\nWAL记录类型。（TODO不同的修改有不同的记录方式？） 这个页的修改方式 被修改的页的信息。被修改的页通过一个唯一ID标识，也可以有更多的关联数据（\u0026ldquo;record-specific data associated with the block\u0026rdquo;）。如果要写full page，就没有关联数据 构建一个WAL记录包含5个核心函数 void XLogBeginInsert(void) 初始化相关状态 如果当前无法构建WAL日志（例如在recovery模式），则报错 void XLogRegisterBuffer(uint8 block_id, Buffer buf, uint8 flags); 增加了数据块的信息；注册一个buffer的引用，相当于上述WAL日志的第三部分 block_id is an arbitrary number used to identify this page reference in the redo routine\n在redo阶段，可以根据这些信息找到需要redo的page regbuf = \u0026amp;registered_buffers[block_id]; /* * Returns the relfilenode, fork number and block number associated with * a buffer */ BufferGetTag(buffer, \u0026amp;regbuf-\u0026gt;rnode, \u0026amp;regbuf-\u0026gt;forkno, \u0026amp;regbuf-\u0026gt;block); regbuf-\u0026gt;page = BufferGetPage(buffer); regbuf-\u0026gt;flags = flags; regbuf-\u0026gt;rdata_tail = (XLogRecData *) \u0026amp;regbuf-\u0026gt;rdata_head; regbuf-\u0026gt;rdata_len = 0; registered_buffer的结构\ntypedef struct { /* xxx */ /* info to re-find the page */ ForkNumber\tforkno; BlockNumber block; Page\tpage; /* a loop-linked structure to store the data change of each buffer */ uint32 rdata_len; /* total length of data in rdata chain */ XLogRecData *rdata_head; /* head of the chain of data registered with * this block */ XLogRecData *rdata_tail;\t/* last entry in the chain, or \u0026amp;rdata_head if * empty */ /* xxx */ } registered_buffer; typedef struct XLogRecData { struct XLogRecData *next; /* next struct in chain, or NULL */ char *data; /* start of rmgr data to include */ uint32 len; /* length of rmgr data to include */ } XLogRecData; void XLogRegisterData(char *data, int len); 向WAL日志中写入任意数据 可多次调用，保证连续。这样在rodo时，就可以得到连续的数据 rdata = \u0026amp;rdatas[num_rdatas++]; rdata-\u0026gt;data = data; rdata-\u0026gt;len = len; void XLogRegisterBufData(uint8 block_id, char *data, int len); rdata = \u0026amp;rdatas[num_rdatas++]; rdata-\u0026gt;data = data; rdata-\u0026gt;len = len; regbuf = \u0026amp;registered_buffers[block_id]; regbuf-\u0026gt;rdata_tail-\u0026gt;next = rdata; regbuf-\u0026gt;rdata_tail = rdata; regbuf-\u0026gt;rdata_len += len; 可见，XLogRegisterBufData 和 XLogRegisterData 的核心区别在，前者写入的数据会关联到具体的buffer，而后者没有\nXLogInsert Insert the record. do { GetFullPageWriteInfo(\u0026amp;RedoRecPtr, \u0026amp;doPageWrites); rdt = XLogRecordAssemble(rmid, info, RedoRecPtr, doPageWrites, \u0026amp;fpw_lsn, \u0026amp;num_fpi); EndPos = XLogInsertRecord(rdt, fpw_lsn, curinsert_flags, num_fpi); } while (EndPos == InvalidXLogRecPtr); 数据结构汇总 registered_buffers 每一个buffer对应registered_buffers中的一个元素（一个registered buffer）\nvoid XLogEnsureRecordSpace(int max_block_id, int ndatas) { if (nbuffers \u0026gt; max_registered_buffers) { registered_buffers = (registered_buffer *) repalloc(registered_buffers, sizeof(registered_buffer) * nbuffers); max_registered_buffers = nbuffers; } } 具体的插入方式 上述代码中的XLogRecordAssemble和XLogInsertRecord已经概括了具体的插入步骤\nXLogRecordAssemble Assemble a WAL record from the registered data and buffers into an XLogRecData chain\nstatic XLogRecData * XLogRecordAssemble(RmgrId rmid, uint8 info, XLogRecPtr RedoRecPtr, bool doPageWrites, XLogRecPtr *fpw_lsn, int *num_fpi) { for (block_id = 0; block_id \u0026lt; max_registered_block_id; block_id++) { if (needs_data) { rdt_datas_last-\u0026gt;next = regbuf-\u0026gt;rdata_head; } } } ","permalink":"https://mobilephone724.github.io/article/wal-insert/","summary":"接口函数 一个WAL记录包含\nWAL记录类型。（TODO不同的修改有不同的记录方式？） 这个页的修改方式 被修改的页的信息。被修改的页通过一个唯一ID标识，也可以有更多的关联数据（\u0026ldquo;record-specific data associated with the block\u0026rdquo;）。如果要写full page，就没有关联数据 构建一个WAL记录包含5个核心函数 void XLogBeginInsert(void) 初始化相关状态 如果当前无法构建WAL日志（例如在recovery模式），则报错 void XLogRegisterBuffer(uint8 block_id, Buffer buf, uint8 flags); 增加了数据块的信息；注册一个buffer的引用，相当于上述WAL日志的第三部分 block_id is an arbitrary number used to identify this page reference in the redo routine\n在redo阶段，可以根据这些信息找到需要redo的page regbuf = \u0026amp;registered_buffers[block_id]; /* * Returns the relfilenode, fork number and block number associated with * a buffer */ BufferGetTag(buffer, \u0026amp;regbuf-\u0026gt;rnode, \u0026amp;regbuf-\u0026gt;forkno, \u0026amp;regbuf-\u0026gt;block); regbuf-\u0026gt;page = BufferGetPage(buffer); regbuf-\u0026gt;flags = flags; regbuf-\u0026gt;rdata_tail = (XLogRecData *) \u0026amp;regbuf-\u0026gt;rdata_head; regbuf-\u0026gt;rdata_len = 0; registered_buffer的结构","title":"WAL日志的插入"},{"content":"从0证明RSA RSA 算法（即一个非对称加密算法）除了应用非常广泛外，其特性也非常吸引人（起码非常吸引我）。我在网上找了很多关于RSA的证明，要么不够详细（例如缺失对前置定理的证明），要么需要引出较多复杂的数论概念。作者本身水平不高，试图绕过这些复杂的概念，从初等数学的开始，完备地证明RSA。\n关于RSA的背景知识可能很多，可以慢慢阅读，我在此尝试从初等数学开始证明。这些背景知识的证明有一定的顺序，如果读者发现某个证明看不懂，可以向前翻阅。\n参考的文章如下：（因为参考的文章太多，大概率不全）\n费马小定理 中国剩余定理 阮一峰的博客——RSA算法原理（一） 阮一峰的博客——RSA算法原理（二） 初等数论笔记Part 1： 欧拉定理 算法学习笔记(9)：逆元 费马小定理 简介 如果 $p$ 是质数且 $\\mathrm{gcd}(a,p)=1$ , 那么 $a^{p-1}\\equiv 1\\ (\\mathrm{mod}\\ p)$\n在证明该定理前，先证明一个简单的引理\n引理1 如果 $p$ 是质数，且 $\\mathrm{gcd}(a,p)=1$ , 那么\n$$ \\lbrace ka \\ \\mathrm{mod}\\ p | k = \\lbrace 1,2,\u0026hellip;,p -1 \\rbrace \\rbrace= \\lbrace 1,2,3,\u0026hellip;,p-1 \\rbrace $$\n即二者存在一对一的关系。由于这两个集合的元素个数相同，所以只要证明左侧集合没有重复元素即可\n证明：假设存在 $k_1$ 和 $k_2$ 满足 $1 \\leq k_1 \u0026lt; k_2 \\leq p-1$ ，且 $k_1a\\ \\mathrm{mod}\\ p = k_2a\\ \\mathrm{mod}\\ p$ . 那么可知\n$$ (k_1+p-k_2)a\\ \\mathrm{mod}\\ p = pa\\ \\mathrm{mod}\\ p=0 $$\n即 $(k_1+p-k_2)a\\ \\mathrm{mod}\\ p=0$ 。由于 $p$ 是质数，那么 $(k_1+p-k_2)a$ 一定是 $p$​​ 的倍数，这显然不可能。\n证明费马小定理 $$ \\begin{align} (1a)(2a)(3a)\u0026hellip;((p-1)a)\\ \\mathrm{mod}\\ p \u0026amp;= (a^{p-1}(p-1)!)\\ \\mathrm{mod}\\ p \\newline (1a\\ \\mathrm{mod}\\ p)(2a\\ \\mathrm{mod}\\ p)\u0026hellip;((p-1)a\\ \\mathrm{mod}\\ p) \u0026amp;= (a^{p-1}(p-1)!)\\ \\mathrm{mod}\\ p\\newline (p-1)! \u0026amp;= (a^{p-1}(p-1)!)\\ \\mathrm{mod}\\ p \\end{align} $$\n整理得 $a^{p-1}\\equiv 1\\ (\\mathrm{mod}\\ p)$​\n模逆元 简介 定义： $a$ 对 $n$ 的模逆元是满足 $ab\\equiv 1\\ (\\mathrm{mod}\\ n)$ 的 $b$​\n模逆元的存在性：模逆元存在的充要条件是 $\\mathrm{gcd}(a,n)=1$​\n为证明该存在性定理，需要先证明引理2\n引理2 若 $\\mathrm{gcd}(a,n)=g$ ，则存在 $x,y\\in Z$，满足 $ax+ny=g$\n证明：\n设集合 $S=\\lbrace ax+ny|x,y\\in Z \\rbrace $，显然，存在 $s\\in S$ 并且 $s\u0026gt;0$ 设 $d$ 为 $S$ 中最小的，大于 $0$ 的元素 若 $a\\ \\mathrm{mod}\\ p\\neq 0$ ，则存在 $k,r$ 满足 $a=kd+r$ ，其中 $0 \u0026lt; r \u0026lt; k$ ，带入 $d=ax_0+ny_0$ ，得到 $r=a(1-kx_0)+n(-ky_0)$ 。 显然 $r\\in S$ ，又 $0 \u0026lt; r \u0026lt; k$ ，这与 $d$ 为 $S$ 中最小的大于 $0$ 的假设不符。 故 $a\\ \\mathrm{mod}\\ d=0$ ，同理 $n\\ \\mathrm{mod}\\ d=0$ 所以 $d$ 为 $a$ 和 $n$ 共同的因数。设 $g=ld$ ， $l\\geq0$ 且 $l\\in Z$ ，那么有 $g=ld=a(lx_0)+n(ly_0)$ 。 证明模逆元 现在证明模逆元\n充分性\n已知 $\\mathrm{gcd}(a,n)=1$ ，则有 $1=ax_0+ny_0$ $(ax_0+ny_0)\\ \\mathrm{mod}\\ n = ax_0 \\ \\mathrm{mod}\\ n $​ $(ax_0+ny_0)\\ \\mathrm{mod}\\ n= 1 \\ \\mathrm{mod}\\ n=1$ 故 $ax_0 \\ \\mathrm{mod}\\ n=1$ 即 $ax_0\\ \\equiv 1\\ (\\mathrm{mod}\\ n)$ ， $b=x_0$ 必要性：\n已经存在 $b$ 满足 $ab\\ \\equiv 1\\ (\\mathrm{mod}\\ n)$​ 则存在 $y,k$ 满足 $(ab+ny)\\ \\mathrm{mod}\\ n=1$ 则存在 $k$ 满足 $ab+ny-1=kn$ 即 $ab + n(y-k)=1$ 根据引理2，有 $\\mathrm{gcd}(a,n)\\leq 1$ ，显然只有 $\\mathrm{gcd}(a,n)=1$ 中国剩余定理 方程组\n$$ \\begin{equation} \\begin{cases} x\\ \\equiv a_1\\ (\\mathrm{mod}\\ m_1)\\newline x\\ \\equiv a_2\\ (\\mathrm{mod}\\ m_2)\\newline \u0026hellip;\\newline x\\ \\equiv a_n\\ (\\mathrm{mod}\\ m_n)\\newline \\end{cases} \\end{equation} $$\n其中对于任意 $i\\neq j$ 有 $\\mathrm{gcd}(m_i,m_j)=1$ ，对于任意 $a_1,a_2,\u0026hellip;,a_n$ 有解。\n证明：如下\n存在性\n令 $M=m_1m_2,\u0026hellip;m_n=\\prod\\limits_{i=1}^{n}m_i$， $M_i=M/m_i$ 显然有 $\\mathrm{gcd}(M_i,m_i)=1$ ，故存在 $t_i$ 满足 $M_it_i\\ \\equiv 1\\ (\\mathrm{mod}\\ m_i)$ 故对于 $x\\ \\equiv a_1\\ (\\mathrm{mod}\\ m_1)$ ，有 $a_iM_it_i\\ \\equiv a_i\\ (\\mathrm{mod}\\ m_i)$ 故可得 $x$ 的一个解 $x=\\sum\\limits_{i=1}^{n}a_iM_it_i$ 完备性\n若 $x_1,x_2$ 都是方程组的解，那么对于任意 $i\\in \\lbrace 1,2,\u0026hellip;,n \\rbrace$ ，有 $(x_1-x_2)\\ \\mathrm{mod}\\ m_i=0$ 故 $x_1-x_2=kM$ 所以通解为 ${kM+}\\sum\\limits_{i=1}^{n}a_iM_it_i$ 欧拉公式 简介 函数 $\\phi(n)$ 为 $\\lbrace 1,2,\u0026hellip;,n \\rbrace$ 中和 $n$ 互质的数的数量 例如 $\\phi(8)=4$ ，因为 $\\mathrm{gcd}(\\lbrace 1,3,5,7 \\rbrace,4)=1$ 性质 （性质1）\n对于质数 $n$ ，有 $\\phi(n)=n-1$\n（性质2）\n若存在质数 $p$ 满足 $n=p^k$ ，则 $\\phi(n)=p^k-p^k/p=n(1-1/p)$ 。思路为 $\\lbrace 1,2,\u0026hellip;,n \\rbrace$ 中除去 $p$ 的倍数\n（性质3）\n若 $\\mathrm{gcd}(m,n)=1$ ，则 $\\phi(mn)=\\phi(m)\\phi(n)$ ​。证明如下：\n对于任意 $0 \u0026lt; N \u0026lt; mn$ ，有 $N=k_1m+p=k_2n+q$ 。假设 $N$ 满足 $\\mathrm{gcd}(N,mn)=1$ 显然， $\\mathrm{gcd}(N,m)=1$ ，故 $\\mathrm{gcd}(k_1m+p,m)=1$ ，显然 $\\mathrm{gcd}(p,m)=1$ 。同理 $\\mathrm{gcd}(q,n)$ 。 对于方程组 $$ \\begin{equation} \\begin{cases} N\\ \\equiv p\\ (\\mathrm{mod}\\ m)\\newline N\\ \\equiv q\\ (\\mathrm{mod}\\ n) \\end{cases} \\end{equation} $$\n根据中国剩余定理，有解 $N=kmn+t_ppn+t_qqm$ 。\n每有一组 $(p,q)$ ，该方程组就有一个解。注意 $\\mathrm{gcd}(p,m)=1$ ,且 $\\mathrm{gcd}(q,n)$ 。\n（性质4）\n对于 $n=\\prod\\limits_{i=1}^rp_i^{k_i}$ ，有 $\\phi(n)=\\phi(\\prod\\limits_{i=1}^rp_i^{k_i})=\\prod\\limits_{i=1}^r\\phi(p_i^{k_i})=\\prod\\limits_{i=1}^r(n(1-1/p_i))=n^r\\prod\\limits_{i=1}^r(1-1/p_i)$\n欧拉定理 简介 若 $n,a$ 为正整数，且 $\\mathrm{gcd}(n,a)=1$ 则 $a^{\\phi(n)}\\ \\equiv 1\\ (\\mathrm{mod}\\ n)$​\n欧拉定理的证明 设 $\\Phi(n)=\\lbrace c_1,c_2,\u0026hellip;,c_{\\phi(n)} \\rbrace$ 为小于 $n$ 且与 $n$ 互质的数的集合，即 $\\mathrm{gcd}(c_i,n)=1$。\n若 $\\mathrm{gcd}(a,n)=1$ ，考虑集合 $\\Phi_a(n)=\\lbrace (ac_1)\\ \\mathrm{mod}\\ n,(ac_2)\\ \\mathrm{mod}\\ n,\u0026hellip;,(ac_{\\phi(n)})\\ \\mathrm{mod}\\ n \\rbrace$ 。我们证明 $\\Phi(n)=\\Phi_a(n)$\n先证明 $\\Phi_a(n)$ 中没有重复的元素，若 $ac_i\\ \\equiv ac_j\\ (\\mathrm{mod}\\ n)$ ,则 $c_i\\ \\equiv c_j\\ (\\mathrm{mod}\\ n)$ ，这显然错误。\n再证明 $\\mathrm{gcd}(ac_i\\ \\mathrm{mod}\\ n, n)=1$ 。设 $ac_i=k_in+r_i$ ，若 $\\mathrm{gcd}(r_i,n)=g$ ，则 $ac_i=g(k_i(n/g)+(r_i/g))$ 。等式右侧是 $g$ 的倍数，而左侧显然不是（ $a$ 和 $c$ 都与 $n$ 互质）\n所以：\n$$ \\prod\\limits_{i=1}^{\\phi(n)}c_i \\ \\equiv \\prod\\limits_{i=1}^{\\phi(n)}c_ia(\\ \\mathrm{mod}\\ n) $$\n即\n$$ \\prod\\limits_{i=1}^{\\phi(n)}c_i \\ \\equiv a^{\\phi(n)}\\prod\\limits_{i=1}^{\\phi(n)}c_i(\\ \\mathrm{mod}\\ n) $$\n显然 $\\mathrm{gcd}(\\prod\\limits_{i=1}^{\\phi(n)}c_i,n)=1$\n故\n$$ a^{\\phi(n)}\\ \\equiv 1\\ (\\mathrm{mod}\\ n) $$\nRSA算法 算法流程 生成秘钥 选择连个大质数 $p$ 和 $q$ ，计算 $n=p * q$ 计算 $\\phi(n)=(p-1)(q-1)$ 选择正整数 $e$ 满足 $1 \u0026lt; e \u0026lt; \\phi(n)$ ，且 $\\mathrm{gcd}(e,\\phi(n))=1$ 计算 $e$ 对 $\\phi(n)$ 的模逆元 $d$ ，即 $ed\\ \\equiv 1\\ (\\mathrm{mod}\\ \\phi(n))$ 得到公钥对 $(n,e)$ ，私钥对 $(n, d)$ 加密 加密的数为 $m$ ，满足 $0\\leq m \u0026lt; n$ 计算 $c=m^e\\ \\mathrm{mod}\\ n$ ，则 $c$ 就是密文 解密 $m=c^d\\ \\mathrm{mod}\\ n$ 算法证明 显然，核心点在于证明 $m=c^d\\ \\mathrm{mod}\\ n$​ 。\n简答化简可得， $c^d\\ \\mathrm{mod}\\ n=(m^e\\ \\mathrm{mod}\\ n)^d\\ \\mathrm{mod}\\ n=m^{ed}\\ \\mathrm{mod}\\ n$​\n根据 $ed\\ \\equiv 1\\ (\\mathrm{mod}\\ \\phi(n))$ ，可知存在 $k$ 使得 $ed=k\\phi(n)+1$ ，带入 $m^{ed}\\ \\mathrm{mod}\\ n$ 得，\n$$ \\begin{align} m^{ed}\\ \\mathrm{mod}\\ n\u0026amp;= m^{k\\phi(n)+1}\\ \\mathrm{mod}\\ n \\newline \u0026amp;= ((m^{\\phi(n)} \\mathrm{mod}\\ n)^k*(m\\ \\mathrm{mod}\\ n))\\ \\mathrm{mod}\\ n \\newline \u0026amp;= (m(m^{\\phi(n)} \\mathrm{mod}\\ n)^k) \\ \\mathrm{mod}\\ n \\end{align} $$\n通常情况 当 $\\mathrm{gcd}(m,n)=1$ 时（即 $m\\neq hp$ 且 $m\\neq hq$ 时），根据欧拉定理 $m^{\\phi(n)}\\ \\equiv 1\\ (\\mathrm{mod}\\ n)$ ，可知 $m^{\\phi(n)} \\mathrm{mod}\\ n=1$​\n则 $m^{ed}\\ \\mathrm{mod}\\ n = m\\ \\mathrm{mod}\\ n=m$\n特殊情况 当 $m=hp$ 时，（ $m=hq$ 同理）有 $m^{\\phi(n)} \\mathrm{mod}\\ n = (hp)^{(p-1)(q-1)}\\mathrm{mod}\\ pq$\n而因为 $q$ 是质数，根据费马小定理，有 $((hp)^{k(p-1)})^{q-1}\\ \\equiv 1\\ (\\mathrm{mod}\\ q)$ 。故\n$$ \\begin{align} ((hp)^{k(p-1)})^{q-1}hp\\ \u0026amp;\\equiv hp\\ (\\mathrm{mod}\\ q) \\newline (hp)^{k(p-1)(q-1)+1}\\ \u0026amp;\\equiv hp\\ (\\mathrm{mod}\\ q) \\newline (hp)^{(cd)}\\ \u0026amp;\\equiv hp\\ (\\mathrm{mod}\\ q) \\end{align} $$\n故存在 $t$ 满足\n$$ (hp)^{ed}=tq+hp $$\n注意，等式左侧是 $p$ 的倍数，而 $p$ 是质数，故 $t$ 必定是 $p$ 的倍数，设 $t=t\u0026rsquo;p$ ，则\n$$ \\begin{align} (hp)^{ed}\u0026amp;=t\u0026rsquo;pq+hp=t\u0026rsquo;n+hp \\newline m^{ed}\\ \u0026amp;\\equiv m\\ (\\mathrm{mod}\\ n) \\newline m^{ed-1} \\ \u0026amp;\\equiv 1\\ (\\mathrm{mod}\\ n) \\newline m^{k\\phi(n)} \\ \u0026amp;\\equiv 1\\ (\\mathrm{mod}\\ n) \\end{align} $$\n同理 $m^{ed}\\ \\mathrm{mod}\\ n = (m(m^{\\phi(n)} \\mathrm{mod}\\ n)^k) \\ \\mathrm{mod}\\ n = m\\ \\mathrm{mod}\\ n=m$\n","permalink":"https://mobilephone724.github.io/article/zero2rsa/","summary":"从0证明RSA RSA 算法（即一个非对称加密算法）除了应用非常广泛外，其特性也非常吸引人（起码非常吸引我）。我在网上找了很多关于RSA的证明，要么不够详细（例如缺失对前置定理的证明），要么需要引出较多复杂的数论概念。作者本身水平不高，试图绕过这些复杂的概念，从初等数学的开始，完备地证明RSA。\n关于RSA的背景知识可能很多，可以慢慢阅读，我在此尝试从初等数学开始证明。这些背景知识的证明有一定的顺序，如果读者发现某个证明看不懂，可以向前翻阅。\n参考的文章如下：（因为参考的文章太多，大概率不全）\n费马小定理 中国剩余定理 阮一峰的博客——RSA算法原理（一） 阮一峰的博客——RSA算法原理（二） 初等数论笔记Part 1： 欧拉定理 算法学习笔记(9)：逆元 费马小定理 简介 如果 $p$ 是质数且 $\\mathrm{gcd}(a,p)=1$ , 那么 $a^{p-1}\\equiv 1\\ (\\mathrm{mod}\\ p)$\n在证明该定理前，先证明一个简单的引理\n引理1 如果 $p$ 是质数，且 $\\mathrm{gcd}(a,p)=1$ , 那么\n$$ \\lbrace ka \\ \\mathrm{mod}\\ p | k = \\lbrace 1,2,\u0026hellip;,p -1 \\rbrace \\rbrace= \\lbrace 1,2,3,\u0026hellip;,p-1 \\rbrace $$\n即二者存在一对一的关系。由于这两个集合的元素个数相同，所以只要证明左侧集合没有重复元素即可\n证明：假设存在 $k_1$ 和 $k_2$ 满足 $1 \\leq k_1 \u0026lt; k_2 \\leq p-1$ ，且 $k_1a\\ \\mathrm{mod}\\ p = k_2a\\ \\mathrm{mod}\\ p$ .","title":"ZERO TO RSA"}]