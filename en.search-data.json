{"/database/":{"data":{"":"","这里记录什么#这里记录什么":" log base pgvector predicate lock google f1 transaction "},"title":"Database"},"/database/executor/hashjoin/":{"data":{"":"","code-level-detail#Code level Detail":"utility ExecHashGetBucketAndBatch : hash value to bucket number and batch number ExecHashGetBucketAndBatch(HashJoinTable hashtable, uint32 hashvalue, int *bucketno, int *batchno) { uint32 nbuckets = (uint32) hashtable-\u003enbuckets; uint32 nbatch = (uint32) hashtable-\u003enbatch; if (nbatch \u003e 1) { *bucketno = hashvalue \u0026 (nbuckets - 1); ### tricky way as MOD *batchno = pg_rotate_right32(hashvalue, hashtable-\u003elog2_nbuckets) \u0026 (nbatch - 1); ### rotate hashvalue and MOD nbatch } else { *bucketno = hashvalue \u0026 (nbuckets - 1); *batchno = 0; } } ExecHashTableInsert : insert hash value ExecHashTableInsert ExecHashGetBucketAndBatch(hashtable, hashvalue, \u0026bucketno, \u0026batchno); if (batchno == hashtable-\u003ecurbatch) ### put into hash table hashTuple = (HashJoinTuple) dense_alloc hashtable-\u003espaceUsed += hashTupleSize; ### For single batch, we may increase the nbucket if (hashtable-\u003enbatch == 1) if (ntuples \u003e (hashtable-\u003enbuckets_optimal * NTUP_PER_BUCKET) \u0026\u0026 xxx) hashtable-\u003enbuckets_optimal *= 2; hashtable-\u003elog2_nbuckets_optimal += 1; ### For multi-batches, we may increase the batches if (hashtable-\u003espaceUsed + hashtable-\u003enbuckets_optimal * sizeof(HashJoinTuple) + \u003e hashtable-\u003espaceAllowed) ExecHashIncreaseNumBatches() else ### put the tuple into a temp file for later batches ExecHashJoinSaveTuple() ExecHashIncreaseNumBatches : increase batches ExecHashIncreaseNumBatches nbatch = oldnbatch * 2; ### double nbatches ### init/update batchfiles if (hashtable-\u003einnerBatchFile == NULL) hashtable-\u003einnerBatchFile = palloc0_array(BufFile *, nbatch); hashtable-\u003eouterBatchFile = palloc0_array(BufFile *, nbatch); PrepareTempTablespaces(); else hashtable-\u003einnerBatchFile = repalloc0_array() hashtable-\u003eouterBatchFile ### resize nbuckets? if (hashtable-\u003enbuckets_optimal != hashtable-\u003enbuckets) hashtable-\u003enbuckets = hashtable-\u003enbuckets_optimal; hashtable-\u003elog2_nbuckets = hashtable-\u003elog2_nbuckets_optimal; hashtable-\u003ebuckets.unshared = repalloc_array() ### scan through allchunks while (oldchunks != NULL) nextchunk = oldchunks-\u003enext.unshared ### scan through all tuples in the chunk idx = 0 while (idx \u003c oldchunks-\u003eused) HashJoinTuple hashTuple = (HashJoinTuple) (HASH_CHUNK_DATA(oldchunks) + idx); ... ### where should the tuple go? ExecHashGetBucketAndBatch(hashtable, hashTuple-\u003ehashvalue, \u0026bucketno, \u0026batchno); if (batchno == curbatch) ### keep the tuple but copy it into the new chunk copyTuple = (HashJoinTuple) dense_alloc(hashtable, hashTupleSize); hashtable-\u003ebuckets.unshared[bucketno] = copyTuple; else ### dump it out ExecHashJoinSaveTuple() idx += MAXALIGN(hashTupleSize); pfree(oldchunks); oldchunks = nextchunk; ExecHashJoinSaveTuple : save a tuple to a batch file. BufFileWrite(file, \u0026hashvalue, sizeof(uint32)); BufFileWrite(file, tuple, tuple-\u003et_len); ### len is record in MinimalTupleData structure single worker build state MultiExecProcNode MultiExecPrivateHash for (;;) slot = ExecProcNode(outerNode); if (ExecHashGetHashValue()) bucketNumber = ExecHashGetSkewBucket if (bucketNumber != INVALID_SKEW_BUCKET_NO) ### skew tuple ExecHashSkewTableInsert else ExecHashTableInsert\t### normal tuple hashtable-\u003etotalTuples += 1; xxx","high-level-view#high level view":"See Queries in PostgreSQL: 6. Hashing\nOne-pass hash join Note that join in PostgreSql, we scan the right relation first, which means that the right relation is the “inner relation” and the left relation is the outer one. Two-pass hash join Since we can’t allocate as much memory as we want, instead of building a hash table of the entire table, PG split the tables to several batches where all tuples have the same hash value flag.\nBatches are splited by hash value. Use several bits in hash value as a flag so we can put the tuples into different batches.\nThere is a simple optimization that we can build the hash table in the first batch while scanning the inner table, and match the pair while scanning the outer table.\nparallel one-pass hash join With parallel workers, we can\nscan inner table and build shared hash table parallelly scan outer table parallelly Although in most cases, the neck of tp system is disk io, but parallel workers can still advance the speed efficiently. Because:\nIn single process situation, the disk IO is synchronous，which means CPU is in idle while waiting IO. So, in the parallel case, CPU can be utilized more sufficiently. OS may has the technique to load the disk’s content in advance, which is perdicularly useful in sequence scan. So multi-workers can read data file content more efficiently. In hash join, the compute of hash value may cost more CPU resource than normal TP operation. parallel two-pass hash join Same as the basic two-pass hash join, parallel workers build batches parallelly, both in reading from inner/outer tuple and writing data to tmp file. Since no worker can obtain a whole batch’s data in the first scan, the technique described above can be used here.","low-level-complement#Low level complement":"Single process inner join What is inner join\nThis is the simplest join method in hash join. So we introduce a simple hash join state machine here. (See ExecHashJoinImpl for detail )\nSTART WITH: state ==\u003e HJ_BUILD_HASHTABLE case HJ_BUILD_HASHTABLE: state ==\u003e HJ_NEED_NEW_OUTER case HJ_NEED_NEW_OUTER: ### generate a new outer tuple state ==\u003e HJ_NEED_NEW_BATCH ### No more tuple in this batch. ==\u003e HJ_SCAN_BUCKET; ### Find a outer tuple. Can this one matches a inner one? case HJ_SCAN_BUCKET: ### Scan the selected hash bucket for matches to current outer state ==\u003e HJ_NEED_NEW_OUTER ### Whether we can find a match or not, we always generate a new outer tuple. case HJ_NEED_NEW_BATCH: ### Try to advance to next batch state ==\u003e HJ_NEED_NEW_OUTER; ==\u003e FINISH right join To complete right join, we can just emit each outer tuple even if there’s no matched innner tuple.\ncase HJ_SCAN_BUCKET: state ==\u003e HJ_FILL_OUTER_TUPLE ### Can not find a match. Is it a left join? ==\u003e HJ_NEED_NEW_OUTER case HJ_FILL_OUTER_TUPLE: state ==\u003e HJ_NEED_NEW_OUTER; ### Whether emit the outer tuple with null-filled left tuple or not, we always generate a new outer tuple. left join To complete this, we must remember whether a inner tuple has been matched. So\ncase HJ_NEED_NEW_OUTER: state ==\u003e HJ_FILL_INNER_TUPLES ### This batch has been finished, see if there are unmatched inner tuples. ==\u003e HJ_NEED_NEW_BATCH ==\u003e HJ_SCAN_BUCKET case HJ_FILL_INNER_TUPLES: state ==\u003e HJ_NEED_NEW_BATCH ### No more unmatched inner tuples, so start the next batch ==\u003e HJ_FILL_INNER_TUPLES ### return an unmatched inner tuple. summary Until now, we can generate a full state machine in non-parallel mode\nSTART WITH: state ==\u003e HJ_BUILD_HASHTABLE case HJ_BUILD_HASHTABLE: state ==\u003e HJ_NEED_NEW_OUTER case HJ_NEED_NEW_OUTER: ### generate a new outer tuple state ==\u003e HJ_FILL_INNER_TUPLES ### This batch has been finished, see if there are unmatched inner tuples. ==\u003e HJ_NEED_NEW_BATCH ### No more tuple in this batch. ==\u003e HJ_SCAN_BUCKET; ### Find a outer tuple. Can this one matches a inner one? case HJ_SCAN_BUCKET: ### Scan the selected hash bucket for matches to current outer state ==\u003e HJ_FILL_OUTER_TUPLE ### Can not find a match. Is it a left join? ==\u003e HJ_NEED_NEW_OUTER ### Whether we can find a match or not, we always generate a new outer tuple. case HJ_NEED_NEW_BATCH: ### Try to advance to next batch state ==\u003e HJ_NEED_NEW_OUTER; ==\u003e FINISH parallel hash Note that BarrierArriveAndWait will increase current phase. So each phase’s status is not be assigned directly but self-increased.\nLet introduce the state machine first\nSTART WITH: case HJ_BUILD_HASHTABLE: ### If multi-batch, we need to hash the outer relation up front. ExecParallelHashJoinPartitionOuter(node); state ==\u003e HJ_NEED_NEW_BATCH ### Select a batch to work on. case HJ_NEED_NEW_OUTER: ExecParallelHashJoinOuterGetTuple sts_parallel_scan_next case HJ_NEED_NEW_BATCH: ExecParallelHashJoinNewBatch() switch PHJ_BATCH_STATE case PHJ_BATCH_ELECT: ### One backend allocates the hash table ExecParallelHashTableAlloc ### Fall through case PHJ_BATCH_ALLOCATE: ### Wait for allocation to complete and Fall through case PHJ_BATCH_LOAD: ### Start (or join in) loading tuples and Fall through. case PHJ_BATCH_PROBE: ### This batch is ready to probe ExecParallelHashTableSetCurrentBatch return true; case PHJ_BATCH_SCAN: ### detach and go around again case PHJ_BATCH_FREE: state ==\u003e HJ_NEED_NEW_OUTER PHJ_BUILD_ELECT ==\u003e PHJ_BUILD_ALLOCATE ExecParallelHashJoinNewBatch "},"title":"hash join"},"/database/log-base/":{"data":{"":"","primitive-operation-if-transactions#Primitive Operation if Transactions":"There are three address spaces that transaction interact in important ways:\nThe space of disk blocks holding the database elements. The memory address space managed by buffer manager. The local address space of the transaction. To describe a transaction, we need some operation notions:(X below is a database element while t is a local varible, and we suppose a database element is no larger than a single block)\nINPUT(X): copy disk block containing X to memroy buffer READ(X, t): Copy X to transaction’s local varible t no matter where X is, which means INPUT(X) may be executed first before READ(X,t). WRITE(X, t): Copy the value of t to X no matter where X is. OUTPUT(X): Copy the block containing X from its buffer to disk ","undo-logging#undo logging":"Undo log makes repairs to the database state by undoing the effects of transactions that may not completed before the crash.\nAn Undo log has the form [T,X,v] which means transaction T has changed the database elememnt X, and its before value was v. The log record is a response to a WRITE action into memory, not an OUTPUT action.\nAn undo log is suffcient to allow recovery from system failure, provided transactions and buffer manager obey two rules:\nIf transaction T modifies database element X, then the log record of form \u003cT,X,v\u003e must be written to disk before the new value of X is written to disk If a transaction commits, the its commit log record must be written to disk only after all database elements changed by the transaction have been written to disk, but as soon there after is possible If a transaction aborts, recovery manager is need to repair the values. When recovering, the recovery manager scan the log from the end. As it travels, it remembers all thos transactions T for which it has seen a [COMMIT T] record or [ABORT T] record, the:\nIf T’s COMMIT record is found, do nothing. Otherwise, T is an incomplete transaction, or aborted transaction. The recovery manager must change the value of X in the database to v,in case X had been altered just before the crash. After this, the recovery manager must write a log record [ABORT T] for each incomplete transaction T that was not previously aborted, and then flush the log "},"title":"Basic Knowledge of Database Log"},"/database/pg16/summary/":{"data":{"":"","content#Content":"From Release Note feature: parallel hash join commit id: 11c2d6fdf5af1aacec lines: 7 files changed, 323 insertions(+), 48 deletions(-) discussion: PostgreSQL: Parallel Full Hash Join difficulty level: extremely hard feature: Improve performance of vacuum freezing * From pganalyz https://pganalyze.com/blog/5mins-postgres-16-faster-copy-bulk-load\nfeature: fast copy commit id: many commits 26158b852d3adf69360 “Use ExtendBufferedRelTo() in XLogReadBufferExtended()” 00d1e02be2498718011 “hio: Use ExtendBufferedRelBy() to extend tables more efficiently” 5279e9db8e8da3c310c “heapam: Pass number of required pages to RelationGetBufferForTuple()” acab1b0914e426d2878 “Convert many uses of ReadBufferExtended to ExtendBufferedRel()” fcdda1e4b50249c344e5 “Use ExtendBufferedRelTo() in {vm,fsm}_extend()” 31966b151e6ab7a6284 “bufmgr: Introduce infrastructure for faster relation extension” 12f3867f5534754c8bac “bufmgr: Support multiple in-progress IOs by using resowner” dad50f677c42de20716 “bufmgr: Acquire and clean victim buffer separately” 794f25944790ed0462c “bufmgr: Add Pin/UnpinLocalBuffer()” 4d330a61bb1969df31f “Add smgrzeroextend(), FileZero(), FileFallocate()” discussion: PostgreSQL: refactoring relation extension and BufferAlloc(), faster COPY difficulty level: very hard feature: fast relation extension commit id: see above ","whats-this#What\u0026rsquo;s this?":"What’s this? Summary some new features and its commitid. These complement will be shared later (if possible)"},"title":"pg16 summary"},"/database/pg_xact/":{"data":{"":"","pg的事务实现#PG的事务实现":"新开一个专题，准备把src/access/transam目录下的代码全部浏览一遍，看看今年能不能搞完一半。"},"title":"postgresql transaction"},"/database/pg_xact/slru/":{"data":{"":"","概述#概述":"该章节记录以slru为buffer实现的模块\nslru 描述如何通过lru的方式实现一个简易且适合特定场景的buffer pool manager "},"title":"slru related"},"/database/pg_xact/slru/clog/":{"data":{"":"","extend-and-truncate#Extend And Truncate":"During the process of generating a new xid, we make sure that the slru page exists.\nIf it’s the first xid of the page, we allocate a new page in clog buffer. Also generate a WAL to record the birth of the page. If not, the page must exist in memory or flushed into disk. So it’s for slru layer to manage such situation. Keep in mind that the general self-increment xid does’t begin at zero:\n#define FirstNormalTransactionId\t((TransactionId) 3) so:\nDuring bootstrap, initialize the first clog page During extend new pages, be careful about the FirstNormalTransactionId, since it is not the first xid in page representation but the first general one. The above behaviors indicate that although a clog segment at most occupies 256K space, it doesn’t have such size just after initialization. We extend 8K pages one by one during the xid increment.\nSince at most half of uint32 xids can be in use, it’s natural to clean up out of date clog files. Different from extending a page, we always delete a whole page. So once we promote the frozenxid, we try to find some clog files to delete:\nThe judgement whether there is a file can be deleted is completed in slru layer(a loop to scan the directory), but clog layer supports a hook to judge one file. Advance the oldest clog xid in shared memory Generate a clog truncate WAL record Real truncate. Complemented in slru layer. Details of the two kind WAL record will be shown later.","in-disk-representation#In-Disk Representation":"Thinking that the commit status of each transaction composites an array clog[] and clog[xid] records the status, we can easily store the array to disk by the slru.\nThe status of one transaction needs two bits to represent:\n#define CLOG_BITS_PER_XACT\t2 #define CLOG_XACTS_PER_BYTE 4 #define CLOG_XACTS_PER_PAGE (BLCKSZ * CLOG_XACTS_PER_BYTE) #define CLOG_XACT_BITMASK\t((1 \u003c\u003c CLOG_BITS_PER_XACT) - 1) So we can get the xid’s index and offset in page and byte.\n#define TransactionIdToPage(xid)\t((xid) / (TransactionId) CLOG_XACTS_PER_PAGE) #define TransactionIdToPgIndex(xid) ((xid) % (TransactionId) CLOG_XACTS_PER_PAGE) #define TransactionIdToByte(xid)\t(TransactionIdToPgIndex(xid) / CLOG_XACTS_PER_BYTE) #define TransactionIdToBIndex(xid)\t((xid) % (TransactionId) CLOG_XACTS_PER_BYTE) Thinking of that one slru segment contains 32 pages, so we name the clog file as 0000(contains xid in [0, 32 * CLOG_XACTS_PER_PAGE - 1]), 0001(contains xid in [32 * CLOG_XACTS_PER_PAGE, 32 * CLOG_XACTS_PER_PAGE * 2 - 1]) and so on. Because four hex numbers can represent $16^4=2^{12}$ files with $2^{12} \\times 32 \\times 8192 \\times 4 = 2^{32}$ transactions’ status(a int32 size)\nAttension, such simple mapping means that the pages in clog file don’t have page headers. So we can’t record LSN, checksum in each page. The lack of LSN means the changes of clog page wouldn’t be recorded in WAL but clog doesn’t need it indeed.","overview#Overview":"This chapter explains the content of clog\nclog(commit log), records the commit status of each transaction. The log exists both in memory mannaged by slru buffer and disk for durability. The commit status can be the four kinds below:\n#define TRANSACTION_STATUS_IN_PROGRESS\t0x00 #define TRANSACTION_STATUS_COMMITTED\t0x01 #define TRANSACTION_STATUS_ABORTED\t0x02 #define TRANSACTION_STATUS_SUB_COMMITTED\t0x03 ","record-changes-in-wal#Record changes in WAL":"Recall what mentioned above:\nExtending a new page and delete a segment will generata a WAL record. Setting commit status wouldn’t For the latter one, it’s unbelievable but tricky. Since only the transactions that changes the content data(some hint flags are exception, such as tuple infomask) will have a xid(and then record on clog segment). During the replay of such transactions’ commit(or abort) WAL record, we can redo the clog by the way.\nFor the former one, it’s a matter of course, since we must guarantee the clog to be recovery-safe. But some details deserve a glance;\nFor extending a new page, it makes no difference that we flush the WAL record now or later. Since once we want to set status in a non-existent page during recovery, we can padding a new empty page. This trick doesn’t affect the page usage. For deleting a clog segment, we have no chance to remedy the lost of clogs, and the disaster means a lot of tuple can be accessed at all. So regardless of the synchronous commit level, we must ensure the WAL record has flushed into disk before really delete the segments. ","set-and-get#Set And Get":"Concerned with subtransactions …\nI can’t totally figure out the commit tree without knowing the mechanism of subtransaction. Just assuming subxids as a set of xids related to the main xid seems not convictive enough for me. So I remain it here now and will finish it after reading subtransactions)\nFor now, it’s enough to knowing that\nThe pair of operations wouldn’t generate any WAL record They are done during the commit or abort procedure. "},"title":"CLOG"},"/database/pg_xact/slru/slru/":{"data":{"":"","#":"本文主要为SLRU本身的结构解读。\n简述 slru用来干什么？ slru是一个简单的buffer管理模块，simple slru 有了buffer pool manager，为什么还要slru？ bpm管理通用的page，比如heap，vm等 slru最大的特点就是lru，非常适合处理xid这样，递增的信息。 下面的代码分析基于pg15 存储结构 与bpm不同，通过slru管理的page，其文件大小固定，一个文件有32个page，一个page有8KB，故一个文件最大为256K。\n与WAL不同，WAL文件的大小在创建时就已经确定为16M，与WAL文件重用保持一致，而slru的文件，先在内存中产生相应的page，再会去落盘。\n#define SLRU_PAGES_PER_SEGMENT\t32 内存slru 全局 buffer 数组 typedef struct SlruSharedData { LWLock\t*ControlLock; /* Number of buffers managed by this SLRU structure */ int\tnum_slots; /* * Arrays holding info for each buffer slot. Page number is undefined * when status is EMPTY, as is page_lru_count. */ char\t**page_buffer; SlruPageStatus *page_status; bool\t*page_dirty; int\t*page_number; int\t*page_lru_count; LWLockPadded *buffer_locks; XLogRecPtr *group_lsn; int\tlsn_groups_per_page; /*---------- * We mark a page \"most recently used\" by setting *\tpage_lru_count[slotno] = ++cur_lru_count; * The oldest page is therefore the one with the highest value of *\tcur_lru_count - page_lru_count[slotno] * The counts will eventually wrap around, but this calculation still * works as long as no page's age exceeds INT_MAX counts. *---------- */ int\tcur_lru_count; } SlruSharedData; 从内存结构上看，是一个数组，每个元素代表一个page。同时，记录这些page的使用次数。\npage_lru_count[slotno] = ++cur_lru_count; 同时每个page，都有状态标识，以在刷脏时，确定脏页。实际上这里没有脏页这个选项，因为只有 valid 状态的页才有可能是脏页，有包含关系。故在SlruSharedData 中使用 page_dirty 进行单独标识。\ntypedef enum { SLRU_PAGE_EMPTY,\t/* buffer is not in use */ SLRU_PAGE_READ_IN_PROGRESS, /* page is being read in */ SLRU_PAGE_VALID,\t/* page is valid and not being written */ SLRU_PAGE_WRITE_IN_PROGRESS /* page is being written out */ } SlruPageStatus; 关于为什么需要记录LSN信息 group_lsn：这与 WAL 设计有关。对于 WAL 而言，无论是同步提交或是异步提交，都需要在对应的 buffer page 落盘前落盘,所以 slru 也需要满足这样的规则。同时，可能是为了节约内存（节约的内存实在有限），或是减少WAL flush的调用次数以增加 IO 效率，slru的实现中并不记录每个buffer page的 LSN，而是记录一组 page 的 LSN，在刷下一个 page 前，需要把一组 page 中最大的 LSN 前的 WAL 落盘。而这样的“一组”的长度，就为lsn_groups_per_page\n各个进程私有的pointer /* * SlruCtlData is an unshared structure that points to the active information * in shared memory. */ typedef struct SlruCtlData { SlruShared\tshared; /* * Decide whether a page is \"older\" for truncation and as a hint for * evicting pages in LRU order. */ bool\t(*PagePrecedes) (int, int); /* * Dir is set during SimpleLruInit and does not change thereafter. Since * it's always the same, it doesn't need to be in shared memory. */ char\tDir[64]; } SlruCtlData; 初始化时，即返回一个SlruCtlData。Dir 是初始化时的标记，不同模块会填充对应的名称。\n核心功能 SimpleLruZeroPage：新增一个page SimpleLruReadPage ：读一个page SimpleLruWritePage ：写一个page 基础函数 选择一个空slot /* Select the slot to re-use when we need a free slot. */ /* Control lock must be held at entry, and will be held at exit. */ static int SlruSelectLRUPage(SlruCtl ctl, int pageno) { for (;;) # return if we have such a slot # return if we have an empty slot \"SLRU_PAGE_EMPTY\" # select a lru slot # return it if it's clean. Or # victim it if dirty # loop end -- It's a very clever design to dealing with corner cases # such as the victim page being re-dirtied while we wrote it. } 记录一个\"most recently used\"的page，cur_lru_count++ 并用其赋值 #define SlruRecentlyUsed(shared, slotno)\t\\ do { \\ int\tnew_lru_count = (shared)-\u003ecur_lru_count; \\ if (new_lru_count != (shared)-\u003epage_lru_count[slotno]) { \\ (shared)-\u003ecur_lru_count = ++new_lru_count; \\ (shared)-\u003epage_lru_count[slotno] = new_lru_count; \\ } \\ } while (0) 从磁盘中读取一个 page SlruPhysicalReadPage { int\tsegno = pageno / SLRU_PAGES_PER_SEGMENT; SlruFileName(ctl, path, segno); /* * In a crash-and-restart situation, it's possible for us to receive * commands to set the commit status of transactions whose bits are in * already-truncated segments of the commit log */ fd = OpenTransientFile(path, O_RDONLY | PG_BINARY); if (fd \u003c 0 \u0026\u0026 !InRecovery) ereport() pg_pread(fd, shared-\u003epage_buffer[slotno], BLCKSZ, offset) } 向磁盘中写入一个 page SlruPhysicalWritePage { /* We must flush WAL before flush slru pages */ if (shared-\u003egroup_lsn != NULL) { max_lsn = shared-\u003egroup_lsn[lsnindex++]; XLogFlush(max_lsn); } SlruFileName(ctl, path, segno); fd = OpenTransientFile(path, O_RDWR | O_CREAT | PG_BINARY); pg_pwrite(fd, shared-\u003epage_buffer[slotno], BLCKSZ, offset) /* Queue up a sync request for the checkpointer. */ ... } interface 新增一个 page 到buffer。 /* Initialize (or reinitialize) a page to zeroes. */ int SimpleLruZeroPage(SlruCtl ctl, int pageno) { slotno = SlruSelectLRUPage(ctl, pageno); SlruRecentlyUsed(shared, slotno); # SlruSelectLRUPage may return a in-use page, we must clear it MemSet(shared-\u003epage_buffer[slotno], 0, BLCKSZ); SimpleLruZeroLSNs(ctl, slotno); } 从 disk 中读取一个 page /* Control lock must be held at entry, and will be held at exit. */ SimpleLruReadPage { #infinite loop slotno = SlruSelectLRUPage(ctl, pageno); # for in IO slots, just wait /* update in-memory status */ shared-\u003epage_number[slotno] = pageno; shared-\u003epage_status[slotno] = SLRU_PAGE_READ_IN_PROGRESS; shared-\u003epage_dirty[slotno] = false; /* Acquire per-buffer lock and release control lock */ LWLockAcquire(\u0026shared-\u003ebuffer_locks[slotno].lock, LW_EXCLUSIVE); LWLockRelease(shared-\u003eControlLock); ok = SlruPhysicalReadPage(ctl, pageno, slotno); /* re-acquire control lock */ LWLockAcquire(shared-\u003eControlLock, LW_EXCLUSIVE); # others } 这里的锁设计很特别：\n在 SlruSelectLRUPage 需要获取全局锁 在 SimpleLruReadPage 中，先初始化内存，再获取 per-buffer 锁，同时释放 ControlLock 在看函数 SimpleLruZeroPage\n/* Control lock must be held at entry, and will be held at exit. */ SimpleLruZeroPage { slotno = SlruSelectLRUPage(ctl, pageno); shared-\u003epage_number[slotno] = pageno; shared-\u003epage_status[slotno] = SLRU_PAGE_VALID; shared-\u003epage_dirty[slotno] = true; } 难道，一旦获取 ControlLock，即可对任意 slot 进行修改？\n实际上，SimpleLruReadPage 读取的 page，必须已存在于磁盘（或者经由 WAL 来保证）。 而 SimpleLruZeroPage 所初始化的 page 必须不存在。从使用逻辑上保证二者不产生冲突。\nSimpleLruWritePage(SlruInternalWritePage) /* Control lock must be held at entry, and will be held at exit. */ SlruInternalWritePage { /* If a write is in progress, wait for it to finish */ /* Do nothing if page is not dirty */ /* update in-memory status */ shared-\u003epage_status[slotno] = SLRU_PAGE_WRITE_IN_PROGRESS; shared-\u003epage_dirty[slotno] = false; /* Acquire per-buffer lock and release control lock */ LWLockAcquire(\u0026shared-\u003ebuffer_locks[slotno].lock, LW_EXCLUSIVE); LWLockRelease(shared-\u003eControlLock); SlruPhysicalWritePage(ctl, pageno, slotno, fdata); /* re-acquire control lock */ LWLockAcquire(shared-\u003eControlLock, LW_EXCLUSIVE); shared-\u003epage_status[slotno] = SLRU_PAGE_VALID; } "},"title":"SLRU"},"/database/pg_xact/wal/":{"data":{"":"","todo-汇总#TODO 汇总":" source manager 用来干什么？ ","整理一些wal相关的内容#整理一些WAL相关的内容":"希望通过理解WAL，理解进一步理解 事务、缓存、存储、复制等模块。","等内容写完了再整理该页面#等内容写完了再整理该页面":""},"title":"WAL(write ahead log)"},"/database/pg_xact/wal/basic/":{"data":{"":" From access/transam/README","write-ahead-log-coding#Write-Ahead Log Coding":"基本思想，日志在数据页前落盘\nLSN：刷脏前检查LSN对应的日志已经落盘 优势：仅在必要的时候等待XLOG的IO。（异步IO） LSN的检查模块只用在 buffer manager 中实现 在WAL回放时，避免相同的日志被重复回放（可重入）。（TODO：full page write是否在另一个层面上保证了可重入） WAL 包含一个（或一小组）页的增量更新的重做信息。 依赖文件系统和硬件的原子写，不可靠！ checkpoint，checkpointer后的第一次写全页。通过 checkpoint 留下的 LSN 来判断是否为第一次写 写下WAL日志的逻辑为 pin and exclusive-lock the shared buffer START_CRIT_SECTION，发生错误时确保整个数据库能立即重启 在shared buffer上，进行对应的修改 标记为脏页， 必须在WAL日志写入前完成（TODO，为什么？SyncOneBuffer） 只有在要写WAL时，才能标记脏页（TODO，为什么？） 使用XLogBeginInsert 和 XLogRegister* 函数构建WAL，使用返回的LSN来更新page END_CRIT_SECTION，退出 解锁和unpin （注意顺序） 一些复杂的操作，需要原子地写下一串WAL记录，但中间状态必须自洽(self-consistent)。这样在回放wal日志时，如果中断，系统还能够正常运行。注意：此时相当于事务回滚，但是其部分更改已经落盘。举例：\n在btree索引中，页的分裂分为两步（1）分配一个新页（2）在上一层的页(parent page)中新插入一条数据。 但是因为锁，这会形成两个独立的WAL日志。在回放WAL日志时 回放第（1）个日志： 分配一个新页，将元组移动进去 设置标记位，表示上一层的页没有更新 回放第（2）个日志： 在上一层的页中新插入一条数据 清除第（1）个日志中的标记位 标志位通常情况下不可见，因为对 child page 的修改时持有的锁，在两个操作完成后才会释放。 仅在写下第（2）个日志前，数据库恰好崩溃，标志位才会被感知。（该标志位应该没有MVCC，否则会在事务层屏蔽） 搜索时，不管这个中间状态 插入时，如果发现这个中间状态，先在上一层的页插入对应key，以修复这个“崩溃”状态，再继续插入 "},"title":"WAL基础"},"/database/pg_xact/wal/insert/":{"data":{"":"","具体的插入方式#具体的插入方式":"上述代码中的XLogRecordAssemble和XLogInsertRecord已经概括了具体的插入步骤\nXLogRecordAssemble Assemble a WAL record from the registered data and buffers into an XLogRecData chain\nstatic XLogRecData * XLogRecordAssemble(RmgrId rmid, uint8 info, XLogRecPtr RedoRecPtr, bool doPageWrites, XLogRecPtr *fpw_lsn, int *num_fpi) { for (block_id = 0; block_id \u003c max_registered_block_id; block_id++) { if (needs_data) { rdt_datas_last-\u003enext = regbuf-\u003erdata_head; } } } ","接口函数#接口函数":"一个WAL记录包含\nWAL记录类型。（TODO不同的修改有不同的记录方式？） 这个页的修改方式 被修改的页的信息。被修改的页通过一个唯一ID标识，也可以有更多的关联数据（“record-specific data associated with the block”）。如果要写full page，就没有关联数据 构建一个WAL记录包含5个核心函数 void XLogBeginInsert(void) 初始化相关状态 如果当前无法构建WAL日志（例如在recovery模式），则报错 void XLogRegisterBuffer(uint8 block_id, Buffer buf, uint8 flags); 增加了数据块的信息；注册一个buffer的引用，相当于上述WAL日志的第三部分 block_id is an arbitrary number used to identify this page reference in the redo routine\n在redo阶段，可以根据这些信息找到需要redo的page regbuf = \u0026registered_buffers[block_id]; /* * Returns the relfilenode, fork number and block number associated with * a buffer */ BufferGetTag(buffer, \u0026regbuf-\u003ernode, \u0026regbuf-\u003eforkno, \u0026regbuf-\u003eblock); regbuf-\u003epage = BufferGetPage(buffer); regbuf-\u003eflags = flags; regbuf-\u003erdata_tail = (XLogRecData *) \u0026regbuf-\u003erdata_head; regbuf-\u003erdata_len = 0; registered_buffer的结构\ntypedef struct { /* xxx */ /* info to re-find the page */ ForkNumber\tforkno; BlockNumber block; Page\tpage; /* a loop-linked structure to store the data change of each buffer */ uint32 rdata_len; /* total length of data in rdata chain */ XLogRecData *rdata_head; /* head of the chain of data registered with * this block */ XLogRecData *rdata_tail;\t/* last entry in the chain, or \u0026rdata_head if * empty */ /* xxx */ } registered_buffer; typedef struct XLogRecData { struct XLogRecData *next; /* next struct in chain, or NULL */ char *data; /* start of rmgr data to include */ uint32 len; /* length of rmgr data to include */ } XLogRecData; void XLogRegisterData(char *data, int len); 向WAL日志中写入任意数据 可多次调用，保证连续。这样在rodo时，就可以得到连续的数据 rdata = \u0026rdatas[num_rdatas++]; rdata-\u003edata = data; rdata-\u003elen = len; void XLogRegisterBufData(uint8 block_id, char *data, int len); rdata = \u0026rdatas[num_rdatas++]; rdata-\u003edata = data; rdata-\u003elen = len; regbuf = \u0026registered_buffers[block_id]; regbuf-\u003erdata_tail-\u003enext = rdata; regbuf-\u003erdata_tail = rdata; regbuf-\u003erdata_len += len; 可见，XLogRegisterBufData 和 XLogRegisterData 的核心区别在，前者写入的数据会关联到具体的buffer，而后者没有\nXLogInsert Insert the record. do { GetFullPageWriteInfo(\u0026RedoRecPtr, \u0026doPageWrites); rdt = XLogRecordAssemble(rmid, info, RedoRecPtr, doPageWrites, \u0026fpw_lsn, \u0026num_fpi); EndPos = XLogInsertRecord(rdt, fpw_lsn, curinsert_flags, num_fpi); } while (EndPos == InvalidXLogRecPtr); ","数据结构汇总#数据结构汇总":"registered_buffers 每一个buffer对应registered_buffers中的一个元素（一个registered buffer）\nvoid XLogEnsureRecordSpace(int max_block_id, int ndatas) { if (nbuffers \u003e max_registered_buffers) { registered_buffers = (registered_buffer *) repalloc(registered_buffers, sizeof(registered_buffer) * nbuffers); max_registered_buffers = nbuffers; } } "},"title":"WAL日志的插入"},"/database/pgvec/":{"data":{"":"","hnsw#HNSW":"概览 HNSW 算法主要包括以下几个步骤\n索引构建 构建层级邻近图 每一层都是邻近图 —— 每个点都记录它最近的几个点 高一层的图是低一层图的缩略图 —— 只有低一层图的部分点 ——，最低一层的图有全部点的信息。 查询阶段，对于目标点$p$ 对于每一层图： 维护一个图中距点$p$最近的点集合$S$，依次从候选点集合$C$中选取一个元素$c$：如果$c$的邻居$neighbor(c)$比$S$中距$p$最远的点$s$距$p$更近，即$d(neighbor(c), p) \u003c d(s,c)$ ，则用$neighbor(c)$替换集合$S$中的点$s$，并将$s$加入到候选集合$C$中。重复以上步骤直到$|c| = 0$ 从高层图向底层图搜索，使用高层图的结果$S$作为低层图$S$和$C$的初始值。 算法介绍 一下顺序只是为了便于理解，不代表论文发布顺序。更多细节可参考论文。\nNSW —— HNSW的起源? NSW可以视为邻近图，每个点维护至多$K$个距离其最近的点，此时HNSW退化为只有一层的特殊情况。\nNSW的构建 构建NSW的算法如下（此处忽略边角情况以方便理解\nINPUT: a set of points S OUTPUT: graph G BUILD_LAYER(S) G = [] # Insert each point into the graph FOREACH point IN S: ---------- INSERT_POINT(graph, point) neighbors[] = select_one_random(G) candidate[] = neighbors[] visited_points[] = neighbors[] # Code in this WHILE loop is to find the neighbors of the point # in current graph WHILE (!candidate.empty()) nearest_candidate = candidate.pop_nearest(point) furthest_neighbor = neighbors.get_furthest(point) # no candidate can b closer IF (distant(nearest_candidate, point) \u003e distant(furthest_neighbor, point)) break; # This candidate is great, but what about its neighbors? FOREACH candidate_neighbor in nearest_candidate.neighbors() IF (visited_points.has(candidate_neighbor)) continue visited_points.append(candidate_neighbor) # the furthest one can be changed furthest_neighbor = neighbors.get_furthest(point) # The neighbor of this candidate is also great, its neighbors # can also be candidates IF (distant(candidate_neighbor, point) \u003c distant(furthest_neighbor, point)) candidate.append(candidate_neighbor) neighbors.append(candidate_neighbor) IF (neighbors.size() \u003e MAX_NEIGHBORS) neighbors.pop_furthest(point) # Now we have found the neighbors, add a bidirection connections # between the each neighbor and the point FOREACH this_neighbor in neighbors add_bidirection_direction(this_neighbor, point) # Since the neighbor has one more connection, we may need # to shrink. This is a point to optimize. Read paper for detail. IF this_neighbor.neighbors().size() \u003e MAX_NEIGHBORS this_neighbor.drop_longest_connection() # All points have been added RETURN G NSW的搜索 OUTPUT: graph G, point P RETURN K nearest neighbors SEARCH_LAYER(G, p, K) candidates = select_one_random(G) visited_points = candidates # LOOP until we have K stable points WHILE TRUE candidates_old = candidates FOREACH candidate in candidates FOREACH neighbor in candidate.neighbors() if (visited_points.has(neighbor)) continue visited_points.add(neighbor) furthest_candidate = candidates.get_furthest(point) IF (distant(neighbor, P) \u003c distant(furthest_candidate, P) || candidates.size() \u003c K) candidates.add(neighbor) IF (candidates.size() \u003e K) candidates.pop_furthest(P) IF candidates_old == candidates BREAK RETURN candidates HNSW —— NSW的进化 显然，上述过程最大的问题之一为：\n对于图的构建：每新加入一个点，都需要从一个随机点开始搜索它的邻居。 对于图的搜索：需要从一个随机点开始搜索。 以上两点导致，NSW算法搜索了很多无用的点。 H(hierarchy)NSW 为解决这个问题，从NSW图（layer=0）中选出部分点，再构建一个缩略的NSW图（layer=1）。在搜索的时候，只需要从layer=1的图中搜索出一个粗略结果，将该结果用于layer=0搜索过程中的初始化，即可大量减少无用的搜索。同理，层数也不一定只有2层，可以有更多。 （这个思想在科研中似乎经常使用:先得出一个粗略的结果，再进一步精细化） 为了构建一个这样的图，我们在插入一个点时。\nINPUT: point P, a series of NSW graph G[] cur_layer = -ln(unif(0, 1)) * MAX_LAYER # for layer upper than current layer, just get a candidate FOR l from MAX_LAYER to cur_layer + 1 closest_points = SEARCH_LAYER(G[l], P, 1, candidates = closest_point) # insert into each layer from top to bottom of the below layers FOR l from cur_layer to 0 closest_points = SEARCH_LAYER(G[l], P, 1, candidates = closest_points) INSERT_POINT(graph, P, neighbors = closest_points) 同理在搜索时\nINPUT: point P, a series of NSW graph G[] cur_layer = P.layer # for layer upper than current layer, just get a candidate FOR l from MAX_LAYER to cur_layer + 1 closest_points = SEARCH_LAYER(G[l], P, 1, candidates = closest_point) # for layer leq than current layer, just get a candidate FOR l from cur_layer to 0 closest_points = SEARCH_LAYER(G[l], P, 1, candidates = closest_point) return closest_points PGVECTOR中的算法实现 INSERT /* * Algorithm 1 from paper: update graph by inserting an element * Parms: * @element: element to insert * @entryPoint: the initial entry point * @index? * @procinfo * @collation * @m: same as \"M\" in algo(number of established connections) */ HnswInsertElement(HnswElement element, HnswElement entryPoint, Relation index, FmgrInfo *procinfo, Oid collation, int m, int efConstruction, bool existing) level = element-\u003elevel; q = PointerGetDatum(element-\u003evec) # fill entry point list with the initial one ep = list_make1(HnswEntryCandidate(entryPoint,)) # for layers upper than the element's level for (int lc = entryLevel; lc \u003e= level + 1; lc--) # only get the nearest element now w = HnswSearchLayer() ep = w; # for the below layers for (int lc = level; lc \u003e= 0; lc) # search for top efConstruction nearest ones w = HnswSearchLayer(efConstruction) lw = w # get neighbors neighbors = SelectNeighbors(lw, lm, lc, procinfo, collation, NULL); # add connection # Is this different from paper? # bidirectional vs single directional # shrink directions or not shrink AddConnections() foreach(lc2, neighbors) a-\u003eitems[a-\u003elength++] = *((HnswCandidate *) lfirst(lc2)); search layer /* * Algorithm 2 from paper: search this layer with specifiyed enter points to * return \"ef\" closest neighbors * Parms: * @q: same as algo * @ep: enter points * @ef: count of closest neighbors * @lc: layer number * @index: * @procinfo: * @collation: * @inserting: * @skipElement: */ List * HnswSearchLayer(Datum q, List *ep, int ef, int lc, Relation index, FmgrInfo *procinfo, Oid collation, int m, bool inserting, HnswElement skipElement) v = NULL. # visited points C = NULL # set of candidates, nearer first W = NULL # dynamic found nearest neighbors # for each candidate in enter points foreach(lc2, ep) hc = (HnswCandidate *) lfirst(lc2); # HNSW candidates v.add(hc) C.add(hc) W.add(hc) # loop until no more candidates while (!C.empty()) c = C.pop_nearest() # for each neighbor \"e\" in the nearest candicate \"c\" neighborhood = \u0026c-\u003eelement-\u003eneighbors[lc]; for (int i = 0; i \u003c neighborhood-\u003elength; i++) # neighbor e HnswCandidate *e = \u0026neighborhood-\u003eitems[i]; v.add(e) DO # continue if visited # f is the furthest element in dynamic neighbors f = W.furthest() # find a good neighbor who is closer to q than the worst one in W if (DISTANT(e, q) \u003c DISTANT(f, q) || wlen \u003c ef) ec = e # neighbor of ec can also be the candidates C.add(ec) # add ec to W to promote the lower bound W.add(ec) # clean W if it's too large if (skipElement == NULL || list_length(e-\u003eelement-\u003eheaptids) != 0) wlen++; /* No need to decrement wlen */ if (wlen \u003e ef) W.pop_furthest return W pairing heap 配对堆 - OI Wiki (oi-wiki.org)\ninsert($\\mathrm{log}n$) random_select($\\mathrm{log} n$) select_min($\\mathrm{log} n$) delete_min($\\mathrm{log} n$) select neighbors /* * Algorithm 4: select neighbors starting with specified candidates * PARAMS: * @c : candidates * @m : number of neighbors to return * @lc: layer number * @ * * NOTES: * extendCandidates = false * keepPrunedConnections = true * pruned */ static List * SelectNeighbors(List *c, int m, int lc, FmgrInfo *procinfo, Oid collation, HnswCandidate * *pruned) r = NULL # results---returning neighbors w = c # working candidates wd = NULL; # discarded candidates; # Since we don't extend candidates, if the starting candidates isn't enought # just return. if (list_length(w) \u003c= m) return w # loop untils no more working candidate or enought neighbors while (length(w) \u003e 0 \u0026\u0026 length(r) \u003c m) *e = llast(w); # get the nearest candidates closer = CheckElementCloser(e, r, lc, procinfo, collation); if(closer) r.append(e) else wd.append(e) # loop until discarded candidates are empty or enough neighbors while (!wd.empty() \u0026\u0026 length(r) \u003c m) r.append(wd.pop_nearest()) prune = wd.nearest() return r data structure typedef struct HnswElementData { List\t*heaptids; uint8\tlevel; uint8\tdeleted; HnswNeighborArray *neighbors; BlockNumber blkno; OffsetNumber offno; OffsetNumber neighborOffno; BlockNumber neighborPage; Vector\t*vec; }\tHnswElementData; typedef struct HnswCandidate { HnswElement element; float\tdistance; }\tHnswCandidate; typedef struct HnswNeighborArray { int\tlength; HnswCandidate *items; }\tHnswNeighborArray; 底层实现中的问题 论文中的图是双向连接，而pgvector实现的是单向连接 pgvector中插入新向量时，没有更新其邻居的连接。（这么低级的问题有待验证） page representation ","ivfflat#IVFFlat":"概览 IVFFlat 算法主要包括以下几个步骤\n索引构建阶段 使用 KMeans 将数据集划分成多个簇(cluster) 查询阶段 通过每个簇的中心点（向量是高维的点）获取N个最近的簇 遍历这N个簇的所有点，从中找到最近的K个点 算法介绍 基础算法kmeans reference k-means clustering - Wikipedia 算法目标：选取K个中心点，使得数据集中的所有点到其最近的中心点“距离”之和最近，以平方和距离为例：\nGiven a set of observations $(x_1, x_2, \\dots, x_n)$, where each observation is a $d$-dimensional real vector, k-means clustering aims to partition the $n$ observations into $k$ ($\\leq n$) sets $S = {S_1, S_2, \\dot, S_k}$ so as to minimize the within-cluster sum of squares (WCSS). Formally, the objective is to find: 算法过程： 我们可以很容易的证明目标函数是关于$S$的凸函数 Given an initial set of $k$ means $m_1^{1}, \\dots , m_k^{(1)}$ (see below), the algorithm proceeds by alternating between two steps:\nAssignment step: Assign each observation to the cluster with the nearest mean: where each $x_p$is assigned to exactly one $S^{t}$, even if it could be assigned to two or more of them.\nUpdate step: Recalculate means (centroids) for observations assigned to each cluster.\nkmeans 优化篇 上述算法虽然简洁，但计算上复杂度高。在pgvector的IVFFlat实现中，使用了一些优化算法，主要是如下两篇论文：\nUsing Triangle Inequality: 使用三角不等式减少两点间距离的计算次数 KMeans++ :使用随机点的选取技巧来提高收敛速度和准确率 Using the Triangle Inequality to Accelerate k-Means (aaai.org) kMeansPP-soda.pdf (stanford.edu) Using Triangle Inequality 思路：\n在高维向量中，计算一次两点之间的距离的代价较高。 根据一些朴素的思想，假如使用的距离函数满足三角不等式$d(a,b) \\leq d(a,c) + d(b,c)$，那么在一次kmeams迭代中，如果点 x 距其中心点 c(x) 的距离很近，而 c(x) 距另一个中心点 c(y) 的距离很远，那么c(y)必然不是x 的中心点，这样就可以避免一次计算。 根据三角不等式可以推出\nLet x be a point and let b and c be centers. If $d(b, c) \u003e 2d(x,b)$, then $d(x,c) \\geq d(x,b)$ Let x be a point and let b and c be centers, then $d(x,c) \\geq \\mathrm{max} {0,d(x,b)-d(b,c)}$ 根据上述定理，在Kmeans迭代期间，维护一些状态，即可减少计算量 过程如 使用三角不等式优化Kmeans\nKMeans++ 论文中的数学分析很多，其主要目的为：通过在初始化的时候选取恰当的中心点，减少迭代次数。方法为： 假设向量的全集为$X={x_1,x_2,\\dots,x_n}\\subset \\mathbb{R}^d$ ,$D(x)$ 表示点 $x$ 到其当前中心点的距离\n从$X$ 中随机选择一个点$c_1$ 以$\\frac{D(x’)}{\\sum_{x\\in X}D(x)}$ 的概率选择$x’$为$c_i$ 重复上一步直到我们选择了 $k$ 个中心点， 使用标准的k-means算法进行后续处理 实现介绍 page representation Key functions index build 索引构建分为以下几个步骤\n计算中心点 构建元信息页（‘meta page’） 构建中心点页（‘centerid pages’） 构建数据页（‘data pages’） ivfflatbuild BuildIndex InitBuildState ComputeCenters CreateMetaPage CreateListPages CreateEntryPages FreeBuildState 计算中心点 实现上，没有扫描所有的行以计算中心点，而是“采样”一些block。 会选择$ncenter \\times 50$ 作为采样block的数量 采样算法 ComputeCenters SampleRows /* The number of target samples is the number of centers times 50 */ numSamples = buildstate-\u003elists * 50; buildstate-\u003esamples = VectorArrayInit(numSamples, buildstate-\u003edimensions); BlockSampler_Init \u003e provides algorithm for block level sampling of a relation as discussed on pgsql-hackers 2004-04-02 (subject \"Large DB\") Since we know the total number of blocks in advance, we can use the straightforward Algorithm S from Knuth 3.4.2, rather than Vitter's algorithm. reservoir_init_selection_state while (BlockSampler_HasMore(\u0026buildstate-\u003ebs)) table_index_build_range_scan: callback=SampleCallback IvfflatKmeans # Do as kmeans algrithm if (samples-\u003elength \u003c= centers-\u003emaxlen) QuickCenters(index, samples, centers); else ElkanKmeans(index, samples, centers); SampleCallback AddSample if (samples-\u003elength \u003c targsamples) VectorArraySet else if (buildstate-\u003erowstoskip \u003c 0) rowstoskip = reservoir_get_next_S #skip some future samples else k = sampler_random_fract VectorArraySet\t# replace a old with this one randomly 构建元信息页 CreateMetaPage # info about meta information IvfflatNewBuffer IvfflatInitRegisterPage IvfflatCommitBuffer 构建中心点页 当一个页的剩余空间不够时，使用字段nextblkno指向下一个页\ntypedef struct IvfflatPageOpaqueData { BlockNumber nextblkno; uint16\tunused; uint16\tpage_id;\t/* for identification of IVFFlat indexes */ }\tIvfflatPageOpaqueData; CreateListPages # info about center infomation foreach sampled vector if (PageGetFreeSpace \u003c listSize) # we need more free space to store the vector IvfflatAppendPage newbuf = IvfflatNewBuffer newpage = GenericXLogRegisterBuffer IvfflatPageGetOpaque old_page-\u003enext = this_page IvfflatInitPage PageAddItem # copy this point to the page 构建数据页 CreateEntryPages # omit parallel optimization here AssignTuples # Scan table for tuples to index tuplesort_performsort InsertTuples for (int i = 0; i \u003c buildstate-\u003ecenters-\u003elength; i++) buf = IvfflatNewBuffer(index, forkNum); # add new page for each data page list startPage = BufferGetBlockNumber(buf); # the first page number foreach tuple in this list: if (PageGetFreeSpace(page) \u003c itemsz) # append page IvfflatAppendPage(index, \u0026buf, \u0026page, \u0026state, forkNum); PageAddItem() IvfflatUpdateList(); # update the first page record of the center page index scan begin scan ivfflatbeginscan IvfflatGetMetaPageInfo(index, \u0026lists, \u0026dimensions); # Get lists and dimensions from metapage get tupele ivfflatgettuple if (first) # try to get the first tuple GetScanLists # find 'probe' centers that are closest while (BlockNumberIsValid(nextblkno)) # search all list pages if (distance \u003c maxDistance) # omit probe here for easier understanding scanlist = (IvfflatScanList *) pairingheap_remove_first(so-\u003elistQueue); pairingheap_add(so-\u003elistQueue, \u0026scanlist-\u003eph_node); maxDistance = ((IvfflatScanList *) pairingheap_first(so-\u003elistQueue))-\u003edistance; GetScanItems # find closest items in the above centers while (!pairingheap_is_empty(so-\u003elistQueue)) # for each center while (BlockNumberIsValid(searchPage)) # for each block in the data list foreach (tuple) tuplesort_puttupleslot ","vector-database-调研#vector database 调研":"qdrant Vector databases are optimized for storing and querying these high-dimensional vectors efficiently, and they often using specialized data structures and indexing techniques such as Hierarchical Navigable Small World (HNSW) – which is used to implement Approximate Nearest Neighbors – and Product Quantization, among others.\n算法与存储 qdrant使用 hnsw 算法\nA key feature of Qdrant is the effective combination of vector and traditional indexes. It is essential to have this because for vector search to work effectively with filters, having vector index only is not enough. In simpler terms, a vector index speeds up vector search, and payload indexes speed up filtering.\npayload 索引仅用于过滤，我们关注向量索引部分\nQdrant currently only uses HNSW as a vector index.\nAll data within one collection is divided into segments. Each segment has its independent vector and payload storage as well as indexes.","后记#后记":" pg官方的新闻：PostgreSQL: pgvector 0.5.0 Released! 。pgvector在社区的热度不小 ","序言#序言":"pgvector是一个向量搜索（根据近似度）的插件，用来加速AKNN（approximate nearest neighbor）。 PASE中提到，向量ANN算法包括4类\ntree-based algorithms KD-Tree RTree quantization-based algorithms IVFFlat IVFADC IMI graph based algorithms HNSW NSG SSG hash-base algorithms LSH pgvector 包括两个算法，IVFFlat 和 HNSW，后续内容将以这两个算法的内容及其实现展开。 ","附录#附录":"trianlge-inequality-Kmeans 维护的状态： lower bound $l(x,c)$ of $d(x,c)$ for each point $x$ and center $c$ each time $d(x,c)$ is computed, set $l(x,c)=d(x,c)$ $c(x)= \\mathrm{argmin}_cd(x,c)$ get its center for each point $x$ upper bound $u(x)$ of $d(x,c)$ for each point $x$, indicating the upper bound of $x$ to its center $r(x)$ is a boolean value indicate whether $u(x, c)$ is out of date 过程： initialization compute $d(x,c)$ for each point $x$ and each center $c$, which means $l(x,c)$ is computed too $u(x)=\\mathrm{min}_c(d,c)$ for each point $x$ repeate until convergence: For each pair of centers $c$ and $c’$ , compute $d(c,c’)$, this is to compute $s(c)=1/2\\min_{c’\\neq c}d(c,c’)$ . So we get the distance to the nearest center of each center identify all point $x$ such that $u(x) \\le s(c(x))$ .If the point is so near to its center, its center can’t be changed in this iteration. See lemma 1 For each remaining point $x$ and centers $c$ such that $c\\neq c(x)$ (not the current center) and $u(x)\u003el(x,c)$ (upper bound to current center greater than lower bound of this center) and $u(x)\u003e\\frac{1}{2}d(c(x),c)$(upper bound to current center greater than half of the two centers distant, See lemma 1) iterm 2 and 3 means $u(x)$ may be too big DO: If $r(x)$ , compute$d(x, c(x))$ and set $r(x)=false$, else $d(x,c(x))=u(x)$ if $u(x)\u003el(x,c)$ and $u(x)\u003e\\frac{1}{2}d(c(x),c)$ then (same as the above) compute $d(x,c)$, if $d(x,c) \u003c d(x,c(x))$ then assign $c(x)=c$ (update center) For each center $c$ , let $m(c)$ be the new mean point For each point $x$ and center $c$, assign $l(x,c)=\\max{l(x,c)-d(c,m(c))}$ (update lower bound by lemma 2) For each point x, assign $u(x)=u(x) + d(m(c(x)),c(x))$ (update lower bound by lemma 2 ) and $r(x)=true$ Replace each center $c$ by $m(c)$ 采样算法 Knuth’s algorithm S ^0ff671\n算法描述： Select $n$ items from a set of $M$ iems with equal probility for $M \\geq n$ 实现 samples = set[0:n-1] for i in (n, M) with prob = n/i: samples[random()%n] = set[i] 参考文档 Knuth’s algorithm S - Rosetta Code 。\n社区讨论 [PostgreSQL: Re: GENERAL] Large DB"},"title":"PGVECTOR AND VECTOR DATABASE"},"/database/predicate_lock/":{"data":{"intro#Intro":"Intro","phantoms#Phantoms":"The idea comes from which is an object, the table or its each tuple. The former\nis unacceptable. But the latter, when tupples appear or disappear from a table,\nselect statements can conflict with the change, which is called the phantoms.","predicate-lock#Predicate Lock":"Although predicate lock is too expensive, it’s a good way to think about things.\npredicate lock can be writen like\n\u003ctid, [slock|xlock], predicate\u003e Two predicate locks are compatible iff\ntid1 = tid2 (a transaction can't conflict with itself), or both are slock (no changes makes no conflict ), or predicate1 and predicate2 can't be satisfied at the same time When applying the lock system, use two structure, a granted set and a waiting list.\nEach time a tx requries a predicate lock, the system compares the lock with each other in the granted set and waiting list. If the predicate lock is compatibal, add it to granted set, otherwise to waiting list. Each time a tx ends, remove all its predicated lock from both the granted set and the waiting list. And grant each predicate lock in the waiting list if it’s compatible now, until reach the end of the list or encouter an incompatible predicate lock. ","the-problem-with-predicate-locks#The problem with predicate locks":" "},"title":"Phantoms Predicate Lock"},"/database/read-google-f1/":{"data":{"":"","abstract#Abstract":" a protocol for schema evolution in a globally distributed database management system with shared data, stateless servers, and no global membership. asynchronous all servers can access and update all data during a schema change ","background#BackGround":"In this section, we:\nseparate the interface provided by the key–value store from its implementation show how we map traditional relational database features into this unique setting Key-value store F1 assumes the key–value store supports three operations put: insert a value with a given key del: delete a value with a given key get: returns any stored values whose key matches a given prefix Note that put and del reference exactly one key–value pair, while get may return multiple key–value pairs\nTwo more requirements Commit timestamps: Every key–value pair has a last-modified timestamp which is updated atomically by the key–value store Atomic test-and-set support: Multiple get and put operations can be executed atomically Relational schema An F1 schema is a set of table definitions that enable F1 to interpret the database located in the key–value store Each table definition has: a list of columns a list of secondary indexes a list of integrity constraints(foreign key or index uniqueness constraints) a list of optimistic locks. required columns that cannot be read directly by client transactions A subset of columns in a table forms the primary key of the table We call a column required if its value must be present in every row. All primary-key columns are implicitly required, while non-key columns may be either required or optional Row representation one pair for each non-primary-key column\nEach key logically includes\nthe name of the table, the primary key values of the containing row, the name of the column whose value is stored in the pair Although this appears to needlessly repeat all primary key values in the key for each column value, in practice, F1’s physical storage format eliminates this redundancy\nA secondary index covers a non-empty subset of columns on a table is itself represented by a set of key–value pairs in the key– value store Each row in the indexed table has an associated index key–value pair The key for this pair is formed by concatenating the table name the index name the row’s indexed column values and the row’s primary key values We denote the index key for row $r$ in index $I$ as $k_r(I)$ the special exists column doesn’t have the associated value Relational operations F1 supports a set of standard relational operations:\n$insert(R,vk_r,vc_r)$ inserts row r to table R with primary key values $vk_r$ and non-key column values $vc_r$. Insert fails if a row with the same primary key values already exists in table R. $delete (R, vk_r )$ $update(R,vk_r,vc_r)$ $query(\\vec{R},\\vec{C},P)$ :returns a projection $\\vec{C}$ of rows from tables in $\\vec{R}$ that satisfy predicate $P$ . ","introduction#INTRODUCTION":" Schema evolution: the ability to change a database’s definition without the loss of data F1 is built on top of Spanner, a globally distributed KV data store Main feature The main features of F1 that impact schema changes are:\nMassively distributed: An instance of F1 consists of hundreds of individual F1 servers Relational schema: Each F1 server has a copy of a relational schema that describes tables, columns, indexes, and constraints. Any modification to the schema requires a distributed schema change to update all servers Shared data storage: All F1 servers in all datacenters have access to all data stored in Spanner. Stateless servers: F1 servers must tolerate machine failures, preemption(取代), and loss of access to network resources clients may connect to any F1 server, even for different statements in the same transaction. No global membership: no reliable mechanism for determining currently running F1 servers, and explicit global synchronization is not possible several constraints on the schema change process:\nFull data availability: the availability of the data managed by F1 is paramount(至为重要的) it is unacceptable to take even a portion of the database offline during a schema change (e.g., locking a column to build an index). Minimal performance impact: the F1 schema changes rapidly to support new features Asynchronous schema change In other words, different F1 servers may transition to using a new schema at different times These requirements influenced the design in several ways\nSince all data must be as available as possible, we do not restrict access to data undergoing reorganization. Because the schema change must have minimal impact on user transactions, we allow transactions to span an arbitrary number of schema changes, although we do not automatically rewrite queries to conform to the schema in use Applying schema changes asynchronously on individual F1 servers means that multiple versions of the schema may be in use simultaneously An example Consider a schema change from schema S1 to schema S2 that adds index I on table R Assume two different servers, M1 and M2, execute the following sequence of operations: Server M2, using schema S2, inserts a new row r to table R. Because S2 contains index I, server M2 also adds a new index entry corresponding to r to the key– value store. Server M1, using schema S1, deletes r. Because S1 does not contain I, M1 removes r from the key–value store but fails to remove the corresponding index entry in I. The second delete leaves the database corrupt. We consider not only changes to the logical schema, such as the addition or removal of columns, but also changes to the physical schema like adding or removing secondary indexes.\nBy ensuring that:\nno more than two schema versions are in use at any given time those schema versions have specific properties? enables distributed schema changes in a way that\ndoes not require global membership, implicit or explicit synchronization between nodes, or the need to retain old schema versions once a schema change is complete"},"title":"Read Google F1"},"/linux/":{"data":{"":"","这里记录什么#这里记录什么":" chapter3:file "},"title":"Linux"},"/linux/chapter3/":{"data":{"":"","atomic-operations#Atomic Operations":"","close-function#\u003ccode\u003eclose\u003c/code\u003e function":"","creat-function#\u003ccode\u003ecreat\u003c/code\u003e function":"","devfd#\u003ccode\u003e/dev/fd\u003c/code\u003e":"The functions described in this chapter are often referred to as unbuffered I/O(which each read or write invokes a system call in the kernel), in contrast to the standard I/O routines\nFile Descriptors To the kernel, all open files are referred to by file descriptors. A file descriptor is a non-negative integer. When we open an existing file or create a new file, the kernel returns a file descriptor to the process. When we want to read or write a file, we identify the file with the file descriptor that was returned by open or creat as an argument to either read or write. By convention, UNIX System shells associate file descriptor 0 with the standard input of a process, file descriptor 1 with the standard output, and file descriptor 2 with the standard error File descriptors range from 0 through OPEN_MAX−1 open and openat code #include \u003cfcntl.h\u003e int open(const char *path, int oflag, ... /* mode_t mode */ ); int openat(int fd, const char *path, int oflag, ... /* mode_t mode */ ); //Both return: file descriptor if OK, −1 on error This function has a multitude of options, which are specified by the oflag argument. This argument is formed by ORing together one or more of the following constants from the\u003cfcntl.h\u003e header\nO_SYNC Have each write wait for physical I/O to complete, including I/O necessary to update file attributes modified as a result of the write. O_DSYNC Have each write wait for physical I/O to complete, but don’t wait for file attributes to be updated if they don’t affect the ability to read the data just written. The O_DSYNC flag affects a file’s attributes only when they need to be updated to reflect a change in the file’s data (for example, update the file’s size to reflect more data)\nO_RSYNC Have each read operation on the file descriptor wait until any pending writes for the same portion of the file are complete\nThe fd parameter distinguishes the openat function from the open function. There are three possibilities\nThe path parameter specifies an absolute pathname. In this case, the fd parameter is ignored and the openat function behaves like the open function. The path parameter specifies a relative pathname and the fd parameter is a file descriptor that specifies the starting location in the file system where the relative pathname is to be evaluated. The fd parameter is obtained by opening the directory where the relative pathname is to be evaluated. The path parameter specifies a relative pathname and the fd parameter has the special value AT_FDCWD. In this case, the pathname is evaluated starting in the current working directory and the openat function behaves like the open function. openat solved two problems\nIt gives threads a way to use relative pathnames to open files in directories other than the current working directory while all threads in the same process share the same current working directory, so this makes it difficult for multiple threads in the same process to work in different directories at the same time it provides a way to avoid time-of-check-to-time-of-use (TOCTTOU) errors whose baisc idea is that a program is vulnerable if it makes two file-based function calls where the second call depends on the results of the first call. Because the two calls are not atomic, the file can change between the two calls, thereby invalidating the results of the first call, leading to a program error. creat function #include \u003cfcntl.h\u003e int creat(const char *path, mode_t mode); This is equivalent to\nopen(path, O_WRONLY | O_CREAT | O_TRUNC, mode); close function #include \u003cunistd.h\u003e int close(int fd); When a process terminates, all of its open files are closed automatically by the kernel\nlseek function Every open file has an associated ‘’current file offset,’’ normally a non-negative integer that measures the number of bytes from the beginning of the file.Read and write operations normally start at the current file offset and cause the offset to be incremented by the number of bytes read or written An open file’s offset can be set explicitly by calling lseek #include \u003cunistd.h\u003e off_t lseek(int fd, off_t offset, int whence); If whence is SEEK_SET, the file’s offset is set to offset bytes from the beginning of the file If whence is SEEK_CUR, the file’s offset is set to its current value plus the offset. The offset can be positive or negative If whence is SEEK_END, the file’s offset is set to the size of the file plus the offset. The offset can be positive or negative Because a successful call to lseek returns the new file offset, we can seek zero bytes from the current position to determine the current offset off_t currpos; currpos = lseek(fd, 0, SEEK_CUR); This technique can also be used to determine if a file is capable of seeking. If the file descriptor refers to a pipe, FIFO, or socket, lseek sets errno to ESPIPE and returns −1 Normally,a file’s current offset must be a non-negative integer . Because negative offsets are possible, we should be careful to compare the return value from lseek as being equal to or not equal to −1, rather than testing whether it is less than 0. The file’s offset can be greater than the file’s current size, in which case the next write to the file will extend the file. This is referred to as creating a hole in a file and is allowed. Any bytes in a file that have not been written are read back as 0. A hole in a file isn’t required to have storage backing it on disk read function #include \u003cunistd.h\u003e ssize_t read(int fd, void *buf, size_t nbytes); If the read is successful, the number of bytes read is returned. If the end of file is encountered, 0 is returned. There are several cases in which the number of bytes actually read is less than the amount requested:\nWhen reading from a regular file, if the end of file is reached before the requested number of bytes has been read. When reading from a terminal device. When reading from a network When reading from a pipe or FIFO. When reading from a record-oriented device When interrupted by a signal and a partial amount of data has already been read. classic definition int read(int fd, char *buf, unsigned nbytes); difference\nvoid * to char * 0 for end-of-file and -1 for an error write function #include \u003cunistd.h\u003e ssize_t write(int fd, const void *buf, size_t nbytes) The return value is usually equal to the nbytes argument; otherwise, an error has occurred. A common cause for a write error is either filling up a disk or exceeding the file size limit for a given process\nI/O efficiency an example\n#include \"apue.h\" #define BUFFSIZE 4096 int main(void) { int n; char buf[BUFFSIZE]; while ((n = read(STDIN_FILENO, buf, BUFFSIZE)) \u003e 0) if (write(STDOUT_FILENO, buf, n) != n) err_sys(\"write error\"); if (n \u003c 0) err_sys(\"read error\"); exit(0); } some caveats\nIt reads from standard input and writes to standard output, assuming that these have been set up by the shell before this program is executed The program doesn’t close the input file or output file. This example works for both text files and binary file how we chose the BUFFSIZE value?\npic/Screenshot-2021-05-04-19-58-12.png file sharing The UNIX System supports the sharing of open files among different processes.\nThe kernel uses three data structures to represent an open file, and the relationships among them determine the effect one process has on another with regard to file sharing\nEvery process has an entry in the process table. Within each process table entry is a table of open file descriptors, which we can think of as a vector, with one entry per descriptor. Associated with each file descriptor are The file descriptor flags A pointer to a file table entry The kernel maintains a file table for all open files. Each file table entry contains The file status flags for the file, such as read, write, append, sync, and nonblocking; more on these in Section 3.14 The current file offset A pointer to the v-node table entry for the file Each open file (or device) has a v-node structure that contains information about the type of file and pointers to functions that operate on the file. For most files, the v-node also contains the i-node for the file. This information is read from disk when the file is opened, so that all the pertinent information about the file is readily available. For example, the i-node contains the owner of the file, the size of the file, pointers to where the actual data blocks for the file are located on disk, and so on file.png If two independent processes have the same file open, we could have the arrangement\nEach process that opens the file gets its own file table entry, but only a single v-node table entry is required for a given file. One reason each process gets its own file table entry is so that each process has its own current offset for the file.\nAfter each write is complete, the current file offset in the file table entry is incremented by the number of bytes written. If this causes the current file offset to exceed the current file size, the current file size in the i-node table entry is set to the current file offset (for example, the file is extended). If a file is opened with the O_APPEND flag, a corresponding flag is set in the file status flags of the file table entry. Each time a write is performed for a file with this append flag set, the current file offset in the file table entry is first set to the current file size from the i-node table entry. This forces every write to be appended to the current end of file. If a file is positioned to its current end of file using lseek, all that happens is the current file offset in the file table entry is set to the current file size from the i-node table entry (Note that this is not the same as if the file was opened with the O_APPEND flag) The lseek function modifies only the current file offset in the file table entry. No I/O takes place It is possible for more than one file descriptor entry to point to the same file table entry. This also happens after a fork when the parent and the child share the same file table entry for each open descriptor\nNote the difference in scope between the file descriptor flags and the file status flags. The former apply only to a single descriptor in a single process, whereas the latter apply to all descriptors in any process that point to the given file table entry\nAtomic Operations Any operation that requires more than one function call cannot be atomic, as there is always the possibility that the kernel might temporarily suspend the process between the two function calls\n#include \u003cunistd.h\u003e ssize_t pread(int fd, void *buf, size_t nbytes, off_t offset); //Returns: number of bytes read, 0 if end of file, −1 on error ssize_t pwrite(int fd, const void *buf, size_t nbytes, off_t offset); //Returns: number of bytes written if OK, −1 on error If the operation is performed atomically, either all the steps are performed (on success) or none are performed (on failure).\ndup and dup2 Functions An existing file descriptor is duplicated by either of the following functions\n#include \u003cunistd.h\u003e int dup(int fd); int dup2(int fd, int fd2); //Both return: new file descriptor if OK, −1 on error With dup2, we specify the value of the new descriptor with the fd2 argument. If fd2 is already open, it is first closed. If fd equals fd2, then dup2 returns fd2 without closing it. Otherwise, the FD_CLOEXEC file descriptor flag is cleared for fd2, so that fd2 is left open if the process calls exec\npic/Screenshot-2021-05-05-09-34-21.png sync,fsync , and fdatasync function Traditional implementations of the UNIX System have a buffer cache or page cache in the kernel through which most disk I/O passes. When we write data to a file, the data is normally copied by the kernel into one of its buffers and queued for writing to disk at some later time. This is called delayed write\nTo ensure consistency of the file system on disk with the contents of the buffer cache, the sync, fsync, and fdatasync functions are provided.\n#include \u003cunistd.h\u003e int fsync(int fd); int fdatasync(int fd); //Returns: 0 if OK, −1 on error void sync(void); The sync function simply queues all the modified block buffers for writing and returns; it does not wait for the disk writes to take place.The function sync is normally called periodically (usually every 30 seconds) from a system daemon, often called update.\nThe function fsync refers only to a single file, specified by the file descriptor fd, and waits for the disk writes to complete before returning.(database)\nThe fdatasync function is similar to fsync, but it affects only the data portions of a file. With fsync, the file’s attributes are also updated synchronously\nfcntl function #include \u003cfcntl.h\u003e int fcntl(int fd, int cmd, ... /* int arg */ ); //Returns: depends on cmd if OK (see following), −1 on error The fcntl function is used for five different purposes\nDuplicate an existing descriptor (cmd = F_DUPFD or F_DUPFD_CLOEXEC) Get/set file descriptor flags (cmd = F_GETFD or F_SETFD) Get/set file status flags (cmd = F_GETFL or F_SETFL) Get/set asynchronous I/O ownership (cmd = F_GETOWN or F_SETOWN) Get/set record locks (cmd = F_GETLK, F_SETLK, or F_SETLKW) ioctl function The ioctl function has always been the catchall for I/O operations. Terminal I/O was the biggest user of this function\n#include \u003cunistd.h\u003e /* System V */ #include \u003csys/ioctl.h\u003e /* BSD and Linux */ int ioctl(int fd, int request, ...); //Returns: −1 on error, something else if OK Normally, additional device-specific headers are required. For example, the ioctl commands for terminal I/O, beyond the basic operations specified by POSIX.1, all require the header.\nEach device driver can define its own set of ioctl commands. The system, however, provides generic ioctl commands for different classes of devices\n/dev/fd Newer systems provide a directory named /dev/fd whose entries are files named 0, 1, 2, and so on\nIn the function call\nfd = open(\"/dev/fd/0\", mode); most systems ignore the specified mode, whereas others require that it be a subset of the mode used when the referenced file (standard input, in this case) was originally opened. Because the previous open is equivalent to\nfd = dup(0); the descriptors 0 and fd share the same file table entry\nFor example, if descriptor 0 was opened read-only, we can only read on fd. Even if the system ignores the open mode and the call succeeds, we still can’t write to fd.\nThe main use of the /dev/fd files is from the shell. It allows programs that use pathname arguments to handle standard input and standard output in the same manner as other pathnames, like cat - to cat /dev/fd/0\nThe special meaning of - as a command-line argument to refer to the standard input or the standard output is a kludge that has crept into many programs. There are also problems if we specify - as the first file, as it looks like the start of another command-line option. Using /dev/fd is a step toward uniformity and cleanliness.","dup-and-dup2-functions#dup and dup2 Functions":"","fcntl-function#\u003ccode\u003efcntl\u003c/code\u003e function":"","file-descriptors#File Descriptors":"","file-sharing#file sharing":"","io-efficiency#I/O efficiency":"","ioctl-function#\u003ccode\u003eioctl\u003c/code\u003e function":"","lseek-function#\u003ccode\u003elseek\u003c/code\u003e function":"","open-and-openat#\u003ccode\u003eopen\u003c/code\u003e and \u003ccode\u003eopenat\u003c/code\u003e":"","read-function#\u003ccode\u003eread\u003c/code\u003e function":"","syncfsync--and-fdatasync-function#\u003ccode\u003esync\u003c/code\u003e,\u003ccode\u003efsync\u003c/code\u003e , and \u003ccode\u003efdatasync\u003c/code\u003e function":"","write-function#\u003ccode\u003ewrite\u003c/code\u003e function":""},"title":"APUE/Chapter3: file and I/O"},"/miscellaneous/cublasdgemmtutor/":{"data":{"":"","basic-use#basic use":"Definition of this function\ncublasStatus_t cublasDgemm(cublasHandle_t handle, cublasOperation_t transa, cublasOperation_t transb, int m, int n, int k, const double *alpha, const double *A, int lda, const double *B, int ldb, const double *beta, double *C, int ldc) Basic information of parameters is show in this page. Simply put, $C = \\alpha A \\times B + \\beta C $ .But it may remains confused for fresher. Below is an simple example.\n/* A is matrix in gpu memory looks like * 1 2 3 * 4 5 6 * 7 8 9 * and ptr_A is a pointer to A * * B is matrix in gpu memory looks like * 1 2 * 3 4 * 5 6 * and ptr_A is a pointer to A * * While memory is one-dimensional while matrix is two-dimensional, I * suggeset that all matrix in gpu memory are stored in column major for * convevient use of cublas. In this case, A in memory is like * [1, 4, 7, 2, 5, 8, 3, 6, 9]. * C is a matrix to store the product of A * B */ //get handle and stat of this function cublasHandle_t handle; cublasStatus_t stat = cublasCreate(\u0026handle); if (stat != CUBLAS_STATUS_SUCCESS) { printf(\"CUBLAS initialization failed\\n\"); return EXIT_FAILURE; } //setting alpha and cuda double alpha = 1.0, beta = 0.0; stat = cublasDgemm(\thandle, CUBLAS_OP_N,\t// we use matrix A instead of A^T CUBLAS_OP_N,\t// we use matrix B instead of B^T 3,\t// the row of A 2,\t// the col of B 3,\t// the row of B(ro col of A) \u0026alpha, devPtrA, 3,\t// the leading dimension of A devPtrB, 3,\t// the leading dimension of B \u0026beta, devPtrC, 3);\t// the leading dimension of C /* * if we want to compute C = A^T * B */ stat = cublasDgemm(\thandle, CUBLAS_OP_T,\t// we use matrix A^T instead of A CUBLAS_OP_N,\t// we use matrix A instead of B^T 3,\t// the row of A^T 2,\t// the col of B 3,\t// the row of B(or col of A^T) \u0026alpha, devPtrA, 3,\t// the leading dimension of A^T. // So whether or not A or A^T, the leading dimension // of A or A^T is the row of A, decided when A is // initialized in memory devPtrB, 3,\t// the leading dimension of B \u0026beta, devPtrC, 3);\t// the leading dimension of C if (stat != CUBLAS_STATUS_SUCCESS) { printf(\"cublasSgemm failed\\n\"); return EXIT_FAILURE; } An obvious question is what is leading dimension for we have know the column and row of A and B, no more information is need to finish this computation.\nMy understanding of leading dimension is the offest to get the element in next column at the same row. An implement to compute the product of submatrix. below is an example. A and B are the same matrix in the previous example.\nAnd what we want to compute is $A[0:1][0:1] \\times B[1:2][0:1]$.\nstat = cublasDgemm(\thandle, CUBLAS_OP_N,\t// we use matrix A[0:1][0:1] instead of A[0:1][0:1]^T CUBLAS_OP_N,\t// we use matrix B[1:2][0:1] instead of B[1:2][0:1]^T 2,\t// the row of A[0:1][0:1] 2,\t// the col of B[1:2][0:1] 2,\t// the row of A[0:1][0:1](or col of B[1:2][0:1]) \u0026alpha, devPtrA,\t// pointer to A[0][0] 3,\t// the offset of A[0][0] to A[0][1] is 3 of double size devPtrB + 1,\t// pointer to B[1][0] 3,\t// the offset of B[1][0] to B[1][1] is 3 of double size \u0026beta, devPtrC, 2);\t// the leading dimension of C So it’s the use of leading dimension which makes matrix production more flexible","concept#concept":"concept cublasDgemm is a convenient function in cublas to compute the product of two matrix, while letter ‘D’ in cublasDgemm means double.\nBefore reading this post, basic cuda functions like cudaMalloc are what you are supposed to know."},"title":"basic of cublasDgemm"},"/miscellaneous/howtoknowwhoseisbigger/":{"data":{"":"","definiteness#Definiteness：":"Suppose Alice has number $i$ and Bob has number $j$ and $1\\leq i,j \\leq 9$. We need a protocol for them to decide whether $i \u003c j$ in the end(aside from their own values)","solution#Solution:":"Let $M$ be the set of all $N$-bit nonnegative integers\nLet $Q_N$ be the set of all one-one and onto function from $M$ to $M$\nAlice generates a public key from $Q_N$, called $E_a$, and the inverse function of $E_a$ is $D_a$ Bob picks a random value $x \\in M$, compute $k = E_a(x)$, then send $k - j$ to Alice Alice computes $y_u=D_a(k - j + u)$ for $u = 1,2,\\dots,9$ Alice generates a random prime $p$ of $N/2$-bit, and computes $z_u=y_u(\\mod p)$ for all $u$. Alice repeats step 4 until all $z_u$ differ by at least 2 in the $\\mod p$ sense Alice sends the $p$ and $z_1,z_2,\\dots,z_i,z_{i+1}+1,\\dots,z_{9} +1$ (all in $\\mod p$ sense)to Bob Bob looks at the $j$-th value(not counting p) sent from Alice, and decides that $i\\geq j$ if it is equal to $x \\mod p$, or $i\u003cj$ otherwise "},"title":"Alice and Bob how to know whose number is bigger without giving away their own's"},"/miscellaneous/somethingtousingdocker/":{"data":{"":"","install-libraries#install libraries":"It’s common to find that a linux os container lacks of libraries when linking because linux docker image is minimal. So how to install libraries when getting an error like cannot find -lcrc32c if you don’t know the library’s name? In ubuntu, you can install libcrc*.l means lib and you can use tab to autocompletion the library’s name, and then you can find the library you need is called libcrcutil-dev.","set-proxy#set proxy":"It’s annoying that you can’t download third-party libraries from github because of network when using linux os docker image to develop. So using proxy in docker is necessary.\nThis is the official website to use proxy in docker\nBelow is one of the ways if you have root’ permission\nsudo mkdir -p /etc/systemd/system/docker.service.d sudo touch /etc/systemd/system/docker.service.d/proxy.conf add the below content to proxy.conf\nsudo touch /etc/systemd/system/docker.service.d/proxy.conf then restart docker\nsudo systemctl daemon-reload sudo systemctl restart docker notice: This is not to say that docker container will connect to the port in your physics machine. You still need to install your vpn client in each of your containers."},"title":"something to using docker"}}