<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
   <meta name="description" content="high level view
See Queries in PostgreSQL: 6. Hashing
One-pass hash join
Note that join in PostgreSql, we scan the right relation first, which means that
the right relation is the &ldquo;inner relation&rdquo; and the left relation is the outer
one.

Two-pass hash join
Since we can&rsquo;t allocate as much memory as we want, instead of building a hash
table of the entire table, PG split the tables to several batches where all
tuples have the same hash value flag.">  

  <title>
    
      hash join
    
  </title>


  <link rel="shortcut icon" type="image/x-icon" href="/" />
  
  
  
  <link rel="stylesheet" href="/css/main.51652302d3a998bf7887aed5c2cf89141bbebdf45a2c8f87b0717a3cf4f51c4e53c694c328fb1de78c3a625a1c01f80745bf1f2f42c040647a245cbbb6c2d1d7.css" integrity="sha512-UWUjAtOpmL94h67Vws&#43;JFBu&#43;vfRaLI&#43;HsHF6PPT1HE5TxpTDKPsd54w6YlocAfgHRb8fL0LAQGR6JFy7tsLR1w==" />
   <script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$','$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };

  window.addEventListener('load', (event) => {
      document.querySelectorAll("mjx-container").forEach(function(x){
        x.parentElement.classList += 'has-jax'})
    });

</script>
<script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
 
</head>
<body a="auto">
        <main class="page-content" aria-label="Content">
            <div class="w">
<a href="/">..</a>


<article>
    <p class="post-meta">
        <time datetime="0001-01-01 00:00:00 &#43;0000 UTC">
            0001-01-01
        </time>
    </p>

    <h1>hash join</h1>

    

    <h2 id="high-level-view">high level view</h2>
<p>See <a href="https://postgrespro.com/blog/pgsql/5969673">Queries in PostgreSQL: 6. Hashing</a></p>
<h3 id="one-pass-hash-join">One-pass hash join</h3>
<p>Note that join in PostgreSql, we scan the right relation first, which means that
the right relation is the &ldquo;inner relation&rdquo; and the left relation is the outer
one.
<img src="https://mobilephone724.oss-cn-beijing.aliyuncs.com/blog/database/hashjoin/One-pass_hash_join.svg" alt="Alt text"></p>
<h3 id="two-pass-hash-join">Two-pass hash join</h3>
<p>Since we can&rsquo;t allocate as much memory as we want, instead of building a hash
table of the entire table, PG split the tables to several <code>batches</code> where all
tuples have the same hash value flag.</p>
<p>Batches are splited by hash value. Use several bits in hash value as a flag so
we can put the tuples into different batches.</p>
<p><img src="https://mobilephone724.oss-cn-beijing.aliyuncs.com/blog/database/hashjoin/Two-pass_hash_join.svg" alt="Alt text"></p>
<p>There is a simple optimization that we can build the hash table in the first batch while scanning the inner table, and match the pair while scanning the outer table.</p>
<h3 id="parallel-one-pass-hash-join">parallel one-pass hash join</h3>
<p><img src="https://raw.githubusercontent.com/mobilephone724/blog_pictures/master/parallel_one-pass_hash_join.2024_02_12_1707674700.svg" alt="parallel_one-pass_hash_join"></p>
<p>With parallel workers, we can</p>
<ul>
<li>scan inner table and build shared hash table parallelly</li>
<li>scan outer table parallelly</li>
</ul>
<p>Although in most cases, the neck of tp system is disk io, but parallel workers can still advance the speed efficiently. Because:</p>
<ul>
<li>In single process situation, the disk IO is synchronousï¼Œwhich means CPU is in idle while waiting IO. So, in the parallel case, CPU can be utilized more sufficiently.</li>
<li>OS may has the technique to load the disk&rsquo;s content in advance, which is perdicularly useful in sequence scan. So multi-workers can read data file content more efficiently.</li>
<li>In hash join, the compute of hash value may cost more CPU resource than normal TP operation.</li>
</ul>
<h3 id="parallel-two-pass-hash-join">parallel two-pass hash join</h3>
<p><img src="https://raw.githubusercontent.com/mobilephone724/blog_pictures/master/parallel_two-pass_hash_join.2024_02_12_1707674716.svg" alt="parallel_two-pass_hash_join"></p>
<p>Same as the basic two-pass hash join, parallel workers build batches parallelly, both in reading from inner/outer tuple and writing data to tmp file. Since no worker can obtain a whole batch&rsquo;s data in the first scan, the technique described above can be used here.</p>
<h2 id="low-level-complement">Low level complement</h2>
<h3 id="single-process">Single process</h3>
<h4 id="inner-join">inner join</h4>
<p><a href="https://www.w3schools.com/sql/sql_join_inner.asp">What is inner join</a></p>
<p>This is the simplest join method in hash join. So we introduce a simple hash join state machine here. (See <code>ExecHashJoinImpl</code> for detail )</p>
<pre tabindex="0"><code>START WITH:
    state ==&gt; HJ_BUILD_HASHTABLE

case HJ_BUILD_HASHTABLE:
    state ==&gt; HJ_NEED_NEW_OUTER

case HJ_NEED_NEW_OUTER:
    ### generate a new outer tuple
    state ==&gt; HJ_NEED_NEW_BATCH ### No more tuple in this batch.
          ==&gt; HJ_SCAN_BUCKET;   ### Find a outer tuple. Can this one matches a
                                    inner one?

case HJ_SCAN_BUCKET:
    ### Scan the selected hash bucket for matches to current outer
    state ==&gt; HJ_NEED_NEW_OUTER ### Whether we can find a match or not, we
                                    always generate a new outer tuple.

case HJ_NEED_NEW_BATCH:
    ### Try to advance to next batch
    state ==&gt; HJ_NEED_NEW_OUTER;
          ==&gt; FINISH
</code></pre><h4 id="right-join">right join</h4>
<p>To complete right join, we can just emit each outer tuple even if there&rsquo;s no
matched innner tuple.</p>
<pre tabindex="0"><code>case HJ_SCAN_BUCKET:
    state ==&gt; HJ_FILL_OUTER_TUPLE  ### Can not find a match. Is it a left join?
          ==&gt; HJ_NEED_NEW_OUTER

case HJ_FILL_OUTER_TUPLE:
    state ==&gt; HJ_NEED_NEW_OUTER;    ### Whether emit the outer tuple with
                                        null-filled left tuple or not, we always
                                        generate a new outer tuple.
</code></pre><h4 id="left-join">left join</h4>
<p>To complete this, we must remember whether a inner tuple has been matched. So</p>
<pre tabindex="0"><code>case HJ_NEED_NEW_OUTER:
    state ==&gt; HJ_FILL_INNER_TUPLES  ### This batch has been finished, see if
                                        there are unmatched inner tuples.
          ==&gt; HJ_NEED_NEW_BATCH
          ==&gt; HJ_SCAN_BUCKET

case HJ_FILL_INNER_TUPLES:
    state ==&gt; HJ_NEED_NEW_BATCH     ### No more unmatched inner tuples, so start
                                        the next batch
          ==&gt; HJ_FILL_INNER_TUPLES  ### return an unmatched inner tuple.
</code></pre><h4 id="summary">summary</h4>
<p>Until now, we can generate a full state machine in non-parallel mode</p>
<pre tabindex="0"><code>START WITH:
    state ==&gt; HJ_BUILD_HASHTABLE

case HJ_BUILD_HASHTABLE:
    state ==&gt; HJ_NEED_NEW_OUTER

case HJ_NEED_NEW_OUTER:
    ### generate a new outer tuple
    state ==&gt; HJ_FILL_INNER_TUPLES  ### This batch has been finished, see if
                                        there are unmatched inner tuples.
          ==&gt; HJ_NEED_NEW_BATCH ### No more tuple in this batch.
          ==&gt; HJ_SCAN_BUCKET;   ### Find a outer tuple. Can this one matches a
                                    inner one?

case HJ_SCAN_BUCKET:
    ### Scan the selected hash bucket for matches to current outer
    state ==&gt; HJ_FILL_OUTER_TUPLE  ### Can not find a match. Is it a left join?
          ==&gt; HJ_NEED_NEW_OUTER ### Whether we can find a match or not, we
                                    always generate a new outer tuple.

case HJ_NEED_NEW_BATCH:
    ### Try to advance to next batch
    state ==&gt; HJ_NEED_NEW_OUTER;
          ==&gt; FINISH
</code></pre><h3 id="parallel-hash">parallel hash</h3>
<blockquote>
<p>Note that <code>BarrierArriveAndWait</code> will increase current phase. So each phase&rsquo;s status is not be assigned directly but self-increased.</p></blockquote>
<p>Let introduce the state machine first</p>
<pre tabindex="0"><code>START WITH:
case HJ_BUILD_HASHTABLE:
    ### If multi-batch, we need to hash the outer relation up front.
    ExecParallelHashJoinPartitionOuter(node);
    state ==&gt; HJ_NEED_NEW_BATCH ### Select a batch to work on.

case HJ_NEED_NEW_OUTER:
    ExecParallelHashJoinOuterGetTuple
        sts_parallel_scan_next
    
case HJ_NEED_NEW_BATCH:
    ExecParallelHashJoinNewBatch()
        switch PHJ_BATCH_STATE
            case PHJ_BATCH_ELECT:
                ### One backend allocates the hash table
                ExecParallelHashTableAlloc
                ### Fall through
            case PHJ_BATCH_ALLOCATE:
                ### Wait for allocation to complete and Fall through
            case PHJ_BATCH_LOAD:
                ### Start (or join in) loading tuples and Fall through.
            case PHJ_BATCH_PROBE:
                ### This batch is ready to probe
                ExecParallelHashTableSetCurrentBatch
                return true;
            case PHJ_BATCH_SCAN:
                ### detach and go around again
            case PHJ_BATCH_FREE:
    state ==&gt; HJ_NEED_NEW_OUTER
</code></pre><pre tabindex="0"><code>    PHJ_BUILD_ELECT ==&gt; PHJ_BUILD_ALLOCATE
</code></pre><pre tabindex="0"><code>ExecParallelHashJoinNewBatch
</code></pre><h2 id="code-level-detail">Code level Detail</h2>
<h3 id="utility">utility</h3>
<ul>
<li><code>ExecHashGetBucketAndBatch</code> : hash value to bucket number and batch number</li>
</ul>
<pre tabindex="0"><code>ExecHashGetBucketAndBatch(HashJoinTable hashtable,
                          uint32 hashvalue,
                          int *bucketno,
                          int *batchno)
{
    uint32      nbuckets = (uint32) hashtable-&gt;nbuckets;
    uint32      nbatch = (uint32) hashtable-&gt;nbatch;

    if (nbatch &gt; 1)
    {
        *bucketno = hashvalue &amp; (nbuckets - 1); ### tricky way as MOD
        *batchno = pg_rotate_right32(hashvalue,
                                     hashtable-&gt;log2_nbuckets) &amp; (nbatch - 1);
        ### rotate hashvalue and MOD nbatch
    }
    else
    {
        *bucketno = hashvalue &amp; (nbuckets - 1);
        *batchno = 0;
    }
}
</code></pre><ul>
<li><code>ExecHashTableInsert</code> : insert hash value</li>
</ul>
<pre tabindex="0"><code>ExecHashTableInsert
    ExecHashGetBucketAndBatch(hashtable, hashvalue,
                              &amp;bucketno, &amp;batchno);
    if (batchno == hashtable-&gt;curbatch) ### put into hash table
        hashTuple = (HashJoinTuple) dense_alloc
        hashtable-&gt;spaceUsed += hashTupleSize;

        ### For single batch, we may increase the nbucket
        if (hashtable-&gt;nbatch == 1)
            if (ntuples &gt; (hashtable-&gt;nbuckets_optimal * NTUP_PER_BUCKET) &amp;&amp; xxx)
                hashtable-&gt;nbuckets_optimal *= 2;
                hashtable-&gt;log2_nbuckets_optimal += 1;

        ### For multi-batches, we may increase the batches
        if (hashtable-&gt;spaceUsed +
            hashtable-&gt;nbuckets_optimal * sizeof(HashJoinTuple) +
            &gt; hashtable-&gt;spaceAllowed)
            ExecHashIncreaseNumBatches()
    else    ### put the tuple into a temp file for later batches
        ExecHashJoinSaveTuple()
</code></pre><ul>
<li><code>ExecHashIncreaseNumBatches</code> : increase batches</li>
</ul>
<pre tabindex="0"><code>ExecHashIncreaseNumBatches
    nbatch = oldnbatch * 2; ### double nbatches

    ### init/update batchfiles
    if (hashtable-&gt;innerBatchFile == NULL)
        hashtable-&gt;innerBatchFile = palloc0_array(BufFile *, nbatch);
        hashtable-&gt;outerBatchFile = palloc0_array(BufFile *, nbatch);
        PrepareTempTablespaces();
    else
        hashtable-&gt;innerBatchFile = repalloc0_array()
        hashtable-&gt;outerBatchFile

    ### resize nbuckets?
    if (hashtable-&gt;nbuckets_optimal != hashtable-&gt;nbuckets)
        hashtable-&gt;nbuckets = hashtable-&gt;nbuckets_optimal;
        hashtable-&gt;log2_nbuckets = hashtable-&gt;log2_nbuckets_optimal;
        hashtable-&gt;buckets.unshared = repalloc_array()

    ### scan through allchunks
    while (oldchunks != NULL)
        nextchunk = oldchunks-&gt;next.unshared

        ### scan through all tuples in the chunk
        idx = 0
        while (idx &lt; oldchunks-&gt;used)
            HashJoinTuple hashTuple = (HashJoinTuple) (HASH_CHUNK_DATA(oldchunks) + idx);
            ...

            ### where should the tuple go?
            ExecHashGetBucketAndBatch(hashtable, hashTuple-&gt;hashvalue,
                                      &amp;bucketno, &amp;batchno);
            if (batchno == curbatch)
                ### keep the tuple but copy it into the new chunk
                copyTuple = (HashJoinTuple) dense_alloc(hashtable, hashTupleSize);
                hashtable-&gt;buckets.unshared[bucketno] = copyTuple;
            else
                ### dump it out
                ExecHashJoinSaveTuple()
            idx += MAXALIGN(hashTupleSize);

        pfree(oldchunks);
        oldchunks = nextchunk;
</code></pre><ul>
<li><code>ExecHashJoinSaveTuple</code> : save a tuple to a batch file.</li>
</ul>
<pre tabindex="0"><code>    BufFileWrite(file, &amp;hashvalue, sizeof(uint32));
    BufFileWrite(file, tuple, tuple-&gt;t_len);    ### len is record in 
                                                    MinimalTupleData structure
</code></pre><h3 id="single-worker">single worker</h3>
<h4 id="build-state">build state</h4>
<pre tabindex="0"><code>MultiExecProcNode
    MultiExecPrivateHash
        for (;;)
            slot = ExecProcNode(outerNode);
            if (ExecHashGetHashValue())
                bucketNumber = ExecHashGetSkewBucket
                if (bucketNumber != INVALID_SKEW_BUCKET_NO) ###  skew tuple
                    ExecHashSkewTableInsert
                else
                    ExecHashTableInsert	### normal tuple
            hashtable-&gt;totalTuples += 1;
</code></pre><p>xxx</p>

</article>

                
    
    
        <hr>
<p>written by mobilephone724</p>
    


            </div>
        </main>
    </body>
</html>
